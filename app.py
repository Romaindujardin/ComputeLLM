"""
ComputeLLM - Application Streamlit (Interface utilisateur).
Interface graphique principale avec trois pages :
  1. Mat√©riel - D√©tection et affichage des informations mat√©rielles
  2. Benchmark - Lancement des benchmarks (bouton unique)
  3. R√©sultats - Affichage, comparaison et visualisation
"""

import streamlit as st
import json
import time
import os
import sys
from pathlib import Path
from datetime import datetime

# Ajouter le r√©pertoire parent au path
sys.path.insert(0, str(Path(__file__).parent))

from src.hardware_detect import get_full_hardware_info, get_hardware_summary
from src.benchmark_classic import run_all_classic_benchmarks
from src.benchmark_ai import (
    run_all_ai_benchmarks,
    list_available_models,
    get_compatible_models,
    is_model_downloaded,
    download_model,
    detect_best_backend,
)
from src.results_manager import (
    save_results,
    list_results,
    load_result,
    compare_results,
    export_to_csv,
)
from src.config import AVAILABLE_MODELS, RESULTS_DIR

# =============================================================================
# Configuration de la page Streamlit
# =============================================================================
st.set_page_config(
    page_title="ComputeLLM - AI Hardware Benchmark",
    page_icon="üñ•Ô∏è",
    layout="wide",
    initial_sidebar_state="expanded",
)

# =============================================================================
# CSS personnalis√©
# =============================================================================
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: 700;
        color: #1E88E5;
        text-align: center;
        margin-bottom: 0.5rem;
    }
    .sub-header {
        text-align: center;
        color: #666;
        margin-bottom: 2rem;
    }
    .metric-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 1.5rem;
        border-radius: 12px;
        color: white;
        text-align: center;
        margin-bottom: 1rem;
    }
    .metric-card h3 {
        margin: 0;
        font-size: 0.9rem;
        opacity: 0.8;
    }
    .metric-card h2 {
        margin: 0.5rem 0 0 0;
        font-size: 1.8rem;
    }
    .status-ok { color: #4CAF50; font-weight: bold; }
    .status-warn { color: #FF9800; font-weight: bold; }
    .status-error { color: #f44336; font-weight: bold; }
    .benchmark-btn {
        display: block;
        margin: 2rem auto;
    }
</style>
""", unsafe_allow_html=True)


# =============================================================================
# Session State Initialization
# =============================================================================
if "hardware_info" not in st.session_state:
    st.session_state.hardware_info = None
if "benchmark_running" not in st.session_state:
    st.session_state.benchmark_running = False
if "classic_results" not in st.session_state:
    st.session_state.classic_results = None
if "ai_results" not in st.session_state:
    st.session_state.ai_results = None
if "last_save_path" not in st.session_state:
    st.session_state.last_save_path = None


# =============================================================================
# Sidebar Navigation
# =============================================================================
st.sidebar.markdown("## üñ•Ô∏è ComputeLLM")
st.sidebar.markdown("---")
page = st.sidebar.radio(
    "Navigation",
    ["üè† Mat√©riel", "üöÄ Benchmark", "üìä R√©sultats"],
    index=0,
)
st.sidebar.markdown("---")
st.sidebar.markdown("**Version** 1.0.0")
st.sidebar.markdown("Benchmark Hardware IA")
st.sidebar.markdown("Multiplateforme (macOS / Windows)")


# =============================================================================
# PAGE 1 : D√©tection Mat√©rielle
# =============================================================================
def page_hardware():
    st.markdown('<h1 class="main-header">üñ•Ô∏è D√©tection Mat√©rielle</h1>', unsafe_allow_html=True)
    st.markdown('<p class="sub-header">Analyse automatique de votre configuration</p>', unsafe_allow_html=True)

    # Bouton de d√©tection
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        if st.button("üîç D√©tecter le mat√©riel", use_container_width=True, type="primary"):
            with st.spinner("Analyse du mat√©riel en cours..."):
                st.session_state.hardware_info = get_full_hardware_info()

    if st.session_state.hardware_info is None:
        st.info("Cliquez sur le bouton ci-dessus pour d√©tecter votre mat√©riel.")
        return

    hw = st.session_state.hardware_info

    # --- Syst√®me d'exploitation ---
    st.markdown("### üíª Syst√®me d'exploitation")
    os_info = hw["os"]
    cols = st.columns(4)
    cols[0].metric("OS", os_info["system"])
    cols[1].metric("Version", os_info["release"])
    cols[2].metric("Architecture", os_info["architecture"])
    cols[3].metric("Python", os_info["python_version"])

    st.markdown("---")

    # --- CPU ---
    st.markdown("### ‚öôÔ∏è Processeur (CPU)")
    cpu = hw["cpu"]
    cols = st.columns(4)
    cols[0].metric("Mod√®le", cpu.get("model", "Unknown"))
    cols[1].metric("C≈ìurs physiques", cpu.get("physical_cores", "?"))
    cols[2].metric("C≈ìurs logiques", cpu.get("logical_cores", "?"))
    arch_type = cpu.get("architecture_type", cpu.get("architecture", "?"))
    cols[3].metric("Architecture", arch_type)

    if cpu.get("is_apple_silicon"):
        cols2 = st.columns(4)
        cols2[0].metric("Type", "Apple Silicon ‚úÖ")
        perf = cpu.get("performance_cores", "?")
        eff = cpu.get("efficiency_cores", "?")
        cols2[1].metric("C≈ìurs Performance", perf)
        cols2[2].metric("C≈ìurs Efficience", eff)
        if cpu.get("frequency_mhz"):
            cols2[3].metric("Fr√©quence", f"{cpu['frequency_mhz'].get('current', '?')} MHz")

    st.markdown("---")

    # --- M√©moire RAM ---
    st.markdown("### üß† M√©moire RAM")
    ram = hw["ram"]
    cols = st.columns(4)
    cols[0].metric("Total", f"{ram['total_gb']} Go")
    cols[1].metric("Disponible", f"{ram['available_gb']} Go")
    cols[2].metric("Utilis√©e", f"{ram['percent_used']}%")
    mem_type = "Unifi√©e (CPU+GPU)" if ram.get("unified_memory") else "D√©di√©e"
    cols[3].metric("Type", mem_type)

    if ram.get("swap_total_gb", 0) > 0:
        st.caption(f"Swap : {ram['swap_used_gb']:.1f} / {ram['swap_total_gb']:.1f} Go")

    st.markdown("---")

    # --- GPU ---
    st.markdown("### üéÆ GPU & Acc√©l√©ration")
    gpu = hw["gpu"]

    if gpu["gpus"]:
        for g in gpu["gpus"]:
            cols = st.columns(4)
            cols[0].metric("GPU", g["name"])
            cols[1].metric("Type", g["type"])
            cols[2].metric("Backend", g["backend"].upper())

            if "vram_total_mb" in g:
                cols[3].metric("VRAM", f"{g['vram_total_mb']:.0f} Mo")
            elif "unified_memory_gb" in g:
                cols[3].metric("M√©moire", f"{g['unified_memory_gb']} Go (unifi√©e)")
            elif "vram" in g:
                cols[3].metric("VRAM", g["vram"])
    else:
        st.warning("Aucun GPU d√©tect√©. L'inf√©rence utilisera le CPU.")

    cols_backend = st.columns(3)
    cols_backend[0].metric("Backend principal", gpu["primary_backend"].upper())
    cols_backend[1].metric("Backends disponibles", ", ".join(b.upper() for b in gpu["backends"]))

    # Biblioth√®ques Python
    py_backends = gpu.get("python_backends", {})
    if py_backends:
        st.markdown("**Biblioth√®ques Python d√©tect√©es :**")
        py_cols = st.columns(3)
        if py_backends.get("llama_cpp"):
            py_cols[0].success(f"‚úÖ llama-cpp-python {py_backends.get('llama_cpp_version', '')}")
        else:
            py_cols[0].error("‚ùå llama-cpp-python non install√©")

        if py_backends.get("pytorch"):
            ver = py_backends.get("pytorch_version", "")
            cuda_str = f" (CUDA {py_backends['pytorch_cuda_version']})" if py_backends.get("pytorch_cuda") else ""
            mps_str = " (MPS ‚úÖ)" if py_backends.get("pytorch_mps") else ""
            py_cols[1].success(f"‚úÖ PyTorch {ver}{cuda_str}{mps_str}")
        else:
            py_cols[1].warning("‚ö†Ô∏è PyTorch non install√© (GPU benchmark indisponible)")

    # Mod√®les compatibles
    st.markdown("---")
    st.markdown("### üì¶ Mod√®les LLM compatibles")
    ram_total = ram["total_gb"]
    compatible = get_compatible_models(ram_total)

    for key, model in AVAILABLE_MODELS.items():
        is_compat = key in compatible
        is_downloaded = is_model_downloaded(key)
        icon = "‚úÖ" if is_compat else "‚ùå"
        dl_icon = "üì•" if is_downloaded else "‚¨ú"

        cols = st.columns([0.5, 2, 1, 1, 1, 1])
        cols[0].write(icon)
        cols[1].write(f"**{model['name']}** ({model['params']})")
        cols[2].write(f"{model['size_gb']} Go")
        cols[3].write(f"RAM min: {model['min_ram_gb']} Go")
        cols[4].write(dl_icon + (" T√©l√©charg√©" if is_downloaded else " Non t√©l√©charg√©"))
        cols[5].write("Compatible" if is_compat else "Incompatible")

    # Export JSON brut
    with st.expander("üìã Donn√©es brutes (JSON)"):
        st.json(hw)


# =============================================================================
# PAGE 2 : Benchmark
# =============================================================================
def page_benchmark():
    st.markdown('<h1 class="main-header">üöÄ Benchmark</h1>', unsafe_allow_html=True)
    st.markdown('<p class="sub-header">Lancer l\'ensemble des benchmarks mat√©riels et IA</p>', unsafe_allow_html=True)

    # V√©rifier que le mat√©riel a √©t√© d√©tect√©
    if st.session_state.hardware_info is None:
        st.warning("‚ö†Ô∏è Veuillez d'abord d√©tecter le mat√©riel dans la page 'Mat√©riel'.")
        if st.button("üîç D√©tecter le mat√©riel maintenant"):
            with st.spinner("D√©tection..."):
                st.session_state.hardware_info = get_full_hardware_info()
            st.rerun()
        return

    hw = st.session_state.hardware_info

    # Configuration des benchmarks
    st.markdown("### ‚öôÔ∏è Configuration")

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("**Benchmarks classiques**")
        run_classic = st.checkbox("CPU Single-Thread", value=True)
        run_classic_mt = st.checkbox("CPU Multi-Thread", value=True)
        run_memory = st.checkbox("Bande passante m√©moire", value=True)
        run_gpu = st.checkbox("GPU (si disponible)", value=True)

    with col2:
        st.markdown("**Benchmarks IA (Inf√©rence LLM)**")
        ram_total = hw["ram"]["total_gb"]
        compatible = get_compatible_models(ram_total)

        selected_models = []
        for key, model in compatible.items():
            default = key == "tinyllama-1.1b"  # S√©lectionner le petit par d√©faut
            if st.checkbox(
                f"{model['name']} ({model['params']}) - {model['size_gb']} Go",
                value=default,
                key=f"model_{key}",
            ):
                selected_models.append(key)

        if not compatible:
            st.warning("Aucun mod√®le compatible avec votre RAM.")

    st.markdown("---")

    # R√©sum√© de la configuration
    backend_info = detect_best_backend()
    st.markdown("### üìã R√©sum√©")
    cols = st.columns(4)
    cols[0].metric("Backend IA", backend_info["backend"].upper())
    cols[1].metric("Mod√®les s√©lectionn√©s", len(selected_models))
    cols[2].metric("RAM disponible", f"{hw['ram']['available_gb']} Go")
    n_tests = sum([run_classic, run_classic_mt, run_memory, run_gpu]) + len(selected_models)
    cols[3].metric("Tests √† ex√©cuter", n_tests)

    st.markdown("---")

    # ===== BOUTON UNIQUE DE LANCEMENT =====
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        launch = st.button(
            "üöÄ LANCER TOUS LES BENCHMARKS",
            use_container_width=True,
            type="primary",
            disabled=st.session_state.benchmark_running,
        )

    if launch:
        st.session_state.benchmark_running = True
        st.session_state.classic_results = None
        st.session_state.ai_results = None

        total_start = time.time()

        # ============================
        # BENCHMARKS CLASSIQUES
        # ============================
        if any([run_classic, run_classic_mt, run_memory, run_gpu]):
            st.markdown("### üìä Benchmarks Classiques")
            classic_progress = st.progress(0.0)
            classic_status = st.status("Benchmarks classiques en cours...", expanded=True)

            def classic_callback(p, msg):
                classic_progress.progress(min(p, 1.0))
                classic_status.update(label=msg)

            with classic_status:
                st.write("Ex√©cution des tests CPU, GPU et m√©moire...")
                try:
                    classic_results = run_all_classic_benchmarks(
                        progress_callback=classic_callback
                    )
                    st.session_state.classic_results = classic_results
                    classic_status.update(
                        label=f"‚úÖ Benchmarks classiques termin√©s ({classic_results['total_time_s']:.1f}s)",
                        state="complete"
                    )
                except Exception as e:
                    classic_status.update(label=f"‚ùå Erreur : {e}", state="error")
                    st.error(f"Erreur benchmarks classiques : {e}")

            classic_progress.progress(1.0)
        else:
            st.info("Benchmarks classiques d√©sactiv√©s.")

        # ============================
        # BENCHMARKS IA
        # ============================
        if selected_models:
            st.markdown("### ü§ñ Benchmarks IA (Inf√©rence LLM)")
            ai_progress = st.progress(0.0)
            ai_status = st.status("Benchmarks IA en cours...", expanded=True)

            def ai_callback(p, msg):
                ai_progress.progress(min(p, 1.0))
                ai_status.update(label=msg)

            with ai_status:
                # T√©l√©chargement des mod√®les si n√©cessaire
                for model_key in selected_models:
                    if not is_model_downloaded(model_key):
                        model_info = AVAILABLE_MODELS[model_key]
                        st.write(f"üì• T√©l√©chargement de {model_info['name']}...")
                        try:
                            download_model(model_key)
                            st.write(f"‚úÖ {model_info['name']} t√©l√©charg√©.")
                        except Exception as e:
                            st.error(f"‚ùå Erreur t√©l√©chargement {model_info['name']}: {e}")

                # Ex√©cution des benchmarks
                st.write("Ex√©cution des inf√©rences...")
                try:
                    ai_results = run_all_ai_benchmarks(
                        model_keys=selected_models,
                        progress_callback=ai_callback,
                    )
                    st.session_state.ai_results = ai_results
                    ai_status.update(
                        label=f"‚úÖ Benchmarks IA termin√©s ({ai_results['total_time_s']:.1f}s)",
                        state="complete"
                    )
                except Exception as e:
                    ai_status.update(label=f"‚ùå Erreur : {e}", state="error")
                    st.error(f"Erreur benchmarks IA : {e}")

            ai_progress.progress(1.0)
        else:
            st.info("Aucun mod√®le IA s√©lectionn√©.")

        # ============================
        # SAUVEGARDE AUTOMATIQUE
        # ============================
        total_time = time.time() - total_start

        st.markdown("---")
        st.markdown("### üíæ Sauvegarde")

        try:
            save_path = save_results(
                hardware_info=hw,
                classic_results=st.session_state.classic_results,
                ai_results=st.session_state.ai_results,
            )
            st.session_state.last_save_path = str(save_path)
            st.success(f"‚úÖ R√©sultats sauvegard√©s : `{save_path.name}`")
        except Exception as e:
            st.error(f"‚ùå Erreur sauvegarde : {e}")

        st.markdown(f"**Temps total : {total_time:.1f} secondes**")

        st.session_state.benchmark_running = False
        st.balloons()

    # Afficher un r√©sum√© rapide si des r√©sultats sont en session
    if st.session_state.classic_results or st.session_state.ai_results:
        st.markdown("---")
        st.markdown("### üìã Derniers r√©sultats")

        if st.session_state.classic_results:
            benchmarks = st.session_state.classic_results.get("benchmarks", {})

            cols = st.columns(4)

            # CPU ST
            cpu_st = benchmarks.get("cpu_single_thread", {}).get("results", {})
            if cpu_st:
                largest = list(cpu_st.values())[-1]
                cols[0].metric("CPU Single-Thread", f"{largest.get('gflops', 0)} GFLOPS")

            # CPU MT
            cpu_mt = benchmarks.get("cpu_multi_thread", {}).get("results", {})
            if cpu_mt:
                largest = list(cpu_mt.values())[-1]
                cols[1].metric("CPU Multi-Thread", f"{largest.get('gflops', 0)} GFLOPS")

            # M√©moire
            mem = benchmarks.get("memory_bandwidth", {}).get("results", {})
            if mem:
                read_bw = mem.get("read", {}).get("bandwidth_gb_s", 0)
                cols[2].metric("M√©moire (lecture)", f"{read_bw} Go/s")

            # GPU
            gpu = benchmarks.get("gpu_compute", {})
            if gpu.get("status") == "completed":
                gpu_res = gpu.get("results", {})
                if gpu_res:
                    largest = list(gpu_res.values())[-1]
                    cols[3].metric("GPU", f"{largest.get('gflops', 0)} GFLOPS")
            else:
                cols[3].metric("GPU", gpu.get("reason", "N/A"))

        if st.session_state.ai_results:
            ai_res = st.session_state.ai_results.get("results", {})
            if ai_res:
                cols = st.columns(len(ai_res))
                for i, (key, data) in enumerate(ai_res.items()):
                    summary = data.get("summary", {})
                    if summary:
                        tps = summary.get("avg_tokens_per_second", 0)
                        ftl = summary.get("avg_first_token_latency_s", 0)
                        cols[i].metric(
                            data.get("model", key),
                            f"{tps} tok/s",
                            f"Latence 1er token : {ftl:.3f}s"
                        )
                    elif data.get("status") == "skipped":
                        cols[i].metric(
                            data.get("model", key),
                            "Ignor√©",
                            data.get("reason", "")
                        )

        st.info("üìä Consultez la page **R√©sultats** pour une analyse d√©taill√©e.")


# =============================================================================
# PAGE 3 : R√©sultats
# =============================================================================
def page_results():
    st.markdown('<h1 class="main-header">üìä R√©sultats</h1>', unsafe_allow_html=True)
    st.markdown('<p class="sub-header">Visualisation et comparaison des benchmarks</p>', unsafe_allow_html=True)

    # Charger la liste des r√©sultats disponibles
    saved_results = list_results()

    if not saved_results:
        st.info("Aucun r√©sultat de benchmark trouv√©. Lancez un benchmark d'abord !")
        return

    # S√©lection des r√©sultats
    st.markdown("### üìÇ R√©sultats disponibles")

    result_options = {
        r["filename"]: r for r in saved_results
    }

    selected_files = st.multiselect(
        "S√©lectionnez un ou plusieurs r√©sultats √† afficher/comparer :",
        options=list(result_options.keys()),
        default=[list(result_options.keys())[0]],
    )

    if not selected_files:
        st.warning("S√©lectionnez au moins un r√©sultat.")
        return

    # Charger les r√©sultats s√©lectionn√©s
    loaded_data = {}
    for fname in selected_files:
        filepath = result_options[fname]["filepath"]
        try:
            loaded_data[fname] = load_result(filepath)
        except Exception as e:
            st.error(f"Erreur chargement {fname}: {e}")

    if not loaded_data:
        return

    # ============================
    # AFFICHAGE SIMPLE (1 r√©sultat)
    # ============================
    if len(loaded_data) == 1:
        fname, data = list(loaded_data.items())[0]
        _display_single_result(data, fname)

    # ============================
    # COMPARAISON (plusieurs r√©sultats)
    # ============================
    else:
        _display_comparison(loaded_data)

    # Export
    st.markdown("---")
    st.markdown("### üì§ Export")
    for fname in selected_files:
        filepath = result_options[fname]["filepath"]
        col1, col2 = st.columns(2)
        with col1:
            if st.button(f"üì• T√©l√©charger JSON - {fname}", key=f"dl_json_{fname}"):
                with open(filepath, "r") as f:
                    st.download_button(
                        label=f"üíæ {fname}",
                        data=f.read(),
                        file_name=fname,
                        mime="application/json",
                        key=f"download_{fname}",
                    )
        with col2:
            if st.button(f"üì• Exporter CSV - {fname}", key=f"dl_csv_{fname}"):
                try:
                    csv_path = export_to_csv(filepath)
                    with open(csv_path, "r") as f:
                        st.download_button(
                            label=f"üíæ {csv_path.name}",
                            data=f.read(),
                            file_name=csv_path.name,
                            mime="text/csv",
                            key=f"download_csv_{fname}",
                        )
                except Exception as e:
                    st.error(f"Erreur export CSV : {e}")


def _display_single_result(data: dict, filename: str):
    """Affiche les d√©tails d'un seul r√©sultat de benchmark."""
    import plotly.graph_objects as go
    import plotly.express as px

    st.markdown(f"### üîç D√©tails : {filename}")

    # Info machine
    hw = data.get("hardware", {})
    cpu_model = hw.get("cpu", {}).get("model", "Unknown")
    gpus = hw.get("gpu", {}).get("gpus", [])
    gpu_name = gpus[0]["name"] if gpus else "None"
    ram_total = hw.get("ram", {}).get("total_gb", 0)
    backend = hw.get("gpu", {}).get("primary_backend", "cpu")

    cols = st.columns(4)
    cols[0].metric("CPU", cpu_model)
    cols[1].metric("GPU", gpu_name)
    cols[2].metric("RAM", f"{ram_total} Go")
    cols[3].metric("Backend", backend.upper())

    st.markdown("---")

    # === Benchmarks Classiques ===
    classic = data.get("classic_benchmarks", {}).get("benchmarks", {})
    if classic:
        st.markdown("### ‚ö° Benchmarks Classiques")

        # Graphique GFLOPS CPU
        cpu_data_chart = {"Test": [], "GFLOPS": [], "Type": []}

        cpu_st = classic.get("cpu_single_thread", {}).get("results", {})
        for size, vals in cpu_st.items():
            cpu_data_chart["Test"].append(size)
            cpu_data_chart["GFLOPS"].append(vals.get("gflops", 0))
            cpu_data_chart["Type"].append("Single-Thread")

        cpu_mt = classic.get("cpu_multi_thread", {}).get("results", {})
        for size, vals in cpu_mt.items():
            cpu_data_chart["Test"].append(size)
            cpu_data_chart["GFLOPS"].append(vals.get("gflops", 0))
            cpu_data_chart["Type"].append("Multi-Thread")

        if cpu_data_chart["Test"]:
            fig = px.bar(
                cpu_data_chart,
                x="Test", y="GFLOPS", color="Type",
                barmode="group",
                title="Performance CPU (GFLOPS) - Multiplication matricielle",
                color_discrete_map={"Single-Thread": "#FF6B6B", "Multi-Thread": "#4ECDC4"},
            )
            fig.update_layout(xaxis_title="Taille matrice", yaxis_title="GFLOPS")
            st.plotly_chart(fig, use_container_width=True)

        # Bandwidth m√©moire
        mem = classic.get("memory_bandwidth", {}).get("results", {})
        if mem:
            mem_chart = {
                "Op√©ration": ["Lecture", "√âcriture", "Copie"],
                "Bande passante (Go/s)": [
                    mem.get("read", {}).get("bandwidth_gb_s", 0),
                    mem.get("write", {}).get("bandwidth_gb_s", 0),
                    mem.get("copy", {}).get("bandwidth_gb_s", 0),
                ],
            }
            fig = px.bar(
                mem_chart,
                x="Op√©ration", y="Bande passante (Go/s)",
                title="Bande passante m√©moire",
                color="Op√©ration",
                color_discrete_sequence=["#667eea", "#764ba2", "#f093fb"],
            )
            st.plotly_chart(fig, use_container_width=True)

        # GPU
        gpu_bench = classic.get("gpu_compute", {})
        if gpu_bench.get("status") == "completed":
            gpu_results = gpu_bench.get("results", {})
            gpu_chart = {"Taille": [], "GFLOPS": []}
            for size, vals in gpu_results.items():
                gpu_chart["Taille"].append(size)
                gpu_chart["GFLOPS"].append(vals.get("gflops", 0))

            fig = px.bar(
                gpu_chart,
                x="Taille", y="GFLOPS",
                title=f"Performance GPU ({gpu_bench.get('backend', '')}) - {gpu_bench.get('device', '')}",
                color_discrete_sequence=["#FF6B6B"],
            )
            st.plotly_chart(fig, use_container_width=True)

        # Utilisation ressources
        resource = data.get("classic_benchmarks", {}).get("resource_usage", {})
        if resource and "cpu" in resource:
            st.markdown("**Utilisation des ressources pendant le benchmark :**")
            res_cols = st.columns(3)
            res_cols[0].metric("CPU moyen", f"{resource['cpu']['avg_percent']}%")
            res_cols[1].metric("CPU max", f"{resource['cpu']['max_percent']}%")
            res_cols[2].metric("RAM pic", f"{resource['ram'].get('peak_used_gb', 0)} Go")

    st.markdown("---")

    # === Benchmarks IA ===
    ai = data.get("ai_benchmarks", {})
    ai_results = ai.get("results", {})
    if ai_results:
        st.markdown("### ü§ñ Benchmarks IA (Inf√©rence LLM)")

        # Tableau r√©capitulatif
        table_data = {
            "Mod√®le": [], "Params": [], "Tokens/s": [],
            "1er token (s)": [], "M√©moire (Go)": [],
            "Stabilit√©": [], "Backend": [],
        }

        for key, model_data in ai_results.items():
            summary = model_data.get("summary", {})
            if summary:
                table_data["Mod√®le"].append(model_data.get("model", key))
                table_data["Params"].append(model_data.get("params", ""))
                table_data["Tokens/s"].append(summary.get("avg_tokens_per_second", 0))
                table_data["1er token (s)"].append(summary.get("avg_first_token_latency_s", 0))
                table_data["M√©moire (Go)"].append(summary.get("peak_memory_gb", 0))
                table_data["Stabilit√©"].append(summary.get("stability", "?"))
                backend_str = model_data.get("backend", {}).get("backend", "?")
                table_data["Backend"].append(backend_str.upper())
            elif model_data.get("status") == "skipped":
                table_data["Mod√®le"].append(model_data.get("model", key))
                table_data["Params"].append("")
                table_data["Tokens/s"].append(0)
                table_data["1er token (s)"].append(0)
                table_data["M√©moire (Go)"].append(0)
                table_data["Stabilit√©"].append("skipped")
                table_data["Backend"].append("")

        import pandas as pd
        df = pd.DataFrame(table_data)
        st.dataframe(df, use_container_width=True, hide_index=True)

        # Graphique Tokens/s
        active_models = {k: v for k, v in ai_results.items() if v.get("summary")}
        if active_models:
            tps_chart = {
                "Mod√®le": [],
                "Tokens/s": [],
            }
            for key, model_data in active_models.items():
                tps_chart["Mod√®le"].append(model_data.get("model", key))
                tps_chart["Tokens/s"].append(
                    model_data["summary"]["avg_tokens_per_second"]
                )

            fig = px.bar(
                tps_chart,
                x="Mod√®le", y="Tokens/s",
                title="D√©bit d'inf√©rence par mod√®le",
                color="Mod√®le",
                color_discrete_sequence=px.colors.qualitative.Set2,
            )
            fig.update_layout(showlegend=False)
            st.plotly_chart(fig, use_container_width=True)

            # Graphique latence premier token
            ftl_chart = {
                "Mod√®le": [],
                "Latence (s)": [],
            }
            for key, model_data in active_models.items():
                ftl_chart["Mod√®le"].append(model_data.get("model", key))
                ftl_chart["Latence (s)"].append(
                    model_data["summary"]["avg_first_token_latency_s"]
                )

            fig = px.bar(
                ftl_chart,
                x="Mod√®le", y="Latence (s)",
                title="Latence du premier token par mod√®le",
                color="Mod√®le",
                color_discrete_sequence=px.colors.qualitative.Pastel,
            )
            fig.update_layout(showlegend=False)
            st.plotly_chart(fig, use_container_width=True)

    # JSON brut
    with st.expander("üìã Donn√©es brutes (JSON)"):
        st.json(data)


def _display_comparison(loaded_data: dict):
    """Affiche la comparaison entre plusieurs r√©sultats avec couleurs distinctes."""
    import plotly.graph_objects as go
    import plotly.express as px
    import pandas as pd

    st.markdown("### ‚öñÔ∏è Comparaison des r√©sultats")

    # ‚îÄ‚îÄ‚îÄ Palette de couleurs distinctes et stables ‚îÄ‚îÄ‚îÄ
    COLORS = [
        "#636EFA",  # bleu
        "#EF553B",  # rouge
        "#00CC96",  # vert
        "#AB63FA",  # violet
        "#FFA15A",  # orange
        "#19D3F3",  # cyan
        "#FF6692",  # rose
        "#B6E880",  # vert clair
    ]

    # ‚îÄ‚îÄ‚îÄ Construire des labels uniques par r√©sultat ‚îÄ‚îÄ‚îÄ
    result_labels = {}   # fname ‚Üí label court unique
    result_colors = {}   # fname ‚Üí couleur
    for idx, (fname, data) in enumerate(loaded_data.items()):
        hw = data.get("hardware", {})
        cpu_short = hw.get("cpu", {}).get("model", "Unknown")
        # Extraire un nom court du CPU (ex: "Apple M1", "i9-13900K")
        for prefix in ["Apple ", "Intel(R) Core(TM) ", "AMD Ryzen "]:
            if prefix in cpu_short:
                cpu_short = cpu_short.split(prefix)[-1].split(" @")[0].split(" CPU")[0]
                break
        timestamp = data.get("timestamp", "")[:16].replace("T", " ")
        label = f"{cpu_short} ({timestamp})"
        # √âviter les doublons exacts
        if label in result_labels.values():
            label += f" #{idx+1}"
        result_labels[fname] = label
        result_colors[fname] = COLORS[idx % len(COLORS)]

    # ‚îÄ‚îÄ‚îÄ Afficher la l√©gende des couleurs ‚îÄ‚îÄ‚îÄ
    legend_html = " &nbsp; ".join(
        f'<span style="display:inline-block;width:14px;height:14px;'
        f'background:{color};border-radius:3px;margin-right:4px;'
        f'vertical-align:middle;"></span>'
        f'<span style="vertical-align:middle;font-weight:600;">{label}</span>'
        for fname, label, color in [
            (f, result_labels[f], result_colors[f]) for f in loaded_data
        ]
    )
    st.markdown(
        f'<div style="background:#f0f2f6;padding:10px 16px;border-radius:8px;'
        f'margin-bottom:20px;">'
        f'<b>üé® L√©gende :</b> &nbsp; {legend_html}</div>',
        unsafe_allow_html=True,
    )

    # ‚îÄ‚îÄ‚îÄ Tableau comparatif mat√©riel ‚îÄ‚îÄ‚îÄ
    st.markdown("#### üñ•Ô∏è Comparaison mat√©rielle")
    hw_table = {"R√©sultat": [], "CPU": [], "GPU": [], "RAM (Go)": [], "Backend": []}
    for fname, data in loaded_data.items():
        hw = data.get("hardware", {})
        hw_table["R√©sultat"].append(result_labels[fname])
        hw_table["CPU"].append(hw.get("cpu", {}).get("model", "?"))
        gpus = hw.get("gpu", {}).get("gpus", [])
        hw_table["GPU"].append(gpus[0]["name"] if gpus else "None")
        hw_table["RAM (Go)"].append(hw.get("ram", {}).get("total_gb", 0))
        hw_table["Backend"].append(
            hw.get("gpu", {}).get("primary_backend", "cpu").upper()
        )
    st.dataframe(pd.DataFrame(hw_table), use_container_width=True, hide_index=True)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # Comparaison CPU GFLOPS
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    st.markdown("#### ‚ö° Comparaison CPU")

    # Collecter CPU ST et MT pour chaque r√©sultat
    has_cpu = False
    fig_cpu = go.Figure()
    for fname, data in loaded_data.items():
        classic = data.get("classic_benchmarks", {}).get("benchmarks", {})
        cpu_st = classic.get("cpu_single_thread", {}).get("results", {})
        cpu_mt = classic.get("cpu_multi_thread", {}).get("results", {})

        if cpu_st:
            largest_key = list(cpu_st.keys())[-1]
            st_gflops = cpu_st[largest_key].get("gflops", 0)
        else:
            st_gflops = 0

        if cpu_mt:
            largest_key = list(cpu_mt.keys())[-1]
            mt_gflops = cpu_mt[largest_key].get("gflops", 0)
        else:
            mt_gflops = 0

        if st_gflops or mt_gflops:
            has_cpu = True
            fig_cpu.add_trace(go.Bar(
                name=result_labels[fname],
                x=["Single-Thread", "Multi-Thread"],
                y=[st_gflops, mt_gflops],
                marker_color=result_colors[fname],
                text=[f"{st_gflops:.1f}", f"{mt_gflops:.1f}"],
                textposition="outside",
            ))

    if has_cpu:
        fig_cpu.update_layout(
            barmode="group",
            title="Performance CPU ‚Äî GFLOPS (plus grande matrice)",
            yaxis_title="GFLOPS",
            legend_title="R√©sultat",
        )
        st.plotly_chart(fig_cpu, use_container_width=True)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # Comparaison M√©moire
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    has_mem = False
    fig_mem = go.Figure()
    for fname, data in loaded_data.items():
        mem = data.get("classic_benchmarks", {}).get("benchmarks", {}).get(
            "memory_bandwidth", {}
        ).get("results", {})
        if mem:
            has_mem = True
            fig_mem.add_trace(go.Bar(
                name=result_labels[fname],
                x=["Lecture", "√âcriture", "Copie"],
                y=[
                    mem.get("read", {}).get("bandwidth_gb_s", 0),
                    mem.get("write", {}).get("bandwidth_gb_s", 0),
                    mem.get("copy", {}).get("bandwidth_gb_s", 0),
                ],
                marker_color=result_colors[fname],
                text=[
                    f"{mem.get('read', {}).get('bandwidth_gb_s', 0):.1f}",
                    f"{mem.get('write', {}).get('bandwidth_gb_s', 0):.1f}",
                    f"{mem.get('copy', {}).get('bandwidth_gb_s', 0):.1f}",
                ],
                textposition="outside",
            ))

    if has_mem:
        st.markdown("#### üß† Comparaison M√©moire")
        fig_mem.update_layout(
            barmode="group",
            title="Bande passante m√©moire (Go/s)",
            yaxis_title="Go/s",
            legend_title="R√©sultat",
        )
        st.plotly_chart(fig_mem, use_container_width=True)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # Comparaison GPU
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    has_gpu = False
    fig_gpu = go.Figure()
    for fname, data in loaded_data.items():
        gpu_bench = data.get("classic_benchmarks", {}).get("benchmarks", {}).get(
            "gpu_compute", {}
        )
        if gpu_bench.get("status") == "completed":
            gpu_results = gpu_bench.get("results", {})
            if gpu_results:
                has_gpu = True
                sizes = list(gpu_results.keys())
                gflops_vals = [gpu_results[s].get("gflops", 0) for s in sizes]
                fig_gpu.add_trace(go.Bar(
                    name=result_labels[fname],
                    x=sizes,
                    y=gflops_vals,
                    marker_color=result_colors[fname],
                    text=[f"{v:.0f}" for v in gflops_vals],
                    textposition="outside",
                ))

    if has_gpu:
        st.markdown("#### üéÆ Comparaison GPU")
        fig_gpu.update_layout(
            barmode="group",
            title="Performance GPU (GFLOPS)",
            yaxis_title="GFLOPS",
            legend_title="R√©sultat",
        )
        st.plotly_chart(fig_gpu, use_container_width=True)

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # Comparaison Inf√©rence IA
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    st.markdown("#### ü§ñ Comparaison Inf√©rence IA")

    # Collecter tous les mod√®les test√©s
    all_models = {}
    for fname, data in loaded_data.items():
        ai_results = data.get("ai_benchmarks", {}).get("results", {})
        for model_key, model_data in ai_results.items():
            if model_data.get("summary"):
                model_name = model_data.get("model", model_key)
                all_models[model_key] = model_name

    if all_models:
        # ‚îÄ‚îÄ Graphique global : Tokens/s par mod√®le ‚îÄ‚îÄ
        fig_tps = go.Figure()
        fig_ftl = go.Figure()
        model_keys_sorted = sorted(all_models.keys())
        model_names_sorted = [all_models[k] for k in model_keys_sorted]

        for fname, data in loaded_data.items():
            ai_results = data.get("ai_benchmarks", {}).get("results", {})
            tps_values = []
            ftl_values = []
            for model_key in model_keys_sorted:
                model_data = ai_results.get(model_key, {})
                summary = model_data.get("summary", {})
                tps_values.append(summary.get("avg_tokens_per_second", 0))
                ftl_values.append(summary.get("avg_first_token_latency_s", 0))

            fig_tps.add_trace(go.Bar(
                name=result_labels[fname],
                x=model_names_sorted,
                y=tps_values,
                marker_color=result_colors[fname],
                text=[f"{v:.1f}" if v > 0 else "" for v in tps_values],
                textposition="outside",
            ))
            fig_ftl.add_trace(go.Bar(
                name=result_labels[fname],
                x=model_names_sorted,
                y=ftl_values,
                marker_color=result_colors[fname],
                text=[f"{v:.3f}" if v > 0 else "" for v in ftl_values],
                textposition="outside",
            ))

        col1, col2 = st.columns(2)
        with col1:
            fig_tps.update_layout(
                barmode="group",
                title="D√©bit d'inf√©rence (tokens/s)",
                yaxis_title="Tokens/s",
                legend_title="R√©sultat",
                xaxis_title="Mod√®le",
            )
            st.plotly_chart(fig_tps, use_container_width=True)

        with col2:
            fig_ftl.update_layout(
                barmode="group",
                title="Latence du premier token (s)",
                yaxis_title="Secondes",
                legend_title="R√©sultat",
                xaxis_title="Mod√®le",
            )
            st.plotly_chart(fig_ftl, use_container_width=True)

        # ‚îÄ‚îÄ Graphique m√©moire pic par mod√®le ‚îÄ‚îÄ
        fig_mem_ai = go.Figure()
        for fname, data in loaded_data.items():
            ai_results = data.get("ai_benchmarks", {}).get("results", {})
            mem_values = []
            for model_key in model_keys_sorted:
                model_data = ai_results.get(model_key, {})
                summary = model_data.get("summary", {})
                mem_values.append(summary.get("peak_memory_gb", 0))

            fig_mem_ai.add_trace(go.Bar(
                name=result_labels[fname],
                x=model_names_sorted,
                y=mem_values,
                marker_color=result_colors[fname],
                text=[f"{v:.2f}" if v > 0 else "" for v in mem_values],
                textposition="outside",
            ))

        fig_mem_ai.update_layout(
            barmode="group",
            title="M√©moire pic par mod√®le (Go)",
            yaxis_title="Go",
            legend_title="R√©sultat",
            xaxis_title="Mod√®le",
        )
        st.plotly_chart(fig_mem_ai, use_container_width=True)

        # ‚îÄ‚îÄ Tableau comparatif IA complet ‚îÄ‚îÄ
        st.markdown("#### üìã Tableau comparatif complet")
        ia_table = {
            "R√©sultat": [], "Mod√®le": [], "Tokens/s": [],
            "Latence 1er token (s)": [], "M√©moire pic (Go)": [],
            "Stabilit√©": [],
        }
        for fname, data in loaded_data.items():
            ai_results = data.get("ai_benchmarks", {}).get("results", {})
            for model_key, model_data in ai_results.items():
                summary = model_data.get("summary", {})
                if summary:
                    ia_table["R√©sultat"].append(result_labels[fname])
                    ia_table["Mod√®le"].append(model_data.get("model", model_key))
                    ia_table["Tokens/s"].append(summary.get("avg_tokens_per_second", 0))
                    ia_table["Latence 1er token (s)"].append(
                        summary.get("avg_first_token_latency_s", 0)
                    )
                    ia_table["M√©moire pic (Go)"].append(
                        summary.get("peak_memory_gb", 0)
                    )
                    ia_table["Stabilit√©"].append(summary.get("stability", "?"))

        if ia_table["R√©sultat"]:
            st.dataframe(
                pd.DataFrame(ia_table), use_container_width=True, hide_index=True
            )
    else:
        st.info("Aucun r√©sultat d'inf√©rence IA √† comparer.")


# =============================================================================
# Router
# =============================================================================
if page == "üè† Mat√©riel":
    page_hardware()
elif page == "üöÄ Benchmark":
    page_benchmark()
elif page == "üìä R√©sultats":
    page_results()
