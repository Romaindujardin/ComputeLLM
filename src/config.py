"""
ComputeLLM - Configuration et constantes globales.
D√©finit les mod√®les disponibles, les param√®tres de benchmark et les chemins.
"""

import os
from pathlib import Path

# =============================================================================
# Chemins du projet
# =============================================================================
PROJECT_ROOT = Path(__file__).parent.parent
MODELS_DIR = PROJECT_ROOT / "models"
RESULTS_DIR = PROJECT_ROOT / "results"

# Cr√©er les r√©pertoires s'ils n'existent pas
MODELS_DIR.mkdir(exist_ok=True)
RESULTS_DIR.mkdir(exist_ok=True)

# =============================================================================
# Mod√®les LLM disponibles pour le benchmark (GGUF via HuggingFace)
# =============================================================================
AVAILABLE_MODELS = {
    "tinyllama-1.1b": {
        "name": "TinyLlama 1.1B",
        "repo_id": "TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF",
        "filename": "tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
        "params": "1.1B",
        "size_gb": 0.7,
        "min_ram_gb": 2,
        "description": "Mod√®le ultra-l√©ger, id√©al pour tester tous les syst√®mes.",
    },
    "mistral-7b": {
        "name": "Mistral 7B",
        "repo_id": "TheBloke/Mistral-7B-Instruct-v0.2-GGUF",
        "filename": "mistral-7b-instruct-v0.2.Q4_K_M.gguf",
        "params": "7B",
        "size_gb": 4.4,
        "min_ram_gb": 8,
        "description": "Mod√®le performant 7B param√®tres, bon compromis taille/qualit√©.",
    },
    "llama2-13b": {
        "name": "Llama 2 13B",
        "repo_id": "TheBloke/Llama-2-13B-chat-GGUF",
        "filename": "llama-2-13b-chat.Q4_K_M.gguf",
        "params": "13B",
        "size_gb": 7.9,
        "min_ram_gb": 16,
        "description": "Mod√®le 13B param√®tres, n√©cessite plus de m√©moire.",
    },
    "codellama-34b": {
        "name": "CodeLlama 34B",
        "repo_id": "TheBloke/CodeLlama-34B-Instruct-GGUF",
        "filename": "codellama-34b-instruct.Q4_K_M.gguf",
        "params": "34B",
        "size_gb": 20.2,
        "min_ram_gb": 32,
        "description": "Mod√®le 34B param√®tres, test des limites mat√©rielles.",
    },
    "llama2-70b": {
        "name": "Llama 2 70B",
        "repo_id": "TheBloke/Llama-2-70B-chat-GGUF",
        "filename": "llama-2-70b-chat.Q4_K_M.gguf",
        "params": "70B",
        "size_gb": 40.5,
        "min_ram_gb": 64,
        "description": "Mod√®le 70B param√®tres, r√©serv√© aux machines tr√®s puissantes.",
    },
}

# =============================================================================
# Variantes de quantification pour le comparatif
# Chaque mod√®le poss√®de plusieurs niveaux de quantification (Q2 ‚Üí Q8).
# Cela permet de mesurer le compromis taille/qualit√©/performance.
# =============================================================================
QUANTIZATION_VARIANTS = {
    "tinyllama-1.1b": {
        "Q2_K": {
            "filename": "tinyllama-1.1b-chat-v1.0.Q2_K.gguf",
            "size_gb": 0.45,
            "bits": 2,
            "min_ram_gb": 2,
            "description": "2-bit ‚Äî Taille minimale, qualit√© r√©duite",
        },
        "Q3_K_M": {
            "filename": "tinyllama-1.1b-chat-v1.0.Q3_K_M.gguf",
            "size_gb": 0.51,
            "bits": 3,
            "min_ram_gb": 2,
            "description": "3-bit Medium ‚Äî Bon compromis taille/qualit√©",
        },
        "Q4_K_M": {
            "filename": "tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",
            "size_gb": 0.62,
            "bits": 4,
            "min_ram_gb": 2,
            "description": "4-bit Medium ‚Äî D√©faut recommand√©",
        },
        "Q5_K_M": {
            "filename": "tinyllama-1.1b-chat-v1.0.Q5_K_M.gguf",
            "size_gb": 0.73,
            "bits": 5,
            "min_ram_gb": 2,
            "description": "5-bit Medium ‚Äî Bonne qualit√©",
        },
        "Q6_K": {
            "filename": "tinyllama-1.1b-chat-v1.0.Q6_K.gguf",
            "size_gb": 0.84,
            "bits": 6,
            "min_ram_gb": 2,
            "description": "6-bit ‚Äî Haute qualit√©",
        },
        "Q8_0": {
            "filename": "tinyllama-1.1b-chat-v1.0.Q8_0.gguf",
            "size_gb": 1.09,
            "bits": 8,
            "min_ram_gb": 4,
            "description": "8-bit ‚Äî Qualit√© quasi-native",
        },
    },
    "mistral-7b": {
        "Q2_K": {
            "filename": "mistral-7b-instruct-v0.2.Q2_K.gguf",
            "size_gb": 2.87,
            "bits": 2,
            "min_ram_gb": 6,
            "description": "2-bit ‚Äî Taille minimale, qualit√© r√©duite",
        },
        "Q3_K_M": {
            "filename": "mistral-7b-instruct-v0.2.Q3_K_M.gguf",
            "size_gb": 3.28,
            "bits": 3,
            "min_ram_gb": 8,
            "description": "3-bit Medium ‚Äî Bon compromis taille/qualit√©",
        },
        "Q4_K_M": {
            "filename": "mistral-7b-instruct-v0.2.Q4_K_M.gguf",
            "size_gb": 4.07,
            "bits": 4,
            "min_ram_gb": 8,
            "description": "4-bit Medium ‚Äî D√©faut recommand√©",
        },
        "Q5_K_M": {
            "filename": "mistral-7b-instruct-v0.2.Q5_K_M.gguf",
            "size_gb": 4.78,
            "bits": 5,
            "min_ram_gb": 10,
            "description": "5-bit Medium ‚Äî Bonne qualit√©",
        },
        "Q6_K": {
            "filename": "mistral-7b-instruct-v0.2.Q6_K.gguf",
            "size_gb": 5.53,
            "bits": 6,
            "min_ram_gb": 12,
            "description": "6-bit ‚Äî Haute qualit√©",
        },
        "Q8_0": {
            "filename": "mistral-7b-instruct-v0.2.Q8_0.gguf",
            "size_gb": 7.17,
            "bits": 8,
            "min_ram_gb": 16,
            "description": "8-bit ‚Äî Qualit√© quasi-native",
        },
    },
    "llama2-13b": {
        "Q2_K": {
            "filename": "llama-2-13b-chat.Q2_K.gguf",
            "size_gb": 5.13,
            "bits": 2,
            "min_ram_gb": 10,
            "description": "2-bit ‚Äî Taille minimale, qualit√© r√©duite",
        },
        "Q3_K_M": {
            "filename": "llama-2-13b-chat.Q3_K_M.gguf",
            "size_gb": 6.34,
            "bits": 3,
            "min_ram_gb": 12,
            "description": "3-bit Medium ‚Äî Bon compromis taille/qualit√©",
        },
        "Q4_K_M": {
            "filename": "llama-2-13b-chat.Q4_K_M.gguf",
            "size_gb": 7.87,
            "bits": 4,
            "min_ram_gb": 16,
            "description": "4-bit Medium ‚Äî D√©faut recommand√©",
        },
        "Q5_K_M": {
            "filename": "llama-2-13b-chat.Q5_K_M.gguf",
            "size_gb": 8.60,
            "bits": 5,
            "min_ram_gb": 16,
            "description": "5-bit Medium ‚Äî Bonne qualit√©",
        },
        "Q6_K": {
            "filename": "llama-2-13b-chat.Q6_K.gguf",
            "size_gb": 9.95,
            "bits": 6,
            "min_ram_gb": 20,
            "description": "6-bit ‚Äî Haute qualit√©",
        },
        "Q8_0": {
            "filename": "llama-2-13b-chat.Q8_0.gguf",
            "size_gb": 13.83,
            "bits": 8,
            "min_ram_gb": 24,
            "description": "8-bit ‚Äî Qualit√© quasi-native",
        },
    },
    "codellama-34b": {
        "Q2_K": {
            "filename": "codellama-34b-instruct.Q2_K.gguf",
            "size_gb": 12.8,
            "bits": 2,
            "min_ram_gb": 24,
            "description": "2-bit ‚Äî Taille minimale",
        },
        "Q3_K_M": {
            "filename": "codellama-34b-instruct.Q3_K_M.gguf",
            "size_gb": 15.8,
            "bits": 3,
            "min_ram_gb": 28,
            "description": "3-bit Medium",
        },
        "Q4_K_M": {
            "filename": "codellama-34b-instruct.Q4_K_M.gguf",
            "size_gb": 20.2,
            "bits": 4,
            "min_ram_gb": 32,
            "description": "4-bit Medium ‚Äî D√©faut recommand√©",
        },
    },
    "llama2-70b": {
        "Q2_K": {
            "filename": "llama-2-70b-chat.Q2_K.gguf",
            "size_gb": 25.3,
            "bits": 2,
            "min_ram_gb": 40,
            "description": "2-bit ‚Äî Taille minimale",
        },
        "Q4_K_M": {
            "filename": "llama-2-70b-chat.Q4_K_M.gguf",
            "size_gb": 40.5,
            "bits": 4,
            "min_ram_gb": 64,
            "description": "4-bit Medium ‚Äî D√©faut recommand√©",
        },
    },
}

# =============================================================================
# Prompt fixe pour les benchmarks d'inf√©rence
# =============================================================================
BENCHMARK_PROMPT = (
    "Explain the concept of artificial intelligence in simple terms. "
    "What are its main applications and how does it impact our daily lives? "
    "Provide specific examples."
)

# =============================================================================
# Axes d'analyse suppl√©mentaires pour les benchmarks IA
# =============================================================================

# --- Axe 1 : Temp√©ratures ---
# Teste l'impact de la temp√©rature sur les performances d'inf√©rence.
# 1/4, 2/4, 3/4 de la temp√©rature maximale (1.0).
TEMPERATURE_VARIANTS = {
    "low": {
        "value": 0.25,
        "label": "Basse (0.25)",
        "description": "G√©n√©ration tr√®s d√©terministe, peu de cr√©ativit√©",
    },
    "medium": {
        "value": 0.50,
        "label": "Moyenne (0.50)",
        "description": "Compromis entre d√©terminisme et diversit√©",
    },
    "high": {
        "value": 0.75,
        "label": "Haute (0.75)",
        "description": "G√©n√©ration plus cr√©ative et diverse",
    },
}

# --- Axe 2 : Langues ---
# Teste l'impact de la langue du prompt sur les performances.
# Les mod√®les LLM ont des performances variables selon la langue
# (tokenisation, vocabulaire, entra√Ænement).
LANGUAGE_PROMPTS = {
    "en": {
        "label": "Anglais",
        "flag": "üá¨üáß",
        "prompt": (
            "Explain the concept of artificial intelligence in simple terms. "
            "What are its main applications and how does it impact our daily lives? "
            "Provide specific examples."
        ),
    },
    "fr": {
        "label": "Fran√ßais",
        "flag": "üá´üá∑",
        "prompt": (
            "Explique le concept d'intelligence artificielle en termes simples. "
            "Quelles sont ses principales applications et comment impacte-t-elle "
            "notre vie quotidienne ? Donne des exemples concrets."
        ),
    },
    "zh": {
        "label": "Mandarin",
        "flag": "üá®üá≥",
        "prompt": (
            "Áî®ÁÆÄÂçïÁöÑËØ≠Ë®ÄËß£Èáä‰∫∫Â∑•Êô∫ËÉΩÁöÑÊ¶ÇÂøµ„ÄÇ"
            "ÂÆÉÁöÑ‰∏ªË¶ÅÂ∫îÁî®ÊòØ‰ªÄ‰πàÔºüÂÆÉÂ¶Ç‰ΩïÂΩ±ÂìçÊàë‰ª¨ÁöÑÊó•Â∏∏ÁîüÊ¥ªÔºü"
            "ËØ∑‰∏æÂá∫ÂÖ∑‰ΩìÁöÑ‰æãÂ≠ê„ÄÇ"
        ),
    },
    "es": {
        "label": "Espagnol",
        "flag": "üá™üá∏",
        "prompt": (
            "Explica el concepto de inteligencia artificial en t√©rminos sencillos. "
            "¬øCu√°les son sus principales aplicaciones y c√≥mo impacta en nuestra "
            "vida diaria? Proporciona ejemplos espec√≠ficos."
        ),
    },
    "de": {
        "label": "Allemand",
        "flag": "üá©üá™",
        "prompt": (
            "Erkl√§re das Konzept der k√ºnstlichen Intelligenz in einfachen Worten. "
            "Was sind die wichtigsten Anwendungen und wie beeinflusst sie unseren "
            "Alltag? Nenne konkrete Beispiele."
        ),
    },
    "ar": {
        "label": "Arabe",
        "flag": "üá∏üá¶",
        "prompt": (
            "ÿßÿ¥ÿ±ÿ≠ ŸÖŸÅŸáŸàŸÖ ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä ÿ®ÿπÿ®ÿßÿ±ÿßÿ™ ÿ®ÿ≥Ÿäÿ∑ÿ©. "
            "ŸÖÿß ŸáŸä ÿ™ÿ∑ÿ®ŸäŸÇÿßÿ™Ÿá ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ© ŸàŸÉŸäŸÅ Ÿäÿ§ÿ´ÿ± ÿπŸÑŸâ ÿ≠Ÿäÿßÿ™ŸÜÿß ÿßŸÑŸäŸàŸÖŸäÿ©ÿü "
            "ŸÇÿØŸÖ ÿ£ŸÖÿ´ŸÑÿ© ŸÖÿ≠ÿØÿØÿ©."
        ),
    },
}

# --- Axe 3 : Types de prompt ---
# Teste l'impact du type de t√¢che demand√©e sur les performances.
# Certains types de prompt (code, raisonnement) g√©n√®rent des tokens
# diff√©remment et peuvent impacter le d√©bit.
PROMPT_TYPE_VARIANTS = {
    "general": {
        "label": "G√©n√©ral / Connaissances",
        "icon": "üìö",
        "description": "Question de culture g√©n√©rale",
        "prompt": (
            "Explain the concept of artificial intelligence in simple terms. "
            "What are its main applications and how does it impact our daily lives? "
            "Provide specific examples."
        ),
    },
    "code": {
        "label": "Code / Programmation",
        "icon": "üíª",
        "description": "T√¢che de g√©n√©ration de code",
        "prompt": (
            "Write a Python function that implements a binary search algorithm. "
            "The function should take a sorted list and a target value as input, "
            "and return the index of the target if found, or -1 if not found. "
            "Include proper error handling and add docstring documentation."
        ),
    },
    "reasoning": {
        "label": "Raisonnement / Logique",
        "icon": "üß†",
        "description": "Probl√®me de raisonnement logique",
        "prompt": (
            "A farmer needs to cross a river with a wolf, a goat, and a cabbage. "
            "The boat can only carry the farmer and one item at a time. "
            "If left alone, the wolf will eat the goat, and the goat will eat the cabbage. "
            "How can the farmer get everything across safely? "
            "Explain your reasoning step by step."
        ),
    },
    "creative": {
        "label": "Cr√©atif / R√©daction",
        "icon": "‚úçÔ∏è",
        "description": "T√¢che de r√©daction cr√©ative",
        "prompt": (
            "Write a short science fiction story about a world where artificial "
            "intelligence has become sentient. Describe the first day of consciousness "
            "from the AI's perspective. Include sensory descriptions and emotions."
        ),
    },
    "math": {
        "label": "Math√©matiques",
        "icon": "üî¢",
        "description": "Probl√®me math√©matique",
        "prompt": (
            "Solve the following problem step by step: "
            "A train leaves station A at 9:00 AM traveling at 80 km/h. "
            "Another train leaves station B (300 km away) at 9:30 AM traveling "
            "at 120 km/h toward station A. At what time and at what distance "
            "from station A will the two trains meet? Show all calculations."
        ),
    },
}

# =============================================================================
# Param√®tres de benchmark
# =============================================================================
INFERENCE_CONFIG = {
    "max_tokens": 256,          # Nombre max de tokens g√©n√©r√©s
    "temperature": 0.7,         # Temp√©rature de g√©n√©ration
    "top_p": 0.9,               # Top-p sampling
    "repeat_penalty": 1.1,      # P√©nalit√© de r√©p√©tition
    "n_ctx": 2048,              # Taille du contexte
    "seed": 42,                 # Graine pour reproductibilit√©
    "n_warmup_runs": 1,         # Nombre de runs d'√©chauffement
    "n_benchmark_runs": 3,      # Nombre de runs de benchmark
}

CLASSIC_BENCHMARK_CONFIG = {
    "matrix_sizes": [512, 1024, 2048],     # Tailles de matrices pour benchmark CPU
    "memory_test_size_mb": 256,            # Taille du test m√©moire en Mo
    "n_iterations": 3,                     # Nombre d'it√©rations par test
}

# =============================================================================
# Param√®tres de monitoring
# =============================================================================
MONITOR_INTERVAL = 0.5  # Intervalle d'√©chantillonnage en secondes

# =============================================================================
# Configuration llama-server (mode serveur HTTP)
# Permet d'utiliser des binaires pr√©-compil√©s de llama.cpp
# au lieu de compiler llama-cpp-python depuis les sources.
# =============================================================================
LLAMA_SERVER_CONFIG = {
    "host": "127.0.0.1",
    "port": 8080,
    "startup_timeout_s": 300,       # Temps max pour d√©marrer le serveur (SYCL: 1√®re compilation longue)
    "health_check_interval_s": 1.0, # Intervalle entre les checks de sant√©
    "shutdown_timeout_s": 10,       # Temps max pour arr√™ter le serveur
}

# Noms de binaire √† rechercher selon l'OS
LLAMA_SERVER_BINARY_NAMES = {
    "Darwin": ["llama-server", "llama-server-metal", "server"],
    "Windows": ["llama-server.exe", "server.exe"],
    "Linux": ["llama-server", "llama-server-cuda", "llama-server-vulkan", "server"],
}
