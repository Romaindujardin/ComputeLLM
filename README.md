# üñ•Ô∏è ComputeLLM ‚Äî AI Hardware Benchmark Tool

**Outil de benchmark multiplateforme d√©di√© au Hardware IA, avec focus sur l'inf√©rence locale de LLM.**

ComputeLLM permet de comparer les performances mat√©rielles (CPU, GPU, RAM) de diff√©rentes machines lors de l'inf√©rence locale de mod√®les de langage, en mettant en √©vidence les diff√©rences d'architecture :

| Architecture           | Exemple                     | Backend        |
| ---------------------- | --------------------------- | -------------- |
| x86 + GPU d√©di√© NVIDIA | Intel/AMD CPU + NVIDIA RTX  | CUDA           |
| x86 + GPU d√©di√© AMD    | Intel/AMD CPU + Radeon RX   | ROCm           |
| ARM + M√©moire unifi√©e  | Apple Silicon (M1/M2/M3/M4) | Metal          |
| CPU seul               | Tout processeur             | CPU (fallback) |

---

## Fonctionnalit√©s

- **D√©tection mat√©rielle automatique** : OS, CPU (mod√®le, c≈ìurs, fr√©quence), GPU (VRAM, backend), RAM (totale, disponible, unifi√©e)
- **Benchmarks classiques** : CPU single-thread, CPU multi-thread, bande passante m√©moire, GPU compute
- **Benchmarks IA** : Inf√©rence locale de LLM via `llama-cpp-python` (GGUF) ou `llama-server` (HTTP)
- **Mode llama-server** : Utilise des binaires pr√©-compil√©s ‚Äî aucune compilation requise c√¥t√© Python
- **Mod√®les support√©s** : TinyLlama 1.1B, Mistral 7B, Llama 2 13B, CodeLlama 34B, Llama 2 70B
- **T√©l√©chargement automatique** depuis Hugging Face
- **M√©triques mesur√©es** : tokens/s, latence du 1er token, m√©moire utilis√©e, stabilit√©
- **Interface graphique** Streamlit avec bouton unique de lancement
- **Comparaison multi-machines** avec graphiques interactifs
- **Export** JSON et CSV

---

## Architecture

```
ComputeLLM/
‚îú‚îÄ‚îÄ main.py                    # Point d'entr√©e (CLI + GUI)
‚îú‚îÄ‚îÄ app.py                     # Application Streamlit (GUI)
‚îú‚îÄ‚îÄ requirements.txt           # D√©pendances Python
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py              # Configuration et constantes
‚îÇ   ‚îú‚îÄ‚îÄ hardware_detect.py     # D√©tection mat√©rielle
‚îÇ   ‚îú‚îÄ‚îÄ benchmark_classic.py   # Benchmarks CPU/GPU/RAM
‚îÇ   ‚îú‚îÄ‚îÄ benchmark_ai.py        # Benchmarks inf√©rence LLM
‚îÇ   ‚îú‚îÄ‚îÄ llama_server.py        # Gestionnaire llama-server (HTTP)
‚îÇ   ‚îî‚îÄ‚îÄ results_manager.py     # Sauvegarde et comparaison
‚îú‚îÄ‚îÄ models/                    # Mod√®les GGUF t√©l√©charg√©s
‚îî‚îÄ‚îÄ results/                   # R√©sultats de benchmark (JSON)
```

---

## Installation

### Pr√©requis

- Python 3.10 ou sup√©rieur
- pip

### 1. Cloner le d√©p√¥t

```bash
git clone https://github.com/Romaindujardin/ComputeLLM.git
cd ComputeLLM
```

### 2. Cr√©er un environnement virtuel

```bash
python3 -m venv .venv
```

## Activer l'environnement - MacOS

```bash
source .venv/bin/activate
```

## Activer l'environnement - Windows

```bash
.venv\Scripts\activate
```

### 2. Installer les d√©pendances de base

```bash
pip install --upgrade pip setuptools wheel
```

```bash
pip install -r requirements.txt
```

### 3. Installer llama-cpp-python (selon votre mat√©riel)

> **Alternative sans compilation** : voir la section [Mode llama-server](#mode-llama-server-sans-compilation) ci-dessous.

#### macOS (Apple Silicon ‚Äî Metal)

```bash
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python
```

#### Windows / Linux (NVIDIA GPU ‚Äî CUDA)

```bash
set CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python
```

#### Linux (AMD GPU ‚Äî ROCm / HIP)

```bash
CMAKE_ARGS="-DGGML_HIPBLAS=on" pip install llama-cpp-python
```

#### CPU uniquement (fallback)

```bash
pip install llama-cpp-python
```

### 4. (Optionnel) Installer PyTorch pour les benchmarks GPU classiques

#### macOS

```bash
pip install torch torchvision
```

#### Windows / Linux (NVIDIA ‚Äî CUDA)

```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

#### Linux (AMD GPU ‚Äî ROCm)

Pour les GPU AMD (Radeon RX, Instinct), installer la version ROCm de PyTorch :

```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.2
```

#### Windows / Linux (Intel GPU ‚Äî XPU)

Pour les GPU Intel (Iris Xe, Arc), installer la version XPU de PyTorch :

```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/xpu
```

### 5. Mode llama-server (sans compilation)

Si vous ne pouvez pas (ou ne voulez pas) compiler `llama-cpp-python` depuis les sources (probl√®mes de Visual Studio, oneAPI Toolkit, etc.), vous pouvez utiliser le **mode llama-server** :

1. **T√©l√©charger le binaire pr√©-compil√©** depuis les [releases de llama.cpp](https://github.com/ggerganov/llama.cpp/releases) :
   - Windows CUDA : `llama-*-bin-win-cuda-*`
   - Windows Vulkan : `llama-*-bin-win-vulkan-*`
   - Windows SYCL : `llama-*-bin-win-sycl-*`
   - Linux CUDA : `llama-*-bin-ubuntu-*-cuda-*`
   - macOS Metal : `llama-*-bin-macos-*`

2. **Extraire l'archive** et rep√©rer le binaire `llama-server` (ou `llama-server.exe` sur Windows)

3. **Dans ComputeLLM**, activer le toggle **"Utiliser llama-server"** dans la page Benchmark, puis configurer :
   - **Mode Auto** : indiquer le chemin vers le binaire `llama-server`. ComputeLLM d√©marre/arr√™te le serveur automatiquement.
   - **Mode Manuel** : lancer `llama-server` vous-m√™me, puis renseigner l'adresse (ex: `127.0.0.1:8080`).

#### Lancement manuel de llama-server

```bash
# Exemple : lancer le serveur avec un mod√®le GGUF
./llama-server -m models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf --port 8080 -ngl -1
```

> Le serveur expose une API compatible OpenAI sur `http://127.0.0.1:8080`. ComputeLLM s'y connecte automatiquement pour les benchmarks.

---

## Utilisation

### Interface graphique (recommand√©)

```bash
python main.py --gui
```

ou simplement :

```bash
python main.py
```

L'interface Streamlit s'ouvre dans votre navigateur avec 3 pages :

1. **Mat√©riel** ‚Äî D√©tection et affichage de la configuration
2. **Benchmark** ‚Äî Lancement avec un seul bouton
3. **R√©sultats** ‚Äî Visualisation, comparaison et export

### Ligne de commande (CLI)

```bash
# Tous les benchmarks
python main.py --cli

# Mod√®le sp√©cifique
python main.py --cli --models tinyllama-1.1b mistral-7b

# Benchmarks classiques uniquement
python main.py --cli --skip-ai

# Benchmarks IA uniquement
python main.py --cli --skip-classic

# D√©tecter le mat√©riel uniquement
python main.py --detect
```

---

## Mod√®les disponibles

| Cl√©              | Mod√®le                 | Param√®tres | Taille (Q4_K_M) | RAM min |
| ---------------- | ---------------------- | ---------- | --------------- | ------- |
| `tinyllama-1.1b` | TinyLlama 1.1B Chat    | 1.1B       | 0.7 Go          | 2 Go    |
| `mistral-7b`     | Mistral 7B Instruct    | 7B         | 4.4 Go          | 8 Go    |
| `llama2-13b`     | Llama 2 13B Chat       | 13B        | 7.9 Go          | 16 Go   |
| `codellama-34b`  | CodeLlama 34B Instruct | 34B        | 20.2 Go         | 32 Go   |
| `llama2-70b`     | Llama 2 70B Chat       | 70B        | 40.5 Go         | 64 Go   |

Les mod√®les sont t√©l√©charg√©s automatiquement depuis Hugging Face lors du premier benchmark.

---

## M√©triques mesur√©es

### Benchmarks classiques

| M√©trique              | Description                      |
| --------------------- | -------------------------------- |
| GFLOPS (ST)           | Performance CPU single-thread    |
| GFLOPS (MT)           | Performance CPU multi-thread     |
| Bande passante (Go/s) | Lecture, √©criture, copie m√©moire |
| GFLOPS GPU            | Performance GPU (CUDA/Metal)     |

### Benchmarks IA

| M√©trique                 | Description                    |
| ------------------------ | ------------------------------ |
| Tokens/s                 | D√©bit d'inf√©rence              |
| Latence 1er token (s)    | Temps avant le premier token   |
| Latence inter-token (ms) | Temps moyen entre chaque token |
| M√©moire pic (Go)         | M√©moire maximale utilis√©e      |
| Stabilit√©                | Nombre de runs r√©ussis / total |

### Monitoring en temps r√©el

| M√©trique        | Description                    |
| --------------- | ------------------------------ |
| CPU %           | Utilisation CPU (moyenne, max) |
| RAM Go          | Utilisation RAM (pic)          |
| GPU %           | Utilisation GPU (si NVIDIA)    |
| Temp√©rature GPU | Si disponible                  |

---

## Pipeline d√©taill√© du benchmark

Voici le d√©roulement complet d'un benchmark, √©tape par √©tape.

### Nettoyage syst√®me (`system_cleanup`)

Avant **chaque phase**, un nettoyage complet est effectu√© :

1. **Garbage collector Python** ‚Äî 3 g√©n√©rations (`gc.collect(generation)` pour 0, 1, 2)
2. **Purge cache GPU** ‚Äî `torch.cuda.empty_cache()` (NVIDIA), `torch.xpu.empty_cache()` (Intel), ou `torch.mps.empty_cache()` (Apple)
3. **Purge cache OS** ‚Äî macOS : `sudo purge` ou `memory_pressure -l warn` ¬∑ Linux : `drop_caches`
4. **Pause** ‚Äî 1 seconde de stabilisation

### Monitoring en temps r√©el (`ResourceMonitor`)

Un thread de monitoring tourne en arri√®re-plan pendant chaque phase :

- **Fr√©quence** : 1 √©chantillon toutes les **0,5 s**
- **M√©triques** : CPU % (`psutil`), RAM utilis√©e (Go), GPU % + VRAM + temp√©rature (`nvidia-smi` ou `xpu-smi`)
- **R√©sum√©** : moyenne, min, max CPU ¬∑ pic RAM ¬∑ pic GPU/VRAM/temp√©rature

---

### Phase 1 ‚Äî CPU Single-Thread

| Param√®tre      | Valeur                                                                                                                  |
| -------------- | ----------------------------------------------------------------------------------------------------------------------- |
| Outil          | `numpy.dot` (float32)                                                                                                   |
| Tailles        | 512√ó512, 1024√ó1024, 2048√ó2048                                                                                           |
| It√©rations     | 3 par taille                                                                                                            |
| Thread forcing | `OMP_NUM_THREADS=1`, `MKL_NUM_THREADS=1`, `OPENBLAS_NUM_THREADS=1`, `VECLIB_MAXIMUM_THREADS=1`, `NUMEXPR_NUM_THREADS=1` |

**Calcul** : pour chaque taille $N$, on mesure le temps moyen sur 3 runs, puis :

$$\text{GFLOPS} = \frac{2 \times N^3}{\text{temps\_moyen} \times 10^9}$$

---

### Phase 2 ‚Äî CPU Multi-Thread

| Param√®tre  | Valeur                                          |
| ---------- | ----------------------------------------------- |
| Outil      | `numpy.dot` (float32)                           |
| Tailles    | 512√ó512, 1024√ó1024, 2048√ó2048                   |
| It√©rations | 3 par taille                                    |
| Threads    | Tous les c≈ìurs disponibles (via MKL / OpenBLAS) |

M√™me formule GFLOPS que la phase 1, mais avec tous les threads actifs.

---

### Phase 3 ‚Äî Bande passante m√©moire (RAM)

| Param√®tre      | Valeur          |
| -------------- | --------------- |
| Outil          | NumPy (float64) |
| Taille du bloc | 256 Mo          |
| It√©rations     | 3 par op√©ration |

Trois op√©rations sont mesur√©es :

| Op√©ration | Code                  | Description                 |
| --------- | --------------------- | --------------------------- |
| √âcriture  | `np.ones(n, float64)` | Allocation et remplissage   |
| Lecture   | `np.sum(a)`           | Parcours complet du tableau |
| Copie     | `a.copy()`            | Duplication en m√©moire      |

**Calcul** :

$$\text{Bande passante (Go/s)} = \frac{\text{taille\_donn√©es\_Go}}{\text{temps\_moyen}}$$

---

### Phase 4 ‚Äî GPU Compute

| Param√®tre | Valeur                                         |
| --------- | ---------------------------------------------- |
| Outil     | `torch.mm` (PyTorch, float32)                  |
| Tailles   | 1024√ó1024, 2048√ó2048, 4096√ó4096                |
| Warmup    | 1 run (non comptabilis√©)                       |
| Runs      | 3 par taille                                   |
| Backends  | CUDA (NVIDIA) ¬∑ SYCL/XPU (Intel) ¬∑ MPS (Apple) |

**D√©roulement** :

1. D√©tection automatique du backend GPU (priorit√© : CUDA ‚Üí XPU/IPEX ‚Üí MPS)
2. Warmup : 1 multiplication non chronom√©tr√©e pour initialiser le GPU
3. Pour chaque taille, 3 runs chronom√©tr√©s avec synchronisation (`torch.cuda.synchronize()` / `torch.xpu.synchronize()` / `torch.mps.synchronize()`)
4. Calcul GFLOPS identique aux phases CPU

**Protection Level Zero** : si le driver Intel crashe (`UR_RESULT_ERROR_UNKNOWN`), le benchmark GPU est ignor√© proprement avec un message d'aide.

Si aucun GPU compatible n'est d√©tect√©, la phase est marqu√©e ¬´ Ignor√© ¬ª avec un conseil contextuel (ex : ¬´ Vous avez un GPU Intel mais PyTorch est compil√© pour CUDA ¬ª).

---

### Phase 5 ‚Äî Inf√©rence IA (mod√®les LLM)

Pour **chaque mod√®le s√©lectionn√©** (ex : TinyLlama 1.1B, Mistral 7B‚Ä¶) :

#### 5.1 ‚Äî Pr√©-v√©rifications

- **RAM disponible** : si la RAM totale < RAM minimale requise par le mod√®le ‚Üí skip
- **T√©l√©chargement** : si le fichier GGUF n'est pas d√©j√† pr√©sent dans `models/`, il est t√©l√©charg√© automatiquement depuis Hugging Face

#### 5.2 ‚Äî D√©tection du backend

Ordre de priorit√© :

1. **NVIDIA** ‚Üí CUDA (`n_gpu_layers = -1`, toutes les couches sur GPU)
2. **Intel** ‚Üí SYCL (`n_gpu_layers = -1`, via `_detect_intel_gpu()` 4 m√©thodes)
3. **Apple** ‚Üí Metal (`n_gpu_layers = -1`)
4. **Aucun** ‚Üí CPU uniquement (`n_gpu_layers = 0`)

#### 5.3 ‚Äî Chargement du mod√®le

- Biblioth√®que : `llama-cpp-python` (mode natif) ou `llama-server` (mode serveur HTTP)
- Contexte : `n_ctx = 2048` tokens
- Seed : `42` (reproductibilit√©)
- Le temps de chargement est mesur√© (`model_load_time_s`)

#### 5.4 ‚Äî √âchauffement (warmup)

- **1 run** avec `max_tokens = 32` (r√©sultat ignor√©)
- But : initialiser les caches KV et le runtime GPU

#### 5.5 ‚Äî Runs de benchmark

- **3 runs** cons√©cutifs, chacun mesur√© individuellement
- Le `ResourceMonitor` tourne en arri√®re-plan pendant les 3 runs

**Param√®tres d'inf√©rence** (identiques pour chaque run) :

| Param√®tre        | Valeur                                                             |
| ---------------- | ------------------------------------------------------------------ |
| Prompt           | `"Explain the concept of artificial intelligence in simple terms"` |
| Format           | Chat completion (messages system + user)                           |
| `max_tokens`     | 256                                                                |
| `temperature`    | 0.7                                                                |
| `top_p`          | 0.9                                                                |
| `repeat_penalty` | 1.1                                                                |
| `seed`           | 42                                                                 |

#### 5.6 ‚Äî Mesures par run

Chaque run est en mode **streaming** (token par token). Les m√©triques mesur√©es :

| M√©trique                     | M√©thode de mesure                                                          |
| ---------------------------- | -------------------------------------------------------------------------- |
| **Latence 1er token** (s)    | `time.perf_counter()` entre le d√©but et la r√©ception du 1er token non-vide |
| **Tokens/s**                 | `tokens_g√©n√©r√©s / temps_total`                                             |
| **Latence inter-token** (ms) | Moyenne des deltas `time.perf_counter()` entre tokens cons√©cutifs (√ó 1000) |
| **P90 inter-token** (ms)     | 90e percentile des deltas inter-token                                      |
| **M√©moire avant/apr√®s** (Go) | `psutil.Process().memory_info().rss` converti en Go                        |

#### 5.7 ‚Äî Agr√©gation des r√©sultats

Sur les **3 runs** (ou ceux ayant r√©ussi) :

| Statistique                 | Calcul                                         |
| --------------------------- | ---------------------------------------------- |
| `avg_tokens_per_second`     | Moyenne des tokens/s des runs r√©ussis          |
| `std_tokens_per_second`     | √âcart-type des tokens/s                        |
| `avg_first_token_latency_s` | Moyenne des latences 1er token                 |
| `peak_memory_gb`            | Maximum de `memory_after_gb` sur tous les runs |
| `stability`                 | `"stable"` si 3/3 r√©ussis, `"unstable"` sinon  |

---

### Phase 6 ‚Äî Comparaison des quantifications

Pour chaque mod√®le s√©lectionn√©, si plusieurs quantifications sont disponibles (ex : Q2_K, Q3_K_M, Q4_K_M, Q5_K_M, Q6_K, Q8_0) :

1. **Nettoyage syst√®me** entre chaque variante
2. M√™me pipeline que la phase 5 (download ‚Üí load ‚Üí warmup ‚Üí 3 runs)
3. Mesures additionnelles par quantification :

| M√©trique               | Description                                    |
| ---------------------- | ---------------------------------------------- |
| `actual_file_size_gb`  | Taille r√©elle du fichier GGUF sur disque       |
| `model_load_time_s`    | Temps de chargement du mod√®le                  |
| `memory_after_load_gb` | M√©moire RSS apr√®s chargement (avant inf√©rence) |

**Tableau comparatif** g√©n√©r√© automatiquement avec : tokens/s, latence 1er token, latence inter-token, m√©moire pic, temps de chargement, stabilit√© ‚Äî pour chaque quantification.

---

### Phase 7 ‚Äî Sauvegarde des r√©sultats

- **Format** : JSON structur√© dans `results/benchmark_{CPU}_{OS}_{timestamp}.json`
- **Contenu** : hardware d√©tect√©, r√©sultats classiques, r√©sultats IA, monitoring, comparaisons de quantification
- **Export** : CSV disponible depuis l'interface Streamlit

---

### Phase 8 ‚Äî Comparaison de temp√©ratures üå°Ô∏è

Pour chaque mod√®le s√©lectionn√©, teste l'impact de la temp√©rature sur les performances d'inf√©rence.

**Variants test√©s** :
| Cl√© | Temp√©rature | Description |
| -------- | ----------- | -------------------------------- |
| `low` | 0.25 | R√©ponses d√©terministes, pr√©cises |
| `medium` | 0.50 | √âquilibre pr√©cision/cr√©ativit√© |
| `high` | 0.75 | R√©ponses plus cr√©atives/vari√©es |

**M√©thodologie** :

1. Chargement du mod√®le une seule fois (ou d√©marrage du serveur)
2. Phase de chauffe (1 run)
3. Pour chaque temp√©rature : 3 runs de benchmark avec le m√™me prompt, seule la temp√©rature change
4. Agr√©gation : moyenne et √©cart-type de tokens/s, first-token latency, inter-token latency, m√©moire pic, stabilit√©

**Tableau comparatif** : tokens/s, latence inter-token, m√©moire pic ‚Äî par variante de temp√©rature. Graphiques Plotly dans l'interface.

---

### Phase 9 ‚Äî Comparaison multilingue üåç

√âvalue si la langue du prompt impacte les performances d'inf√©rence.

**Langues test√©es** :
| Cl√© | Langue | Drapeau |
| ---- | -------- | ------- |
| `en` | Anglais | üá¨üáß |
| `fr` | Fran√ßais | üá´üá∑ |
| `zh` | Mandarin | üá®üá≥ |
| `es` | Espagnol | üá™üá∏ |
| `de` | Allemand | üá©üá™ |
| `ar` | Arabe | üá∏üá¶ |

**M√©thodologie** :

1. Chargement du mod√®le une seule fois
2. Phase de chauffe
3. Pour chaque langue : 3 runs avec la m√™me question traduite dans la langue cible (temp√©rature fixe)
4. Agr√©gation identique √† la Phase 8

**Objectif** : D√©tecter si la tokenisation de certaines langues (chinois, arabe) impacte le d√©bit en tokens/s ou la latence.

---

### Phase 10 ‚Äî Comparaison par type de prompt üìù

Mesure l'impact du type de t√¢che demand√©e sur les performances d'inf√©rence.

**Types de prompt test√©s** :
| Cl√© | Type | Ic√¥ne | Description |
| ----------- | ---------- | ----- | ----------------------------------- |
| `general` | G√©n√©ral | üí¨ | Question de culture g√©n√©rale |
| `code` | Code | üíª | G√©n√©ration de fonction Python |
| `reasoning` | R√©flexion | üß† | Raisonnement logique / puzzle |
| `creative` | Cr√©atif | üé® | √âcriture cr√©ative (po√®me, histoire) |
| `math` | Maths | üî¢ | R√©solution de probl√®me math√©matique |

**M√©thodologie** :

1. Chargement du mod√®le une seule fois
2. Phase de chauffe
3. Pour chaque type : 3 runs avec un prompt d√©di√© au type de t√¢che (temp√©rature fixe)
4. Agr√©gation identique aux phases pr√©c√©dentes

**Objectif** : Identifier si certains types de prompts (code vs cr√©atif) provoquent des diff√©rences de performance significatives (longueur de g√©n√©ration, patterns de tokens).

---

## Exemple de r√©sultat (JSON)

```json
{
  "id": "20260206_143022",
  "timestamp": "2026-02-06T14:30:22",
  "hardware": {
    "os": { "system": "Darwin", "machine": "arm64" },
    "cpu": { "model": "Apple M2 Pro", "physical_cores": 12 },
    "gpu": { "primary_backend": "metal" },
    "ram": { "total_gb": 32.0, "unified_memory": true }
  },
  "classic_benchmarks": {
    "benchmarks": {
      "cpu_multi_thread": {
        "results": { "2048x2048": { "gflops": 312.5 } }
      }
    }
  },
  "ai_benchmarks": {
    "results": {
      "tinyllama-1.1b": {
        "summary": {
          "avg_tokens_per_second": 85.3,
          "avg_first_token_latency_s": 0.12,
          "stability": "stable"
        }
      }
    }
  }
}
```

---

## Configuration avanc√©e

Modifiez `src/config.py` pour ajuster :

- **Mod√®les** : Ajouter/supprimer des mod√®les GGUF
- **Prompt** : Modifier le prompt de benchmark
- **Param√®tres d'inf√©rence** : `max_tokens`, `temperature`, nombre de runs
- **Tailles de matrices** : Pour les benchmarks CPU
- **Intervalle de monitoring** : Fr√©quence d'√©chantillonnage

---

## To-Do

### Haute priorit√©

- [x] **D√©tection mat√©rielle** ‚Äî ‚úÖ Support AMD complet
  - ~~Ajouter la d√©tection des GPU AMD~~ ‚úîÔ∏è
    - `rocm-smi` (Linux) ‚úîÔ∏è
    - `lspci` (fallback Linux) ‚úîÔ∏è
    - WMI / Win32_VideoController (Windows) ‚úîÔ∏è
    - PyTorch ROCm (`torch.version.hip`) ‚úîÔ∏è
  - ~~Ajouter la d√©tection des GPU Intel (Arc / XPU)~~ ‚úîÔ∏è
  - ~~Ajouter les backends d√©tect√©s : `rocm`, `xpu`, `sycl`~~ ‚úîÔ∏è

- [x] **Benchmark classique GPU** ‚Äî ‚úÖ Support AMD complet
  - ~~Support explicite AMD ROCm (identifier via `torch.version.hip`)~~ ‚úîÔ∏è
  - ~~Support Intel XPU (`torch.xpu.is_available()`)~~ ‚úîÔ∏è
  - ~~Synchronisation adapt√©e par device~~ ‚úîÔ∏è (ROCm utilise `torch.cuda.synchronize()`)
  - ~~Ajouter monitoring AMD : `rocm-smi --showuse --showmemuse --showtemp`~~ ‚úîÔ∏è
  - ~~Distinguer CUDA vs ROCm dans l‚Äôaffichage des r√©sultats~~ ‚úîÔ∏è

- [ ] **Benchmark AI / LLM**
  - √âtendre `detect_best_backend()` :
    - ~~ROCm (HIPBLAS)~~ ‚úîÔ∏è
    - Vulkan
    - ~~SYCL (Intel)~~ ‚úîÔ∏è
    - CLBlast / OpenCL (fallback g√©n√©rique)
  - G√©rer explicitement les backends llama-cpp-python :
    - `-DGGML_CUDA=on`
    - `-DGGML_HIPBLAS=on`
    - `-DGGML_VULKAN=on`
    - `-DGGML_SYCL=on`
    - `-DGGML_CLBLAST=on`
  - V√©rifier √† l‚Äôex√©cution que le backend compil√© correspond au GPU d√©tect√© - ~~Mode llama-server (binaires pr√©-compil√©s, z√©ro compilation)~~ ‚úÖ

---

### Priorit√© moyenne

- [ ] **Installation guid√©e de llama-cpp-python**
  - D√©tecter automatiquement le GPU au premier lancement
  - Proposer la bonne commande d‚Äôinstallation selon la plateforme
  - Ajouter un script d‚Äôinstallation automatique par OS
  - Afficher un avertissement si version CPU-only d√©tect√©e - ~~Mode llama-server comme alternative sans compilation~~ ‚úÖ
- [ ] **Monitoring unifi√©**
  - ~~Agr√©ger les m√©triques NVIDIA / AMD / Intel dans `ResourceMonitor`~~ ‚úîÔ∏è
  - Normaliser le format des m√©triques (utilisation %, VRAM, temp√©rature)

---

### Priorit√© basse

- [ ] **Support multi-GPU**
  - D√©tection de plusieurs GPU
  - S√©lection via :
    - `CUDA_VISIBLE_DEVICES`
    - `HIP_VISIBLE_DEVICES`
    - `ZE_AFFINITY_MASK`
  - Benchmark individuel par GPU

- [ ] **Gestion RAM vs VRAM**
  - Distinguer RAM syst√®me et VRAM GPU
  - Adapter `get_compatible_quantizations()` en fonction de la VRAM
  - Recommandations dynamiques de mod√®les selon m√©moire disponible

- [ ] **Backend Vulkan universel**
  - Ajouter d√©tection via `vulkaninfo`
  - Documenter Vulkan comme backend cross-vendor (AMD / Intel / NVIDIA)

- [ ] **Documentation**
  - Ajouter tableau comparatif des backends support√©s par plateforme
  - Documenter les pr√©requis ROCm / SYCL / Vulkan
  - Ajouter exemples d‚Äôinstallation par architecture :
    - Windows x86 + NVIDIA
    - Linux + AMD ROCm
    - Windows/Linux + Vulkan
    - macOS ARM + Metal

---

## Notes importantes

- Les mod√®les GGUF sont t√©l√©charg√©s dans le dossier `models/` (plusieurs Go par mod√®le).
- L'inf√©rence de gros mod√®les (34B, 70B) n√©cessite beaucoup de RAM.
- Sur Apple Silicon, la m√©moire unifi√©e est partag√©e entre CPU et GPU.
- Les r√©sultats varient selon la charge syst√®me et la temp√©rature du processeur.
- Pour des r√©sultats reproductibles, fermez les applications gourmandes en ressources.
