# üñ•Ô∏è ComputeLLM ‚Äî AI Hardware Benchmark Tool

**Outil de benchmark multiplateforme d√©di√© au Hardware IA, avec focus sur l'inf√©rence locale de LLM.**

ComputeLLM permet de comparer les performances mat√©rielles (CPU, GPU, RAM) de diff√©rentes machines lors de l'inf√©rence locale de mod√®les de langage, en mettant en √©vidence les diff√©rences d'architecture :

| Architecture          | Exemple                     | Backend        |
| --------------------- | --------------------------- | -------------- |
| x86 + GPU d√©di√©       | Intel/AMD + NVIDIA RTX      | CUDA           |
| ARM + M√©moire unifi√©e | Apple Silicon (M1/M2/M3/M4) | Metal          |
| CPU seul              | Tout processeur             | CPU (fallback) |

---

## üìã Fonctionnalit√©s

- **D√©tection mat√©rielle automatique** : OS, CPU (mod√®le, c≈ìurs, fr√©quence), GPU (VRAM, backend), RAM (totale, disponible, unifi√©e)
- **Benchmarks classiques** : CPU single-thread, CPU multi-thread, bande passante m√©moire, GPU compute
- **Benchmarks IA** : Inf√©rence locale de LLM via `llama-cpp-python` (GGUF)
- **Mod√®les support√©s** : TinyLlama 1.1B, Mistral 7B, Llama 2 13B, CodeLlama 34B, Llama 2 70B
- **T√©l√©chargement automatique** depuis Hugging Face
- **M√©triques mesur√©es** : tokens/s, latence du 1er token, m√©moire utilis√©e, stabilit√©
- **Interface graphique** Streamlit avec bouton unique de lancement
- **Comparaison multi-machines** avec graphiques interactifs
- **Export** JSON et CSV

---

## üèóÔ∏è Architecture

```
ComputeLLM/
‚îú‚îÄ‚îÄ main.py                    # Point d'entr√©e (CLI + GUI)
‚îú‚îÄ‚îÄ app.py                     # Application Streamlit (GUI)
‚îú‚îÄ‚îÄ requirements.txt           # D√©pendances Python
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py              # Configuration et constantes
‚îÇ   ‚îú‚îÄ‚îÄ hardware_detect.py     # D√©tection mat√©rielle
‚îÇ   ‚îú‚îÄ‚îÄ benchmark_classic.py   # Benchmarks CPU/GPU/RAM
‚îÇ   ‚îú‚îÄ‚îÄ benchmark_ai.py        # Benchmarks inf√©rence LLM
‚îÇ   ‚îî‚îÄ‚îÄ results_manager.py     # Sauvegarde et comparaison
‚îú‚îÄ‚îÄ models/                    # Mod√®les GGUF t√©l√©charg√©s
‚îî‚îÄ‚îÄ results/                   # R√©sultats de benchmark (JSON)
```

---

## üöÄ Installation

### Pr√©requis

- Python 3.10 ou sup√©rieur
- pip

### 1. Cloner le d√©p√¥t

```bash
git clone https://github.com/Romaindujardin/ComputeLLM.git
cd ComputeLLM
```

### 2. Cr√©er un environnement virtuel

```bash
python3 -m venv .venv
```

## Activer l'environnement - MacOS

```bash
source .venv/bin/activate
```

## Activer l'environnement - Windows

```bash
.venv\Scripts\activate
```

### 2. Installer les d√©pendances de base

```bash
pip install --upgrade pip setuptools wheel
```

```bash
pip install -r requirements.txt
```

### 3. Installer llama-cpp-python (selon votre mat√©riel)

#### macOS (Apple Silicon ‚Äî Metal)

```bash
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python
```

#### Windows / Linux (NVIDIA GPU ‚Äî CUDA)

```bash
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python
```

#### CPU uniquement (fallback)

```bash
pip install llama-cpp-python
```

### 4. (Optionnel) Installer PyTorch pour les benchmarks GPU classiques

#### macOS

```bash
pip install torch torchvision
```

#### Windows / Linux (CUDA)

```bash
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
```

---

## üíª Utilisation

### Interface graphique (recommand√©)

```bash
python main.py --gui
```

ou simplement :

```bash
python main.py
```

L'interface Streamlit s'ouvre dans votre navigateur avec 3 pages :

1. **üè† Mat√©riel** ‚Äî D√©tection et affichage de la configuration
2. **üöÄ Benchmark** ‚Äî Lancement avec un seul bouton
3. **üìä R√©sultats** ‚Äî Visualisation, comparaison et export

### Ligne de commande (CLI)

```bash
# Tous les benchmarks
python main.py --cli

# Mod√®le sp√©cifique
python main.py --cli --models tinyllama-1.1b mistral-7b

# Benchmarks classiques uniquement
python main.py --cli --skip-ai

# Benchmarks IA uniquement
python main.py --cli --skip-classic

# D√©tecter le mat√©riel uniquement
python main.py --detect
```

---

## üì¶ Mod√®les disponibles

| Cl√©              | Mod√®le                 | Param√®tres | Taille (Q4_K_M) | RAM min |
| ---------------- | ---------------------- | ---------- | --------------- | ------- |
| `tinyllama-1.1b` | TinyLlama 1.1B Chat    | 1.1B       | 0.7 Go          | 2 Go    |
| `mistral-7b`     | Mistral 7B Instruct    | 7B         | 4.4 Go          | 8 Go    |
| `llama2-13b`     | Llama 2 13B Chat       | 13B        | 7.9 Go          | 16 Go   |
| `codellama-34b`  | CodeLlama 34B Instruct | 34B        | 20.2 Go         | 32 Go   |
| `llama2-70b`     | Llama 2 70B Chat       | 70B        | 40.5 Go         | 64 Go   |

Les mod√®les sont t√©l√©charg√©s automatiquement depuis Hugging Face lors du premier benchmark.

---

## üìä M√©triques mesur√©es

### Benchmarks classiques

| M√©trique              | Description                      |
| --------------------- | -------------------------------- |
| GFLOPS (ST)           | Performance CPU single-thread    |
| GFLOPS (MT)           | Performance CPU multi-thread     |
| Bande passante (Go/s) | Lecture, √©criture, copie m√©moire |
| GFLOPS GPU            | Performance GPU (CUDA/Metal)     |

### Benchmarks IA

| M√©trique                 | Description                    |
| ------------------------ | ------------------------------ |
| Tokens/s                 | D√©bit d'inf√©rence              |
| Latence 1er token (s)    | Temps avant le premier token   |
| Latence inter-token (ms) | Temps moyen entre chaque token |
| M√©moire pic (Go)         | M√©moire maximale utilis√©e      |
| Stabilit√©                | Nombre de runs r√©ussis / total |

### Monitoring en temps r√©el

| M√©trique        | Description                    |
| --------------- | ------------------------------ |
| CPU %           | Utilisation CPU (moyenne, max) |
| RAM Go          | Utilisation RAM (pic)          |
| GPU %           | Utilisation GPU (si NVIDIA)    |
| Temp√©rature GPU | Si disponible                  |

---

## üìÅ Exemple de r√©sultat (JSON)

```json
{
  "id": "20260206_143022",
  "timestamp": "2026-02-06T14:30:22",
  "hardware": {
    "os": { "system": "Darwin", "machine": "arm64" },
    "cpu": { "model": "Apple M2 Pro", "physical_cores": 12 },
    "gpu": { "primary_backend": "metal" },
    "ram": { "total_gb": 32.0, "unified_memory": true }
  },
  "classic_benchmarks": {
    "benchmarks": {
      "cpu_multi_thread": {
        "results": { "2048x2048": { "gflops": 312.5 } }
      }
    }
  },
  "ai_benchmarks": {
    "results": {
      "tinyllama-1.1b": {
        "summary": {
          "avg_tokens_per_second": 85.3,
          "avg_first_token_latency_s": 0.12,
          "stability": "stable"
        }
      }
    }
  }
}
```

---

## üîß Configuration avanc√©e

Modifiez `src/config.py` pour ajuster :

- **Mod√®les** : Ajouter/supprimer des mod√®les GGUF
- **Prompt** : Modifier le prompt de benchmark
- **Param√®tres d'inf√©rence** : `max_tokens`, `temperature`, nombre de runs
- **Tailles de matrices** : Pour les benchmarks CPU
- **Intervalle de monitoring** : Fr√©quence d'√©chantillonnage

---

## ü§ù Contribuer

1. Fork le d√©p√¥t
2. Cr√©er une branche : `git checkout -b feature/ma-feature`
3. Commit : `git commit -am "Ajout de ma feature"`
4. Push : `git push origin feature/ma-feature`
5. Ouvrir une Pull Request

---

## üìú Licence

MIT License

---

## ‚ö†Ô∏è Notes importantes

- Les mod√®les GGUF sont t√©l√©charg√©s dans le dossier `models/` (plusieurs Go par mod√®le).
- L'inf√©rence de gros mod√®les (34B, 70B) n√©cessite beaucoup de RAM.
- Sur Apple Silicon, la m√©moire unifi√©e est partag√©e entre CPU et GPU.
- Les r√©sultats varient selon la charge syst√®me et la temp√©rature du processeur.
- Pour des r√©sultats reproductibles, fermez les applications gourmandes en ressources.
