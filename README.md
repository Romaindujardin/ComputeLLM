# üñ•Ô∏è ComputeLLM ‚Äî AI Hardware Benchmark Tool

**Outil de benchmark multiplateforme d√©di√© au Hardware IA, avec focus sur l'inf√©rence locale de LLM.**

ComputeLLM permet de comparer les performances mat√©rielles (CPU, GPU, RAM) de diff√©rentes machines lors de l'inf√©rence locale de mod√®les de langage, en mettant en √©vidence les diff√©rences d'architecture :

| Architecture          | Exemple                     | Backend        |
| --------------------- | --------------------------- | -------------- |
| x86 + GPU d√©di√©       | Intel/AMD + NVIDIA RTX      | CUDA           |
| ARM + M√©moire unifi√©e | Apple Silicon (M1/M2/M3/M4) | Metal          |
| CPU seul              | Tout processeur             | CPU (fallback) |

---

## Fonctionnalit√©s

- **D√©tection mat√©rielle automatique** : OS, CPU (mod√®le, c≈ìurs, fr√©quence), GPU (VRAM, backend), RAM (totale, disponible, unifi√©e)
- **Benchmarks classiques** : CPU single-thread, CPU multi-thread, bande passante m√©moire, GPU compute
- **Benchmarks IA** : Inf√©rence locale de LLM via `llama-cpp-python` (GGUF) ou `llama-server` (HTTP)
- **Mode llama-server** : Utilise des binaires pr√©-compil√©s ‚Äî aucune compilation requise c√¥t√© Python
- **Mod√®les support√©s** : TinyLlama 1.1B, Mistral 7B, Llama 2 13B, CodeLlama 34B, Llama 2 70B
- **T√©l√©chargement automatique** depuis Hugging Face
- **M√©triques mesur√©es** : tokens/s, latence du 1er token, m√©moire utilis√©e, stabilit√©
- **Interface graphique** Streamlit avec bouton unique de lancement
- **Comparaison multi-machines** avec graphiques interactifs
- **Export** JSON et CSV

---

## Architecture

```
ComputeLLM/
‚îú‚îÄ‚îÄ main.py                    # Point d'entr√©e (CLI + GUI)
‚îú‚îÄ‚îÄ app.py                     # Application Streamlit (GUI)
‚îú‚îÄ‚îÄ requirements.txt           # D√©pendances Python
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ config.py              # Configuration et constantes
‚îÇ   ‚îú‚îÄ‚îÄ hardware_detect.py     # D√©tection mat√©rielle
‚îÇ   ‚îú‚îÄ‚îÄ benchmark_classic.py   # Benchmarks CPU/GPU/RAM
‚îÇ   ‚îú‚îÄ‚îÄ benchmark_ai.py        # Benchmarks inf√©rence LLM
‚îÇ   ‚îú‚îÄ‚îÄ llama_server.py        # Gestionnaire llama-server (HTTP)
‚îÇ   ‚îî‚îÄ‚îÄ results_manager.py     # Sauvegarde et comparaison
‚îú‚îÄ‚îÄ models/                    # Mod√®les GGUF t√©l√©charg√©s
‚îî‚îÄ‚îÄ results/                   # R√©sultats de benchmark (JSON)
```

---

## Installation

### Pr√©requis

- Python 3.10 ou sup√©rieur
- pip

### 1. Cloner le d√©p√¥t

```bash
git clone https://github.com/Romaindujardin/ComputeLLM.git
cd ComputeLLM
```

### 2. Cr√©er un environnement virtuel

```bash
python3 -m venv .venv
```

## Activer l'environnement - MacOS

```bash
source .venv/bin/activate
```

## Activer l'environnement - Windows

```bash
.venv\Scripts\activate
```

### 2. Installer les d√©pendances de base

```bash
pip install --upgrade pip setuptools wheel
```

```bash
pip install -r requirements.txt
```

### 3. Installer llama-cpp-python (selon votre mat√©riel)

> **Alternative sans compilation** : voir la section [Mode llama-server](#mode-llama-server-sans-compilation) ci-dessous.

#### macOS (Apple Silicon ‚Äî Metal)

```bash
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python
```

#### Windows / Linux (NVIDIA GPU ‚Äî CUDA)

```bash
set CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python
```

#### CPU uniquement (fallback)

```bash
pip install llama-cpp-python
```

### 4. (Optionnel) Installer PyTorch pour les benchmarks GPU classiques

#### macOS

```bash
pip install torch torchvision
```

#### Windows / Linux (CUDA)

```bash
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
```

### 5. Mode llama-server (sans compilation)

Si vous ne pouvez pas (ou ne voulez pas) compiler `llama-cpp-python` depuis les sources (probl√®mes de Visual Studio, oneAPI Toolkit, etc.), vous pouvez utiliser le **mode llama-server** :

1. **T√©l√©charger le binaire pr√©-compil√©** depuis les [releases de llama.cpp](https://github.com/ggerganov/llama.cpp/releases) :
   - Windows CUDA : `llama-*-bin-win-cuda-*`
   - Windows Vulkan : `llama-*-bin-win-vulkan-*`
   - Windows SYCL : `llama-*-bin-win-sycl-*`
   - Linux CUDA : `llama-*-bin-ubuntu-*-cuda-*`
   - macOS Metal : `llama-*-bin-macos-*`

2. **Extraire l'archive** et rep√©rer le binaire `llama-server` (ou `llama-server.exe` sur Windows)

3. **Dans ComputeLLM**, activer le toggle **"Utiliser llama-server"** dans la page Benchmark, puis configurer :
   - **Mode Auto** : indiquer le chemin vers le binaire `llama-server`. ComputeLLM d√©marre/arr√™te le serveur automatiquement.
   - **Mode Manuel** : lancer `llama-server` vous-m√™me, puis renseigner l'adresse (ex: `127.0.0.1:8080`).

#### Lancement manuel de llama-server

```bash
# Exemple : lancer le serveur avec un mod√®le GGUF
./llama-server -m models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf --port 8080 -ngl -1
```

> Le serveur expose une API compatible OpenAI sur `http://127.0.0.1:8080`. ComputeLLM s'y connecte automatiquement pour les benchmarks.

---

## Utilisation

### Interface graphique (recommand√©)

```bash
python main.py --gui
```

ou simplement :

```bash
python main.py
```

L'interface Streamlit s'ouvre dans votre navigateur avec 3 pages :

1. **Mat√©riel** ‚Äî D√©tection et affichage de la configuration
2. **Benchmark** ‚Äî Lancement avec un seul bouton
3. **R√©sultats** ‚Äî Visualisation, comparaison et export

### Ligne de commande (CLI)

```bash
# Tous les benchmarks
python main.py --cli

# Mod√®le sp√©cifique
python main.py --cli --models tinyllama-1.1b mistral-7b

# Benchmarks classiques uniquement
python main.py --cli --skip-ai

# Benchmarks IA uniquement
python main.py --cli --skip-classic

# D√©tecter le mat√©riel uniquement
python main.py --detect
```

---

## Mod√®les disponibles

| Cl√©              | Mod√®le                 | Param√®tres | Taille (Q4_K_M) | RAM min |
| ---------------- | ---------------------- | ---------- | --------------- | ------- |
| `tinyllama-1.1b` | TinyLlama 1.1B Chat    | 1.1B       | 0.7 Go          | 2 Go    |
| `mistral-7b`     | Mistral 7B Instruct    | 7B         | 4.4 Go          | 8 Go    |
| `llama2-13b`     | Llama 2 13B Chat       | 13B        | 7.9 Go          | 16 Go   |
| `codellama-34b`  | CodeLlama 34B Instruct | 34B        | 20.2 Go         | 32 Go   |
| `llama2-70b`     | Llama 2 70B Chat       | 70B        | 40.5 Go         | 64 Go   |

Les mod√®les sont t√©l√©charg√©s automatiquement depuis Hugging Face lors du premier benchmark.

---

## M√©triques mesur√©es

### Benchmarks classiques

| M√©trique              | Description                      |
| --------------------- | -------------------------------- |
| GFLOPS (ST)           | Performance CPU single-thread    |
| GFLOPS (MT)           | Performance CPU multi-thread     |
| Bande passante (Go/s) | Lecture, √©criture, copie m√©moire |
| GFLOPS GPU            | Performance GPU (CUDA/Metal)     |

### Benchmarks IA

| M√©trique                 | Description                    |
| ------------------------ | ------------------------------ |
| Tokens/s                 | D√©bit d'inf√©rence              |
| Latence 1er token (s)    | Temps avant le premier token   |
| Latence inter-token (ms) | Temps moyen entre chaque token |
| M√©moire pic (Go)         | M√©moire maximale utilis√©e      |
| Stabilit√©                | Nombre de runs r√©ussis / total |

### Monitoring en temps r√©el

| M√©trique        | Description                    |
| --------------- | ------------------------------ |
| CPU %           | Utilisation CPU (moyenne, max) |
| RAM Go          | Utilisation RAM (pic)          |
| GPU %           | Utilisation GPU (si NVIDIA)    |
| Temp√©rature GPU | Si disponible                  |

---

## Exemple de r√©sultat (JSON)

```json
{
  "id": "20260206_143022",
  "timestamp": "2026-02-06T14:30:22",
  "hardware": {
    "os": { "system": "Darwin", "machine": "arm64" },
    "cpu": { "model": "Apple M2 Pro", "physical_cores": 12 },
    "gpu": { "primary_backend": "metal" },
    "ram": { "total_gb": 32.0, "unified_memory": true }
  },
  "classic_benchmarks": {
    "benchmarks": {
      "cpu_multi_thread": {
        "results": { "2048x2048": { "gflops": 312.5 } }
      }
    }
  },
  "ai_benchmarks": {
    "results": {
      "tinyllama-1.1b": {
        "summary": {
          "avg_tokens_per_second": 85.3,
          "avg_first_token_latency_s": 0.12,
          "stability": "stable"
        }
      }
    }
  }
}
```

---

## Configuration avanc√©e

Modifiez `src/config.py` pour ajuster :

- **Mod√®les** : Ajouter/supprimer des mod√®les GGUF
- **Prompt** : Modifier le prompt de benchmark
- **Param√®tres d'inf√©rence** : `max_tokens`, `temperature`, nombre de runs
- **Tailles de matrices** : Pour les benchmarks CPU
- **Intervalle de monitoring** : Fr√©quence d'√©chantillonnage

---

## To-Do

### Haute priorit√©

- [ ] **D√©tection mat√©rielle**
  - Ajouter la d√©tection des GPU AMD :
    - `rocm-smi` (Linux)
    - `lspci` (fallback Linux)
    - WMI / Win32_VideoController (Windows)
  - Ajouter la d√©tection des GPU Intel (Arc / XPU) :
    - `xpu-smi`
    - Level Zero
    - `lspci` (Linux)
    - WMI (Windows)
  - Ajouter les backends d√©tect√©s : `rocm`, `xpu`, `sycl`

- [ ] **Benchmark classique GPU**
  - Support explicite AMD ROCm (identifier via `torch.version.hip`)
  - Support Intel XPU (`torch.xpu.is_available()`)
  - Synchronisation adapt√©e par device :
    - `torch.cuda.synchronize()`
    - `torch.xpu.synchronize()`
    - `torch.mps.synchronize()`
  - Ajouter monitoring :
    - AMD : `rocm-smi --showuse --showmemuse --showtemp`
    - Intel : `xpu-smi dump` / `intel_gpu_top`
  - Distinguer CUDA vs ROCm dans l‚Äôaffichage des r√©sultats

- [ ] **Benchmark AI / LLM**
  - √âtendre `detect_best_backend()` :
    - ROCm (HIPBLAS)
    - Vulkan
    - SYCL (Intel)
    - CLBlast / OpenCL (fallback g√©n√©rique)
  - G√©rer explicitement les backends llama-cpp-python :
    - `-DGGML_CUDA=on`
    - `-DGGML_HIPBLAS=on`
    - `-DGGML_VULKAN=on`
    - `-DGGML_SYCL=on`
    - `-DGGML_CLBLAST=on`
  - V√©rifier √† l‚Äôex√©cution que le backend compil√© correspond au GPU d√©tect√© - ~~Mode llama-server (binaires pr√©-compil√©s, z√©ro compilation)~~ ‚úÖ

---

### Priorit√© moyenne

- [ ] **Installation guid√©e de llama-cpp-python**
  - D√©tecter automatiquement le GPU au premier lancement
  - Proposer la bonne commande d‚Äôinstallation selon la plateforme
  - Ajouter un script d‚Äôinstallation automatique par OS
  - Afficher un avertissement si version CPU-only d√©tect√©e - ~~Mode llama-server comme alternative sans compilation~~ ‚úÖ
- [ ] **Monitoring unifi√©**
  - Agr√©ger les m√©triques NVIDIA / AMD / Intel dans `ResourceMonitor`
  - Normaliser le format des m√©triques (utilisation %, VRAM, temp√©rature)

---

### Priorit√© basse

- [ ] **Support multi-GPU**
  - D√©tection de plusieurs GPU
  - S√©lection via :
    - `CUDA_VISIBLE_DEVICES`
    - `HIP_VISIBLE_DEVICES`
    - `ZE_AFFINITY_MASK`
  - Benchmark individuel par GPU

- [ ] **Gestion RAM vs VRAM**
  - Distinguer RAM syst√®me et VRAM GPU
  - Adapter `get_compatible_quantizations()` en fonction de la VRAM
  - Recommandations dynamiques de mod√®les selon m√©moire disponible

- [ ] **Backend Vulkan universel**
  - Ajouter d√©tection via `vulkaninfo`
  - Documenter Vulkan comme backend cross-vendor (AMD / Intel / NVIDIA)

- [ ] **Documentation**
  - Ajouter tableau comparatif des backends support√©s par plateforme
  - Documenter les pr√©requis ROCm / SYCL / Vulkan
  - Ajouter exemples d‚Äôinstallation par architecture :
    - Windows x86 + NVIDIA
    - Linux + AMD ROCm
    - Windows/Linux + Vulkan
    - macOS ARM + Metal

---

## Notes importantes

- Les mod√®les GGUF sont t√©l√©charg√©s dans le dossier `models/` (plusieurs Go par mod√®le).
- L'inf√©rence de gros mod√®les (34B, 70B) n√©cessite beaucoup de RAM.
- Sur Apple Silicon, la m√©moire unifi√©e est partag√©e entre CPU et GPU.
- Les r√©sultats varient selon la charge syst√®me et la temp√©rature du processeur.
- Pour des r√©sultats reproductibles, fermez les applications gourmandes en ressources.
