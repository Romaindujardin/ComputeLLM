{
  "id": "20260216_174117",
  "timestamp": "2026-02-16T17:41:17.252130",
  "version": "1.0.0",
  "hardware": {
    "os": {
      "system": "Darwin",
      "release": "25.3.0",
      "version": "Darwin Kernel Version 25.3.0: Wed Jan 28 20:53:01 PST 2026; root:xnu-12377.81.4~5/RELEASE_ARM64_T8103",
      "machine": "arm64",
      "architecture": "64bit",
      "platform": "macOS-26.3-arm64-arm-64bit",
      "python_version": "3.11.9"
    },
    "cpu": {
      "physical_cores": 8,
      "logical_cores": 8,
      "architecture": "arm64",
      "model": "Apple M1",
      "is_apple_silicon": true,
      "architecture_type": "ARM (Apple Silicon)",
      "performance_cores": 4,
      "efficiency_cores": 4,
      "frequency_mhz": {
        "current": 3204,
        "min": 600,
        "max": 3204
      }
    },
    "gpu": {
      "gpus": [
        {
          "type": "Apple",
          "name": "Apple M1",
          "backend": "metal",
          "metal_support": "",
          "unified_memory": true,
          "unified_memory_gb": 8.0
        }
      ],
      "backends": [
        "metal",
        "cpu"
      ],
      "primary_backend": "metal",
      "python_backends": {
        "pytorch": false,
        "llama_cpp": false,
        "llama_server": false,
        "ipex": false
      }
    },
    "ram": {
      "total_gb": 8.0,
      "available_gb": 1.6,
      "used_gb": 3.08,
      "percent_used": 80.1,
      "swap_total_gb": 4.0,
      "swap_used_gb": 2.95,
      "unified_memory": true,
      "note": "Apple Silicon utilise une mémoire unifiée partagée entre CPU et GPU.",
      "hw_memsize_bytes": 8589934592
    }
  },
  "classic_benchmarks": {
    "type": "classic_benchmarks",
    "total_time_s": 11.8,
    "benchmarks": {
      "cpu_single_thread": {
        "test": "CPU Single-Thread",
        "method": "Matrix multiplication (NumPy, 1 thread)",
        "results": {
          "512x512": {
            "times_s": [
              0.0015,
              0.0004,
              0.0005
            ],
            "mean_s": 0.0008,
            "std_s": 0.0005,
            "gflops": 345.56
          },
          "1024x1024": {
            "times_s": [
              0.0044,
              0.0027,
              0.0035
            ],
            "mean_s": 0.0035,
            "std_s": 0.0007,
            "gflops": 608.42
          },
          "2048x2048": {
            "times_s": [
              0.0261,
              0.0223,
              0.0225
            ],
            "mean_s": 0.0237,
            "std_s": 0.0017,
            "gflops": 726.27
          }
        }
      },
      "cpu_multi_thread": {
        "test": "CPU Multi-Thread",
        "method": "Matrix multiplication (NumPy, 8 threads)",
        "n_threads": 8,
        "results": {
          "512x512": {
            "times_s": [
              0.003,
              0.0006,
              0.0006
            ],
            "mean_s": 0.0014,
            "std_s": 0.0011,
            "gflops": 187.88
          },
          "1024x1024": {
            "times_s": [
              0.0096,
              0.0028,
              0.0029
            ],
            "mean_s": 0.0051,
            "std_s": 0.0032,
            "gflops": 418.71
          },
          "2048x2048": {
            "times_s": [
              0.0245,
              0.0234,
              0.0203
            ],
            "mean_s": 0.0228,
            "std_s": 0.0018,
            "gflops": 754.96
          }
        }
      },
      "memory_bandwidth": {
        "test": "Memory Bandwidth",
        "method": "NumPy array operations (256 Mo)",
        "data_size_gb": 0.25,
        "results": {
          "write": {
            "mean_s": 0.0715,
            "bandwidth_gb_s": 3.5
          },
          "read": {
            "mean_s": 0.0083,
            "bandwidth_gb_s": 30.26
          },
          "copy": {
            "mean_s": 0.0252,
            "bandwidth_gb_s": 9.91
          }
        }
      },
      "gpu_compute": {
        "test": "GPU Compute",
        "status": "skipped",
        "reason": "PyTorch non installé"
      }
    },
    "resource_usage": {
      "n_samples": 22,
      "duration_s": 11.27323603630066,
      "cpu": {
        "avg_percent": 27.1,
        "max_percent": 53.8,
        "min_percent": 0.0
      },
      "ram": {
        "avg_percent": 82.8,
        "max_percent": 84.5,
        "peak_used_gb": 3.14
      }
    }
  },
  "ai_benchmarks": {
    "type": "ai_benchmarks",
    "prompt": "Explain the concept of artificial intelligence in simple terms. What are its main applications and how does it impact our daily lives? Provide specific examples.",
    "inference_config": {
      "max_tokens": 256,
      "temperature": 0.7,
      "top_p": 0.9,
      "repeat_penalty": 1.1,
      "n_ctx": 2048,
      "seed": 42,
      "n_warmup_runs": 1,
      "n_benchmark_runs": 3
    },
    "total_time_s": 44.94,
    "models_tested": 1,
    "results": {
      "tinyllama-1.1b": {
        "model": "TinyLlama 1.1B",
        "params": "1.1B",
        "status": "error",
        "runs": [],
        "backend": {
          "backend": "metal",
          "n_gpu_layers": -1,
          "details": "Apple Silicon détecté, utilisation de Metal"
        },
        "error": "llama-cpp-python n'est pas installé.\nInstallation :\n  macOS (Metal) : CMAKE_ARGS=\"-DGGML_METAL=on\" pip install llama-cpp-python\n  Windows (CUDA): CMAKE_ARGS=\"-DGGML_CUDA=on\" pip install llama-cpp-python\n  Intel (SYCL)  : CMAKE_ARGS=\"-DGGML_SYCL=on\" pip install llama-cpp-python\n  CPU only      : pip install llama-cpp-python\n\nAlternative sans compilation : utilisez le mode llama-server\n  Téléchargez le binaire depuis https://github.com/ggerganov/llama.cpp/releases\n  Puis activez 'Utiliser llama-server' dans le Benchmark."
      }
    },
    "quantization_comparison": {
      "tinyllama-1.1b": {
        "model_name": "TinyLlama 1.1B",
        "model_key": "tinyllama-1.1b",
        "params": "1.1B",
        "variants_tested": [
          "Q2_K",
          "Q3_K_M",
          "Q4_K_M",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "results": {
          "Q2_K": {
            "quantization": "Q2_K",
            "bits": 2,
            "file_size_gb": 0.45,
            "status": "error",
            "runs": [],
            "actual_file_size_gb": 0.45,
            "backend": {
              "backend": "metal",
              "n_gpu_layers": -1,
              "details": "Apple Silicon détecté, utilisation de Metal"
            },
            "error": "llama-cpp-python n'est pas installé.\nInstallation :\n  macOS (Metal) : CMAKE_ARGS=\"-DGGML_METAL=on\" pip install llama-cpp-python\n  Windows (CUDA): CMAKE_ARGS=\"-DGGML_CUDA=on\" pip install llama-cpp-python\n  Intel (SYCL)  : CMAKE_ARGS=\"-DGGML_SYCL=on\" pip install llama-cpp-python\n  CPU only      : pip install llama-cpp-python\n\nAlternative sans compilation : utilisez le mode llama-server\n  Téléchargez le binaire depuis https://github.com/ggerganov/llama.cpp/releases\n  Puis activez 'Utiliser llama-server' dans le Benchmark."
          },
          "Q3_K_M": {
            "quantization": "Q3_K_M",
            "bits": 3,
            "file_size_gb": 0.51,
            "status": "error",
            "runs": [],
            "actual_file_size_gb": 0.513,
            "backend": {
              "backend": "metal",
              "n_gpu_layers": -1,
              "details": "Apple Silicon détecté, utilisation de Metal"
            },
            "error": "llama-cpp-python n'est pas installé.\nInstallation :\n  macOS (Metal) : CMAKE_ARGS=\"-DGGML_METAL=on\" pip install llama-cpp-python\n  Windows (CUDA): CMAKE_ARGS=\"-DGGML_CUDA=on\" pip install llama-cpp-python\n  Intel (SYCL)  : CMAKE_ARGS=\"-DGGML_SYCL=on\" pip install llama-cpp-python\n  CPU only      : pip install llama-cpp-python\n\nAlternative sans compilation : utilisez le mode llama-server\n  Téléchargez le binaire depuis https://github.com/ggerganov/llama.cpp/releases\n  Puis activez 'Utiliser llama-server' dans le Benchmark."
          },
          "Q4_K_M": {
            "quantization": "Q4_K_M",
            "bits": 4,
            "file_size_gb": 0.62,
            "status": "error",
            "runs": [],
            "actual_file_size_gb": 0.623,
            "backend": {
              "backend": "metal",
              "n_gpu_layers": -1,
              "details": "Apple Silicon détecté, utilisation de Metal"
            },
            "error": "llama-cpp-python n'est pas installé.\nInstallation :\n  macOS (Metal) : CMAKE_ARGS=\"-DGGML_METAL=on\" pip install llama-cpp-python\n  Windows (CUDA): CMAKE_ARGS=\"-DGGML_CUDA=on\" pip install llama-cpp-python\n  Intel (SYCL)  : CMAKE_ARGS=\"-DGGML_SYCL=on\" pip install llama-cpp-python\n  CPU only      : pip install llama-cpp-python\n\nAlternative sans compilation : utilisez le mode llama-server\n  Téléchargez le binaire depuis https://github.com/ggerganov/llama.cpp/releases\n  Puis activez 'Utiliser llama-server' dans le Benchmark."
          },
          "Q5_K_M": {
            "quantization": "Q5_K_M",
            "bits": 5,
            "file_size_gb": 0.73,
            "status": "error",
            "runs": [],
            "actual_file_size_gb": 0.729,
            "backend": {
              "backend": "metal",
              "n_gpu_layers": -1,
              "details": "Apple Silicon détecté, utilisation de Metal"
            },
            "error": "llama-cpp-python n'est pas installé.\nInstallation :\n  macOS (Metal) : CMAKE_ARGS=\"-DGGML_METAL=on\" pip install llama-cpp-python\n  Windows (CUDA): CMAKE_ARGS=\"-DGGML_CUDA=on\" pip install llama-cpp-python\n  Intel (SYCL)  : CMAKE_ARGS=\"-DGGML_SYCL=on\" pip install llama-cpp-python\n  CPU only      : pip install llama-cpp-python\n\nAlternative sans compilation : utilisez le mode llama-server\n  Téléchargez le binaire depuis https://github.com/ggerganov/llama.cpp/releases\n  Puis activez 'Utiliser llama-server' dans le Benchmark."
          },
          "Q6_K": {
            "quantization": "Q6_K",
            "bits": 6,
            "file_size_gb": 0.84,
            "status": "error",
            "runs": [],
            "actual_file_size_gb": 0.842,
            "backend": {
              "backend": "metal",
              "n_gpu_layers": -1,
              "details": "Apple Silicon détecté, utilisation de Metal"
            },
            "error": "llama-cpp-python n'est pas installé.\nInstallation :\n  macOS (Metal) : CMAKE_ARGS=\"-DGGML_METAL=on\" pip install llama-cpp-python\n  Windows (CUDA): CMAKE_ARGS=\"-DGGML_CUDA=on\" pip install llama-cpp-python\n  Intel (SYCL)  : CMAKE_ARGS=\"-DGGML_SYCL=on\" pip install llama-cpp-python\n  CPU only      : pip install llama-cpp-python\n\nAlternative sans compilation : utilisez le mode llama-server\n  Téléchargez le binaire depuis https://github.com/ggerganov/llama.cpp/releases\n  Puis activez 'Utiliser llama-server' dans le Benchmark."
          },
          "Q8_0": {
            "quantization": "Q8_0",
            "bits": 8,
            "file_size_gb": 1.09,
            "status": "error",
            "runs": [],
            "actual_file_size_gb": 1.09,
            "backend": {
              "backend": "metal",
              "n_gpu_layers": -1,
              "details": "Apple Silicon détecté, utilisation de Metal"
            },
            "error": "llama-cpp-python n'est pas installé.\nInstallation :\n  macOS (Metal) : CMAKE_ARGS=\"-DGGML_METAL=on\" pip install llama-cpp-python\n  Windows (CUDA): CMAKE_ARGS=\"-DGGML_CUDA=on\" pip install llama-cpp-python\n  Intel (SYCL)  : CMAKE_ARGS=\"-DGGML_SYCL=on\" pip install llama-cpp-python\n  CPU only      : pip install llama-cpp-python\n\nAlternative sans compilation : utilisez le mode llama-server\n  Téléchargez le binaire depuis https://github.com/ggerganov/llama.cpp/releases\n  Puis activez 'Utiliser llama-server' dans le Benchmark."
          }
        },
        "total_time_s": 26.01,
        "comparison_table": []
      }
    },
    "temperature_comparison": {
      "tinyllama-1.1b": {
        "axis": "temperature",
        "model_name": "TinyLlama 1.1B",
        "model_key": "tinyllama-1.1b",
        "params": "1.1B",
        "variants_tested": [
          "low",
          "medium",
          "high"
        ],
        "status": "error",
        "results": {},
        "backend": {
          "backend": "metal",
          "n_gpu_layers": -1,
          "details": "Apple Silicon détecté, utilisation de Metal"
        },
        "error": "llama-cpp-python n'est pas installé.\nInstallation :\n  macOS (Metal) : CMAKE_ARGS=\"-DGGML_METAL=on\" pip install llama-cpp-python\n  Windows (CUDA): CMAKE_ARGS=\"-DGGML_CUDA=on\" pip install llama-cpp-python\n  Intel (SYCL)  : CMAKE_ARGS=\"-DGGML_SYCL=on\" pip install llama-cpp-python\n  CPU only      : pip install llama-cpp-python\n\nAlternative sans compilation : utilisez le mode llama-server\n  Téléchargez le binaire depuis https://github.com/ggerganov/llama.cpp/releases\n  Puis activez 'Utiliser llama-server' dans le Benchmark."
      }
    },
    "language_comparison": {
      "tinyllama-1.1b": {
        "axis": "language",
        "model_name": "TinyLlama 1.1B",
        "model_key": "tinyllama-1.1b",
        "params": "1.1B",
        "variants_tested": [
          "en",
          "fr",
          "zh",
          "es",
          "de",
          "ar"
        ],
        "status": "error",
        "results": {},
        "backend": {
          "backend": "metal",
          "n_gpu_layers": -1,
          "details": "Apple Silicon détecté, utilisation de Metal"
        },
        "error": "llama-cpp-python n'est pas installé.\nInstallation :\n  macOS (Metal) : CMAKE_ARGS=\"-DGGML_METAL=on\" pip install llama-cpp-python\n  Windows (CUDA): CMAKE_ARGS=\"-DGGML_CUDA=on\" pip install llama-cpp-python\n  Intel (SYCL)  : CMAKE_ARGS=\"-DGGML_SYCL=on\" pip install llama-cpp-python\n  CPU only      : pip install llama-cpp-python\n\nAlternative sans compilation : utilisez le mode llama-server\n  Téléchargez le binaire depuis https://github.com/ggerganov/llama.cpp/releases\n  Puis activez 'Utiliser llama-server' dans le Benchmark."
      }
    },
    "prompt_type_comparison": {
      "tinyllama-1.1b": {
        "axis": "prompt_type",
        "model_name": "TinyLlama 1.1B",
        "model_key": "tinyllama-1.1b",
        "params": "1.1B",
        "variants_tested": [
          "general",
          "code",
          "reasoning",
          "creative",
          "math"
        ],
        "status": "error",
        "results": {},
        "backend": {
          "backend": "metal",
          "n_gpu_layers": -1,
          "details": "Apple Silicon détecté, utilisation de Metal"
        },
        "error": "llama-cpp-python n'est pas installé.\nInstallation :\n  macOS (Metal) : CMAKE_ARGS=\"-DGGML_METAL=on\" pip install llama-cpp-python\n  Windows (CUDA): CMAKE_ARGS=\"-DGGML_CUDA=on\" pip install llama-cpp-python\n  Intel (SYCL)  : CMAKE_ARGS=\"-DGGML_SYCL=on\" pip install llama-cpp-python\n  CPU only      : pip install llama-cpp-python\n\nAlternative sans compilation : utilisez le mode llama-server\n  Téléchargez le binaire depuis https://github.com/ggerganov/llama.cpp/releases\n  Puis activez 'Utiliser llama-server' dans le Benchmark."
      }
    }
  }
}