{
  "id": "20260219_121807",
  "timestamp": "2026-02-19T12:18:07.281133",
  "version": "1.0.0",
  "hardware": {
    "os": {
      "system": "Windows",
      "release": "11",
      "version": "10.0.26200",
      "machine": "AMD64",
      "architecture": "64bit",
      "platform": "Windows-11-10.0.26200-SP0",
      "python_version": "3.14.3"
    },
    "cpu": {
      "physical_cores": 12,
      "logical_cores": 24,
      "architecture": "AMD64",
      "detection_error": "[WinError 2] Le fichier sp√©cifi√© est introuvable",
      "model": "AMD64 Family 26 Model 68 Stepping 0, AuthenticAMD",
      "is_apple_silicon": false,
      "architecture_type": "AMD64",
      "frequency_mhz": {
        "current": 4400.0,
        "min": null,
        "max": 4400.0
      }
    },
    "gpu": {
      "gpus": [
        {
          "type": "NVIDIA",
          "name": "NVIDIA GeForce RTX 3070",
          "vram_total_mb": 8192.0,
          "vram_free_mb": 6692.0,
          "driver_version": "591.74",
          "compute_capability": "8.6",
          "backend": "cuda",
          "device_index": 0,
          "gpu_index": 0
        },
        {
          "type": "AMD",
          "name": "AMD Radeon(TM) Graphics",
          "backend": "directml",
          "detected_via": "wmi",
          "vram_total_mb": 512,
          "driver_version": "32.0.21041.1000",
          "gpu_index": 1
        }
      ],
      "backends": [
        "cuda",
        "directml",
        "cpu"
      ],
      "primary_backend": "cuda",
      "python_backends": {
        "pytorch": true,
        "pytorch_version": "2.10.0+cpu",
        "pytorch_cuda": false,
        "pytorch_mps": false,
        "pytorch_xpu": false,
        "llama_cpp": false,
        "llama_server": false,
        "pytorch_rocm": false,
        "ipex": false,
        "directml": false
      }
    },
    "ram": {
      "total_gb": 31.11,
      "available_gb": 20.38,
      "used_gb": 10.73,
      "percent_used": 34.5,
      "swap_total_gb": 2.0,
      "swap_used_gb": 0.1,
      "unified_memory": false
    },
    "selected_gpu": {
      "gpu_index": 0,
      "name": "NVIDIA GeForce RTX 3070",
      "backend": "cuda"
    }
  },
  "classic_benchmarks": {
    "type": "classic_benchmarks",
    "total_time_s": 7.58,
    "benchmarks": {
      "cpu_single_thread": {
        "test": "CPU Single-Thread",
        "method": "Matrix multiplication (NumPy, 1 thread, subprocess isol√©)",
        "results": {
          "512x512": {
            "times_s": [
              0.001,
              0.001,
              0.001
            ],
            "mean_s": 0.001,
            "median_s": 0.001,
            "std_s": 0.0,
            "gflops": 269.73
          },
          "1024x1024": {
            "times_s": [
              0.007,
              0.0071,
              0.0071
            ],
            "mean_s": 0.0071,
            "median_s": 0.0071,
            "std_s": 0.0,
            "gflops": 303.84
          },
          "2048x2048": {
            "times_s": [
              0.0541,
              0.0541,
              0.0538
            ],
            "mean_s": 0.054,
            "median_s": 0.0541,
            "std_s": 0.0002,
            "gflops": 317.73
          }
        }
      },
      "cpu_multi_thread": {
        "test": "CPU Multi-Thread",
        "method": "Matrix multiplication (NumPy, 24 threads, subprocess isol√©)",
        "n_threads": 24,
        "results": {
          "512x512": {
            "times_s": [
              0.0007,
              0.0006,
              0.0006
            ],
            "mean_s": 0.0006,
            "median_s": 0.0006,
            "std_s": 0.0001,
            "gflops": 457.46
          },
          "1024x1024": {
            "times_s": [
              0.0021,
              0.002,
              0.0019
            ],
            "mean_s": 0.002,
            "median_s": 0.002,
            "std_s": 0.0001,
            "gflops": 1065.27
          },
          "2048x2048": {
            "times_s": [
              0.0145,
              0.0129,
              0.0132
            ],
            "mean_s": 0.0135,
            "median_s": 0.0132,
            "std_s": 0.0007,
            "gflops": 1304.88
          }
        }
      },
      "memory_bandwidth": {
        "test": "Memory Bandwidth",
        "method": "NumPy array operations (256 Mo)",
        "data_size_gb": 0.25,
        "results": {
          "write": {
            "mean_s": 0.034,
            "median_s": 0.0365,
            "bandwidth_gb_s": 6.86
          },
          "read": {
            "mean_s": 0.0212,
            "median_s": 0.0212,
            "bandwidth_gb_s": 11.77
          },
          "copy": {
            "mean_s": 0.0464,
            "median_s": 0.0487,
            "bandwidth_gb_s": 5.14
          }
        }
      },
      "gpu_compute": {
        "test": "GPU Compute (Raw)",
        "status": "skipped",
        "reason": "GPU NVIDIA s√©lectionn√© (NVIDIA GeForce RTX 3070), mais PyTorch n'a pas le support CUDA activ√© (version install√©e : 2.10.0+cpu).",
        "advice": "Installez PyTorch avec le support CUDA pour utiliser votre GPU NVIDIA :\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
      },
      "gpu_system": {
        "test": "GPU System Score",
        "status": "skipped",
        "reason": "GPU NVIDIA s√©lectionn√© (NVIDIA GeForce RTX 3070), mais PyTorch n'a pas le support CUDA activ√© (version install√©e : 2.10.0+cpu).",
        "advice": "Installez PyTorch avec le support CUDA pour utiliser votre GPU NVIDIA :\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
      }
    },
    "resource_usage": {
      "n_samples": 14,
      "duration_s": 7.037958383560181,
      "cpu": {
        "avg_percent": 8.2,
        "max_percent": 14.1,
        "min_percent": 0.0
      },
      "ram": {
        "avg_percent": 34.8,
        "max_percent": 35.5,
        "peak_used_gb": 11.04
      },
      "gpu": {
        "avg_utilization_percent": 29.5,
        "max_utilization_percent": 39.0,
        "peak_memory_mb": 1398.0,
        "max_temperature_c": 43.0
      }
    }
  },
  "ai_benchmarks": {
    "type": "ai_benchmarks",
    "inference_mode": "server",
    "prompt": "Explain the concept of artificial intelligence in simple terms. What are its main applications and how does it impact our daily lives? Provide specific examples.",
    "inference_config": {
      "max_tokens": 256,
      "temperature": 0.7,
      "top_p": 0.9,
      "repeat_penalty": 1.1,
      "n_ctx": 2048,
      "seed": 42,
      "n_warmup_runs": 2,
      "n_benchmark_runs": 3
    },
    "total_time_s": 236.8,
    "models_tested": 1,
    "results": {
      "tinyllama-1.1b": {
        "model": "TinyLlama 1.1B",
        "params": "1.1B",
        "status": "completed",
        "inference_mode": "server",
        "runs": [
          {
            "tokens_generated": 256,
            "total_time_s": 2.7105,
            "first_token_latency_s": 0.0344,
            "tokens_per_second": 94.45,
            "memory_before_gb": 1.365,
            "memory_after_gb": 1.366,
            "memory_delta_gb": 0.001,
            "server_memory_gb": 1.108,
            "error": null,
            "success": true,
            "inference_mode": "server",
            "avg_inter_token_latency_ms": 10.49,
            "p90_inter_token_latency_ms": 11.05
          },
          {
            "tokens_generated": 256,
            "total_time_s": 2.6349,
            "first_token_latency_s": 0.0314,
            "tokens_per_second": 97.16,
            "memory_before_gb": 1.366,
            "memory_after_gb": 1.373,
            "memory_delta_gb": 0.007,
            "server_memory_gb": 1.114,
            "error": null,
            "success": true,
            "inference_mode": "server",
            "avg_inter_token_latency_ms": 10.21,
            "p90_inter_token_latency_ms": 10.75
          },
          {
            "tokens_generated": 256,
            "total_time_s": 2.6314,
            "first_token_latency_s": 0.0359,
            "tokens_per_second": 97.29,
            "memory_before_gb": 1.373,
            "memory_after_gb": 1.373,
            "memory_delta_gb": 0.0,
            "server_memory_gb": 1.114,
            "error": null,
            "success": true,
            "inference_mode": "server",
            "avg_inter_token_latency_ms": 10.18,
            "p90_inter_token_latency_ms": 10.77
          }
        ],
        "model_load_time_s": 1.53,
        "backend": {
          "backend": "cuda",
          "n_gpu_layers": -1,
          "details": "NVIDIA GPU d√©tect√©, utilisation de CUDA",
          "inference_mode": "server",
          "server_url": "http://127.0.0.1:8080"
        },
        "resource_usage": {
          "n_samples": 15,
          "duration_s": 7.730942010879517,
          "cpu": {
            "avg_percent": 53.3,
            "max_percent": 59.1,
            "min_percent": 0.0
          },
          "ram": {
            "avg_percent": 38.3,
            "max_percent": 38.3,
            "peak_used_gb": 11.93
          },
          "gpu": {
            "avg_utilization_percent": 32.1,
            "max_utilization_percent": 35.0,
            "peak_memory_mb": 1360.0,
            "max_temperature_c": 43.0
          }
        },
        "summary": {
          "n_successful_runs": 3,
          "n_total_runs": 3,
          "avg_tokens_per_second": 96.3,
          "std_tokens_per_second": 1.31,
          "avg_first_token_latency_s": 0.0339,
          "avg_total_time_s": 2.6589,
          "peak_memory_gb": 1.373,
          "stability": "stable"
        }
      }
    },
    "quantization_comparison": {
      "tinyllama-1.1b": {
        "model_name": "TinyLlama 1.1B",
        "model_key": "tinyllama-1.1b",
        "params": "1.1B",
        "variants_tested": [
          "Q2_K",
          "Q3_K_M",
          "Q4_K_M",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "results": {
          "Q2_K": {
            "quantization": "Q2_K",
            "bits": 2,
            "file_size_gb": 0.45,
            "status": "completed",
            "inference_mode": "server",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 2.0664,
                "first_token_latency_s": 0.0337,
                "tokens_per_second": 123.89,
                "memory_before_gb": 0.794,
                "memory_after_gb": 0.795,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 0.536,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 7.97,
                "p90_inter_token_latency_ms": 8.48
              },
              {
                "tokens_generated": 256,
                "total_time_s": 2.0498,
                "first_token_latency_s": 0.0358,
                "tokens_per_second": 124.89,
                "memory_before_gb": 0.795,
                "memory_after_gb": 0.802,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.543,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 7.9,
                "p90_inter_token_latency_ms": 8.38
              },
              {
                "tokens_generated": 256,
                "total_time_s": 1.9904,
                "first_token_latency_s": 0.0291,
                "tokens_per_second": 128.62,
                "memory_before_gb": 0.802,
                "memory_after_gb": 0.802,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 0.543,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 7.69,
                "p90_inter_token_latency_ms": 8.14
              }
            ],
            "actual_file_size_gb": 0.45,
            "model_load_time_s": 0.52,
            "backend": {
              "backend": "cuda",
              "n_gpu_layers": -1,
              "details": "NVIDIA GPU d√©tect√©, utilisation de CUDA",
              "inference_mode": "server",
              "server_url": "http://127.0.0.1:8080"
            },
            "memory_after_load_gb": 0.783,
            "server_memory_after_load_gb": 0.524,
            "resource_usage": {
              "n_samples": 12,
              "duration_s": 6.043200969696045,
              "cpu": {
                "avg_percent": 51.8,
                "max_percent": 58.6,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 36.5,
                "max_percent": 36.5,
                "peak_used_gb": 11.35
              },
              "gpu": {
                "avg_utilization_percent": 24.9,
                "max_utilization_percent": 43.0,
                "peak_memory_mb": 1367.0,
                "max_temperature_c": 43.0
              }
            },
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 125.8,
              "std_tokens_per_second": 2.04,
              "min_tokens_per_second": 123.89,
              "max_tokens_per_second": 128.62,
              "avg_first_token_latency_s": 0.0329,
              "avg_inter_token_latency_ms": 7.85,
              "avg_total_time_s": 2.0355,
              "peak_memory_gb": 0.802,
              "stability": "stable"
            }
          },
          "Q3_K_M": {
            "quantization": "Q3_K_M",
            "bits": 3,
            "file_size_gb": 0.51,
            "status": "completed",
            "inference_mode": "server",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 2.2577,
                "first_token_latency_s": 0.0294,
                "tokens_per_second": 113.39,
                "memory_before_gb": 0.994,
                "memory_after_gb": 0.995,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 0.736,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 8.74,
                "p90_inter_token_latency_ms": 9.29
              },
              {
                "tokens_generated": 256,
                "total_time_s": 2.2556,
                "first_token_latency_s": 0.029,
                "tokens_per_second": 113.5,
                "memory_before_gb": 0.995,
                "memory_after_gb": 1.001,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.743,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 8.73,
                "p90_inter_token_latency_ms": 9.19
              },
              {
                "tokens_generated": 256,
                "total_time_s": 2.2853,
                "first_token_latency_s": 0.03,
                "tokens_per_second": 112.02,
                "memory_before_gb": 1.001,
                "memory_after_gb": 1.001,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 0.743,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 8.84,
                "p90_inter_token_latency_ms": 9.34
              }
            ],
            "actual_file_size_gb": 0.513,
            "model_load_time_s": 0.51,
            "backend": {
              "backend": "cuda",
              "n_gpu_layers": -1,
              "details": "NVIDIA GPU d√©tect√©, utilisation de CUDA",
              "inference_mode": "server",
              "server_url": "http://127.0.0.1:8080"
            },
            "memory_after_load_gb": 0.982,
            "server_memory_after_load_gb": 0.724,
            "resource_usage": {
              "n_samples": 13,
              "duration_s": 6.5982935428619385,
              "cpu": {
                "avg_percent": 52.5,
                "max_percent": 58.5,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 37.2,
                "max_percent": 37.2,
                "peak_used_gb": 11.58
              },
              "gpu": {
                "avg_utilization_percent": 34.1,
                "max_utilization_percent": 54.0,
                "peak_memory_mb": 1360.0,
                "max_temperature_c": 43.0
              }
            },
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 112.97,
              "std_tokens_per_second": 0.67,
              "min_tokens_per_second": 112.02,
              "max_tokens_per_second": 113.5,
              "avg_first_token_latency_s": 0.0295,
              "avg_inter_token_latency_ms": 8.77,
              "avg_total_time_s": 2.2662,
              "peak_memory_gb": 1.001,
              "stability": "stable"
            }
          },
          "Q4_K_M": {
            "quantization": "Q4_K_M",
            "bits": 4,
            "file_size_gb": 0.62,
            "status": "completed",
            "inference_mode": "server",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 2.6453,
                "first_token_latency_s": 0.0315,
                "tokens_per_second": 96.77,
                "memory_before_gb": 1.366,
                "memory_after_gb": 1.366,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 1.108,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.25,
                "p90_inter_token_latency_ms": 10.81
              },
              {
                "tokens_generated": 256,
                "total_time_s": 2.6303,
                "first_token_latency_s": 0.032,
                "tokens_per_second": 97.33,
                "memory_before_gb": 1.366,
                "memory_after_gb": 1.373,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 1.114,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.19,
                "p90_inter_token_latency_ms": 10.67
              },
              {
                "tokens_generated": 256,
                "total_time_s": 2.636,
                "first_token_latency_s": 0.034,
                "tokens_per_second": 97.12,
                "memory_before_gb": 1.373,
                "memory_after_gb": 1.373,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 1.114,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.2,
                "p90_inter_token_latency_ms": 10.68
              }
            ],
            "actual_file_size_gb": 0.623,
            "model_load_time_s": 0.51,
            "backend": {
              "backend": "cuda",
              "n_gpu_layers": -1,
              "details": "NVIDIA GPU d√©tect√©, utilisation de CUDA",
              "inference_mode": "server",
              "server_url": "http://127.0.0.1:8080"
            },
            "memory_after_load_gb": 1.354,
            "server_memory_after_load_gb": 1.095,
            "resource_usage": {
              "n_samples": 15,
              "duration_s": 7.756795644760132,
              "cpu": {
                "avg_percent": 59.9,
                "max_percent": 100.0,
                "min_percent": 55.6
              },
              "ram": {
                "avg_percent": 38.4,
                "max_percent": 38.4,
                "peak_used_gb": 11.94
              },
              "gpu": {
                "avg_utilization_percent": 34.9,
                "max_utilization_percent": 41.0,
                "peak_memory_mb": 1352.0,
                "max_temperature_c": 43.0
              }
            },
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 97.07,
              "std_tokens_per_second": 0.23,
              "min_tokens_per_second": 96.77,
              "max_tokens_per_second": 97.33,
              "avg_first_token_latency_s": 0.0325,
              "avg_inter_token_latency_ms": 10.21,
              "avg_total_time_s": 2.6372,
              "peak_memory_gb": 1.373,
              "stability": "stable"
            }
          },
          "Q5_K_M": {
            "quantization": "Q5_K_M",
            "bits": 5,
            "file_size_gb": 0.73,
            "status": "completed",
            "inference_mode": "server",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 3.0111,
                "first_token_latency_s": 0.0356,
                "tokens_per_second": 85.02,
                "memory_before_gb": 1.02,
                "memory_after_gb": 1.021,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 0.762,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 11.67,
                "p90_inter_token_latency_ms": 12.21
              },
              {
                "tokens_generated": 256,
                "total_time_s": 3.0137,
                "first_token_latency_s": 0.0399,
                "tokens_per_second": 84.95,
                "memory_before_gb": 1.021,
                "memory_after_gb": 1.027,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.769,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 11.66,
                "p90_inter_token_latency_ms": 12.25
              },
              {
                "tokens_generated": 256,
                "total_time_s": 2.9999,
                "first_token_latency_s": 0.0256,
                "tokens_per_second": 85.34,
                "memory_before_gb": 1.027,
                "memory_after_gb": 1.027,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 0.769,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 11.66,
                "p90_inter_token_latency_ms": 12.27
              }
            ],
            "actual_file_size_gb": 0.729,
            "model_load_time_s": 0.52,
            "backend": {
              "backend": "cuda",
              "n_gpu_layers": -1,
              "details": "NVIDIA GPU d√©tect√©, utilisation de CUDA",
              "inference_mode": "server",
              "server_url": "http://127.0.0.1:8080"
            },
            "memory_after_load_gb": 1.008,
            "server_memory_after_load_gb": 0.75,
            "resource_usage": {
              "n_samples": 17,
              "duration_s": 8.852246046066284,
              "cpu": {
                "avg_percent": 54.0,
                "max_percent": 59.7,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 37.3,
                "max_percent": 37.3,
                "peak_used_gb": 11.6
              },
              "gpu": {
                "avg_utilization_percent": 34.8,
                "max_utilization_percent": 44.0,
                "peak_memory_mb": 1359.0,
                "max_temperature_c": 43.0
              }
            },
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 85.1,
              "std_tokens_per_second": 0.17,
              "min_tokens_per_second": 84.95,
              "max_tokens_per_second": 85.34,
              "avg_first_token_latency_s": 0.0337,
              "avg_inter_token_latency_ms": 11.66,
              "avg_total_time_s": 3.0082,
              "peak_memory_gb": 1.027,
              "stability": "stable"
            }
          },
          "Q6_K": {
            "quantization": "Q6_K",
            "bits": 6,
            "file_size_gb": 0.84,
            "status": "completed",
            "inference_mode": "server",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 3.4511,
                "first_token_latency_s": 0.0387,
                "tokens_per_second": 74.18,
                "memory_before_gb": 1.125,
                "memory_after_gb": 1.125,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 0.867,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 13.38,
                "p90_inter_token_latency_ms": 13.99
              },
              {
                "tokens_generated": 256,
                "total_time_s": 3.4059,
                "first_token_latency_s": 0.0282,
                "tokens_per_second": 75.16,
                "memory_before_gb": 1.125,
                "memory_after_gb": 1.132,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.873,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 13.25,
                "p90_inter_token_latency_ms": 13.86
              },
              {
                "tokens_generated": 256,
                "total_time_s": 3.4019,
                "first_token_latency_s": 0.038,
                "tokens_per_second": 75.25,
                "memory_before_gb": 1.132,
                "memory_after_gb": 1.132,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 0.873,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 13.19,
                "p90_inter_token_latency_ms": 13.79
              }
            ],
            "actual_file_size_gb": 0.842,
            "model_load_time_s": 0.53,
            "backend": {
              "backend": "cuda",
              "n_gpu_layers": -1,
              "details": "NVIDIA GPU d√©tect√©, utilisation de CUDA",
              "inference_mode": "server",
              "server_url": "http://127.0.0.1:8080"
            },
            "memory_after_load_gb": 1.113,
            "server_memory_after_load_gb": 0.854,
            "resource_usage": {
              "n_samples": 19,
              "duration_s": 9.953965425491333,
              "cpu": {
                "avg_percent": 53.9,
                "max_percent": 58.8,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 37.6,
                "max_percent": 37.6,
                "peak_used_gb": 11.7
              },
              "gpu": {
                "avg_utilization_percent": 32.6,
                "max_utilization_percent": 37.0,
                "peak_memory_mb": 1360.0,
                "max_temperature_c": 43.0
              }
            },
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 74.86,
              "std_tokens_per_second": 0.48,
              "min_tokens_per_second": 74.18,
              "max_tokens_per_second": 75.25,
              "avg_first_token_latency_s": 0.035,
              "avg_inter_token_latency_ms": 13.27,
              "avg_total_time_s": 3.4196,
              "peak_memory_gb": 1.132,
              "stability": "stable"
            }
          },
          "Q8_0": {
            "quantization": "Q8_0",
            "bits": 8,
            "file_size_gb": 1.09,
            "status": "completed",
            "inference_mode": "server",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 4.3332,
                "first_token_latency_s": 0.0381,
                "tokens_per_second": 59.08,
                "memory_before_gb": 1.358,
                "memory_after_gb": 1.359,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 1.1,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 16.84,
                "p90_inter_token_latency_ms": 17.53
              },
              {
                "tokens_generated": 256,
                "total_time_s": 4.3275,
                "first_token_latency_s": 0.0343,
                "tokens_per_second": 59.16,
                "memory_before_gb": 1.359,
                "memory_after_gb": 1.366,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 1.107,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 16.84,
                "p90_inter_token_latency_ms": 17.52
              },
              {
                "tokens_generated": 256,
                "total_time_s": 4.3305,
                "first_token_latency_s": 0.0374,
                "tokens_per_second": 59.12,
                "memory_before_gb": 1.366,
                "memory_after_gb": 1.366,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 1.107,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 16.84,
                "p90_inter_token_latency_ms": 17.55
              }
            ],
            "actual_file_size_gb": 1.09,
            "model_load_time_s": 0.51,
            "backend": {
              "backend": "cuda",
              "n_gpu_layers": -1,
              "details": "NVIDIA GPU d√©tect√©, utilisation de CUDA",
              "inference_mode": "server",
              "server_url": "http://127.0.0.1:8080"
            },
            "memory_after_load_gb": 1.346,
            "server_memory_after_load_gb": 1.088,
            "resource_usage": {
              "n_samples": 24,
              "duration_s": 12.812145709991455,
              "cpu": {
                "avg_percent": 55.0,
                "max_percent": 61.4,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 38.3,
                "max_percent": 38.3,
                "peak_used_gb": 11.93
              },
              "gpu": {
                "avg_utilization_percent": 33.5,
                "max_utilization_percent": 40.0,
                "peak_memory_mb": 1368.0,
                "max_temperature_c": 43.0
              }
            },
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 59.12,
              "std_tokens_per_second": 0.03,
              "min_tokens_per_second": 59.08,
              "max_tokens_per_second": 59.16,
              "avg_first_token_latency_s": 0.0366,
              "avg_inter_token_latency_ms": 16.84,
              "avg_total_time_s": 4.3304,
              "peak_memory_gb": 1.366,
              "stability": "stable"
            }
          }
        },
        "comparison_table": [
          {
            "quantization": "Q2_K",
            "bits": 2,
            "file_size_gb": 0.45,
            "tokens_per_second": 125.8,
            "first_token_latency_s": 0.0329,
            "inter_token_latency_ms": 7.85,
            "peak_memory_gb": 0.802,
            "model_load_time_s": 0.52,
            "stability": "stable"
          },
          {
            "quantization": "Q3_K_M",
            "bits": 3,
            "file_size_gb": 0.513,
            "tokens_per_second": 112.97,
            "first_token_latency_s": 0.0295,
            "inter_token_latency_ms": 8.77,
            "peak_memory_gb": 1.001,
            "model_load_time_s": 0.51,
            "stability": "stable"
          },
          {
            "quantization": "Q4_K_M",
            "bits": 4,
            "file_size_gb": 0.623,
            "tokens_per_second": 97.07,
            "first_token_latency_s": 0.0325,
            "inter_token_latency_ms": 10.21,
            "peak_memory_gb": 1.373,
            "model_load_time_s": 0.51,
            "stability": "stable"
          },
          {
            "quantization": "Q5_K_M",
            "bits": 5,
            "file_size_gb": 0.729,
            "tokens_per_second": 85.1,
            "first_token_latency_s": 0.0337,
            "inter_token_latency_ms": 11.66,
            "peak_memory_gb": 1.027,
            "model_load_time_s": 0.52,
            "stability": "stable"
          },
          {
            "quantization": "Q6_K",
            "bits": 6,
            "file_size_gb": 0.842,
            "tokens_per_second": 74.86,
            "first_token_latency_s": 0.035,
            "inter_token_latency_ms": 13.27,
            "peak_memory_gb": 1.132,
            "model_load_time_s": 0.53,
            "stability": "stable"
          },
          {
            "quantization": "Q8_0",
            "bits": 8,
            "file_size_gb": 1.09,
            "tokens_per_second": 59.12,
            "first_token_latency_s": 0.0366,
            "inter_token_latency_ms": 16.84,
            "peak_memory_gb": 1.366,
            "model_load_time_s": 0.51,
            "stability": "stable"
          }
        ]
      }
    },
    "temperature_comparison": {
      "tinyllama-1.1b": {
        "axis": "temperature",
        "model_name": "TinyLlama 1.1B",
        "model_key": "tinyllama-1.1b",
        "params": "1.1B",
        "variants_tested": [
          "low",
          "medium",
          "high"
        ],
        "inference_mode": "server",
        "status": "completed",
        "results": {
          "low": {
            "temperature": 0.25,
            "label": "Basse (0.25)",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 2.6335,
                "first_token_latency_s": 0.0611,
                "tokens_per_second": 97.21,
                "memory_before_gb": 1.366,
                "memory_after_gb": 1.366,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 1.108,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.09,
                "p90_inter_token_latency_ms": 10.73
              },
              {
                "tokens_generated": 256,
                "total_time_s": 2.6265,
                "first_token_latency_s": 0.0341,
                "tokens_per_second": 97.47,
                "memory_before_gb": 1.366,
                "memory_after_gb": 1.373,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 1.114,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.17,
                "p90_inter_token_latency_ms": 10.69
              },
              {
                "tokens_generated": 256,
                "total_time_s": 2.6361,
                "first_token_latency_s": 0.0315,
                "tokens_per_second": 97.11,
                "memory_before_gb": 1.373,
                "memory_after_gb": 1.373,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 1.114,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.21,
                "p90_inter_token_latency_ms": 10.74
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 97.26,
              "std_tokens_per_second": 0.15,
              "avg_first_token_latency_s": 0.0422,
              "avg_inter_token_latency_ms": 10.16,
              "avg_total_time_s": 2.632,
              "peak_memory_gb": 1.373,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 15,
              "duration_s": 7.769002199172974,
              "cpu": {
                "avg_percent": 54.3,
                "max_percent": 59.0,
                "min_percent": 27.9
              },
              "ram": {
                "avg_percent": 38.3,
                "max_percent": 38.3,
                "peak_used_gb": 11.92
              },
              "gpu": {
                "avg_utilization_percent": 34.2,
                "max_utilization_percent": 41.0,
                "peak_memory_mb": 1352.0,
                "max_temperature_c": 43.0
              }
            }
          },
          "medium": {
            "temperature": 0.5,
            "label": "Moyenne (0.50)",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 2.6475,
                "first_token_latency_s": 0.032,
                "tokens_per_second": 96.7,
                "memory_before_gb": 1.373,
                "memory_after_gb": 1.374,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 1.115,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.26,
                "p90_inter_token_latency_ms": 10.77
              },
              {
                "tokens_generated": 256,
                "total_time_s": 2.64,
                "first_token_latency_s": 0.0347,
                "tokens_per_second": 96.97,
                "memory_before_gb": 1.374,
                "memory_after_gb": 1.38,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 1.122,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.22,
                "p90_inter_token_latency_ms": 10.79
              },
              {
                "tokens_generated": 256,
                "total_time_s": 2.6423,
                "first_token_latency_s": 0.0297,
                "tokens_per_second": 96.89,
                "memory_before_gb": 1.38,
                "memory_after_gb": 1.38,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 1.122,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.25,
                "p90_inter_token_latency_ms": 10.81
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 96.85,
              "std_tokens_per_second": 0.11,
              "avg_first_token_latency_s": 0.0321,
              "avg_inter_token_latency_ms": 10.24,
              "avg_total_time_s": 2.6433,
              "peak_memory_gb": 1.38,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 15,
              "duration_s": 7.749229669570923,
              "cpu": {
                "avg_percent": 53.4,
                "max_percent": 59.8,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 38.3,
                "max_percent": 38.4,
                "peak_used_gb": 11.95
              },
              "gpu": {
                "avg_utilization_percent": 33.5,
                "max_utilization_percent": 43.0,
                "peak_memory_mb": 1359.0,
                "max_temperature_c": 43.0
              }
            }
          },
          "high": {
            "temperature": 0.75,
            "label": "Haute (0.75)",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 2.634,
                "first_token_latency_s": 0.0137,
                "tokens_per_second": 97.19,
                "memory_before_gb": 1.38,
                "memory_after_gb": 1.381,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 1.122,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.28,
                "p90_inter_token_latency_ms": 10.8
              },
              {
                "tokens_generated": 256,
                "total_time_s": 2.6419,
                "first_token_latency_s": 0.0359,
                "tokens_per_second": 96.9,
                "memory_before_gb": 1.381,
                "memory_after_gb": 1.387,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 1.129,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.22,
                "p90_inter_token_latency_ms": 10.77
              },
              {
                "tokens_generated": 256,
                "total_time_s": 2.6415,
                "first_token_latency_s": 0.0254,
                "tokens_per_second": 96.91,
                "memory_before_gb": 1.387,
                "memory_after_gb": 1.387,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 1.129,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.26,
                "p90_inter_token_latency_ms": 10.78
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 97.0,
              "std_tokens_per_second": 0.13,
              "avg_first_token_latency_s": 0.025,
              "avg_inter_token_latency_ms": 10.25,
              "avg_total_time_s": 2.6391,
              "peak_memory_gb": 1.387,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 15,
              "duration_s": 7.700888156890869,
              "cpu": {
                "avg_percent": 53.3,
                "max_percent": 60.5,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 38.4,
                "max_percent": 38.4,
                "peak_used_gb": 11.95
              },
              "gpu": {
                "avg_utilization_percent": 35.0,
                "max_utilization_percent": 47.0,
                "peak_memory_mb": 1367.0,
                "max_temperature_c": 43.0
              }
            }
          }
        },
        "backend": {
          "backend": "cuda",
          "n_gpu_layers": -1,
          "details": "NVIDIA GPU d√©tect√©, utilisation de CUDA",
          "inference_mode": "server"
        },
        "comparison_table": [
          {
            "temperature_key": "low",
            "temperature": 0.25,
            "label": "Basse (0.25)",
            "tokens_per_second": 97.26,
            "first_token_latency_s": 0.0422,
            "inter_token_latency_ms": 10.16,
            "peak_memory_gb": 1.373,
            "stability": "stable"
          },
          {
            "temperature_key": "medium",
            "temperature": 0.5,
            "label": "Moyenne (0.50)",
            "tokens_per_second": 96.85,
            "first_token_latency_s": 0.0321,
            "inter_token_latency_ms": 10.24,
            "peak_memory_gb": 1.38,
            "stability": "stable"
          },
          {
            "temperature_key": "high",
            "temperature": 0.75,
            "label": "Haute (0.75)",
            "tokens_per_second": 97.0,
            "first_token_latency_s": 0.025,
            "inter_token_latency_ms": 10.25,
            "peak_memory_gb": 1.387,
            "stability": "stable"
          }
        ]
      }
    },
    "language_comparison": {
      "tinyllama-1.1b": {
        "axis": "language",
        "model_name": "TinyLlama 1.1B",
        "model_key": "tinyllama-1.1b",
        "params": "1.1B",
        "variants_tested": [
          "en",
          "fr",
          "zh",
          "es",
          "de",
          "ar"
        ],
        "inference_mode": "server",
        "status": "completed",
        "results": {
          "en": {
            "language": "en",
            "label": "Anglais",
            "flag": "üá¨üáß",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 2.7534,
                "first_token_latency_s": 0.0303,
                "tokens_per_second": 92.98,
                "memory_before_gb": 1.366,
                "memory_after_gb": 1.366,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 1.108,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.68,
                "p90_inter_token_latency_ms": 11.31
              },
              {
                "tokens_generated": 256,
                "total_time_s": 2.7634,
                "first_token_latency_s": 0.0257,
                "tokens_per_second": 92.64,
                "memory_before_gb": 1.366,
                "memory_after_gb": 1.373,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 1.114,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.74,
                "p90_inter_token_latency_ms": 11.36
              },
              {
                "tokens_generated": 256,
                "total_time_s": 2.745,
                "first_token_latency_s": 0.0351,
                "tokens_per_second": 93.26,
                "memory_before_gb": 1.373,
                "memory_after_gb": 1.373,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 1.114,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.63,
                "p90_inter_token_latency_ms": 11.16
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 92.96,
              "std_tokens_per_second": 0.25,
              "avg_first_token_latency_s": 0.0304,
              "avg_inter_token_latency_ms": 10.68,
              "avg_total_time_s": 2.7539,
              "peak_memory_gb": 1.373,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 15,
              "duration_s": 7.774663209915161,
              "cpu": {
                "avg_percent": 53.5,
                "max_percent": 58.8,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 38.3,
                "max_percent": 38.4,
                "peak_used_gb": 11.93
              },
              "gpu": {
                "avg_utilization_percent": 33.2,
                "max_utilization_percent": 42.0,
                "peak_memory_mb": 1344.0,
                "max_temperature_c": 43.0
              }
            }
          },
          "fr": {
            "language": "fr",
            "label": "Fran√ßais",
            "flag": "üá´üá∑",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 2.8087,
                "first_token_latency_s": 0.069,
                "tokens_per_second": 91.14,
                "memory_before_gb": 1.373,
                "memory_after_gb": 1.374,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 1.115,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.74,
                "p90_inter_token_latency_ms": 11.37
              },
              {
                "tokens_generated": 256,
                "total_time_s": 2.7758,
                "first_token_latency_s": 0.0268,
                "tokens_per_second": 92.22,
                "memory_before_gb": 1.374,
                "memory_after_gb": 1.381,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 1.122,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.78,
                "p90_inter_token_latency_ms": 11.32
              },
              {
                "tokens_generated": 256,
                "total_time_s": 2.775,
                "first_token_latency_s": 0.0288,
                "tokens_per_second": 92.25,
                "memory_before_gb": 1.381,
                "memory_after_gb": 1.381,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 1.122,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.77,
                "p90_inter_token_latency_ms": 11.36
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 91.87,
              "std_tokens_per_second": 0.52,
              "avg_first_token_latency_s": 0.0415,
              "avg_inter_token_latency_ms": 10.76,
              "avg_total_time_s": 2.7865,
              "peak_memory_gb": 1.381,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 16,
              "duration_s": 8.34291672706604,
              "cpu": {
                "avg_percent": 54.2,
                "max_percent": 60.2,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 38.2,
                "max_percent": 38.3,
                "peak_used_gb": 11.92
              },
              "gpu": {
                "avg_utilization_percent": 33.3,
                "max_utilization_percent": 46.0,
                "peak_memory_mb": 1344.0,
                "max_temperature_c": 43.0
              }
            }
          },
          "zh": {
            "language": "zh",
            "label": "Mandarin",
            "flag": "üá®üá≥",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 2.8522,
                "first_token_latency_s": 0.0938,
                "tokens_per_second": 89.76,
                "memory_before_gb": 1.381,
                "memory_after_gb": 1.385,
                "memory_delta_gb": 0.004,
                "server_memory_gb": 1.126,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.82,
                "p90_inter_token_latency_ms": 11.39
              },
              {
                "tokens_generated": 256,
                "total_time_s": 2.7153,
                "first_token_latency_s": 0.0252,
                "tokens_per_second": 94.28,
                "memory_before_gb": 1.385,
                "memory_after_gb": 1.393,
                "memory_delta_gb": 0.008,
                "server_memory_gb": 1.134,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.55,
                "p90_inter_token_latency_ms": 11.08
              },
              {
                "tokens_generated": 256,
                "total_time_s": 2.7237,
                "first_token_latency_s": 0.0396,
                "tokens_per_second": 93.99,
                "memory_before_gb": 1.393,
                "memory_after_gb": 1.401,
                "memory_delta_gb": 0.008,
                "server_memory_gb": 1.141,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.53,
                "p90_inter_token_latency_ms": 11.17
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 92.68,
              "std_tokens_per_second": 2.07,
              "avg_first_token_latency_s": 0.0529,
              "avg_inter_token_latency_ms": 10.63,
              "avg_total_time_s": 2.7637,
              "peak_memory_gb": 1.401,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 16,
              "duration_s": 8.249925136566162,
              "cpu": {
                "avg_percent": 53.7,
                "max_percent": 59.2,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 38.3,
                "max_percent": 38.3,
                "peak_used_gb": 11.91
              },
              "gpu": {
                "avg_utilization_percent": 31.1,
                "max_utilization_percent": 39.0,
                "peak_memory_mb": 1353.0,
                "max_temperature_c": 43.0
              }
            }
          },
          "es": {
            "language": "es",
            "label": "Espagnol",
            "flag": "üá™üá∏",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 2.7362,
                "first_token_latency_s": 0.0755,
                "tokens_per_second": 93.56,
                "memory_before_gb": 1.401,
                "memory_after_gb": 1.402,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 1.142,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.43,
                "p90_inter_token_latency_ms": 10.94
              },
              {
                "tokens_generated": 256,
                "total_time_s": 2.7091,
                "first_token_latency_s": 0.0277,
                "tokens_per_second": 94.5,
                "memory_before_gb": 1.402,
                "memory_after_gb": 1.409,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 1.149,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.51,
                "p90_inter_token_latency_ms": 11.13
              },
              {
                "tokens_generated": 256,
                "total_time_s": 2.6898,
                "first_token_latency_s": 0.0296,
                "tokens_per_second": 95.18,
                "memory_before_gb": 1.409,
                "memory_after_gb": 1.409,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 1.149,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.43,
                "p90_inter_token_latency_ms": 10.96
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 94.41,
              "std_tokens_per_second": 0.66,
              "avg_first_token_latency_s": 0.0443,
              "avg_inter_token_latency_ms": 10.46,
              "avg_total_time_s": 2.7117,
              "peak_memory_gb": 1.409,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 15,
              "duration_s": 7.762034177780151,
              "cpu": {
                "avg_percent": 53.3,
                "max_percent": 61.1,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 38.3,
                "max_percent": 38.3,
                "peak_used_gb": 11.92
              },
              "gpu": {
                "avg_utilization_percent": 31.3,
                "max_utilization_percent": 39.0,
                "peak_memory_mb": 1353.0,
                "max_temperature_c": 43.0
              }
            }
          },
          "de": {
            "language": "de",
            "label": "Allemand",
            "flag": "üá©üá™",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 2.6929,
                "first_token_latency_s": 0.0709,
                "tokens_per_second": 95.07,
                "memory_before_gb": 1.409,
                "memory_after_gb": 1.41,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 1.149,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.28,
                "p90_inter_token_latency_ms": 10.81
              },
              {
                "tokens_generated": 256,
                "total_time_s": 2.6414,
                "first_token_latency_s": 0.0242,
                "tokens_per_second": 96.92,
                "memory_before_gb": 1.41,
                "memory_after_gb": 1.417,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 1.156,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.26,
                "p90_inter_token_latency_ms": 10.76
              },
              {
                "tokens_generated": 256,
                "total_time_s": 2.6589,
                "first_token_latency_s": 0.0295,
                "tokens_per_second": 96.28,
                "memory_before_gb": 1.417,
                "memory_after_gb": 1.418,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 1.157,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.31,
                "p90_inter_token_latency_ms": 10.89
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 96.09,
              "std_tokens_per_second": 0.77,
              "avg_first_token_latency_s": 0.0415,
              "avg_inter_token_latency_ms": 10.28,
              "avg_total_time_s": 2.6644,
              "peak_memory_gb": 1.418,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 15,
              "duration_s": 7.709413051605225,
              "cpu": {
                "avg_percent": 52.8,
                "max_percent": 59.7,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 38.3,
                "max_percent": 38.3,
                "peak_used_gb": 11.93
              },
              "gpu": {
                "avg_utilization_percent": 30.5,
                "max_utilization_percent": 40.0,
                "peak_memory_mb": 1344.0,
                "max_temperature_c": 43.0
              }
            }
          },
          "ar": {
            "language": "ar",
            "label": "Arabe",
            "flag": "üá∏üá¶",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 2.7589,
                "first_token_latency_s": 0.1062,
                "tokens_per_second": 92.79,
                "memory_before_gb": 1.418,
                "memory_after_gb": 1.424,
                "memory_delta_gb": 0.006,
                "server_memory_gb": 1.163,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.4,
                "p90_inter_token_latency_ms": 10.93
              },
              {
                "tokens_generated": 256,
                "total_time_s": 2.6696,
                "first_token_latency_s": 0.0265,
                "tokens_per_second": 95.9,
                "memory_before_gb": 1.424,
                "memory_after_gb": 1.433,
                "memory_delta_gb": 0.009,
                "server_memory_gb": 1.171,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.36,
                "p90_inter_token_latency_ms": 10.87
              },
              {
                "tokens_generated": 256,
                "total_time_s": 2.6798,
                "first_token_latency_s": 0.0331,
                "tokens_per_second": 95.53,
                "memory_before_gb": 1.433,
                "memory_after_gb": 1.442,
                "memory_delta_gb": 0.009,
                "server_memory_gb": 1.179,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.38,
                "p90_inter_token_latency_ms": 10.94
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 94.74,
              "std_tokens_per_second": 1.39,
              "avg_first_token_latency_s": 0.0553,
              "avg_inter_token_latency_ms": 10.38,
              "avg_total_time_s": 2.7028,
              "peak_memory_gb": 1.442,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 15,
              "duration_s": 7.719546318054199,
              "cpu": {
                "avg_percent": 52.3,
                "max_percent": 57.5,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 38.4,
                "max_percent": 38.4,
                "peak_used_gb": 11.95
              },
              "gpu": {
                "avg_utilization_percent": 32.3,
                "max_utilization_percent": 41.0,
                "peak_memory_mb": 1352.0,
                "max_temperature_c": 43.0
              }
            }
          }
        },
        "backend": {
          "backend": "cuda",
          "n_gpu_layers": -1,
          "details": "NVIDIA GPU d√©tect√©, utilisation de CUDA",
          "inference_mode": "server"
        },
        "comparison_table": [
          {
            "language_key": "en",
            "label": "Anglais",
            "flag": "üá¨üáß",
            "tokens_per_second": 92.96,
            "first_token_latency_s": 0.0304,
            "inter_token_latency_ms": 10.68,
            "peak_memory_gb": 1.373,
            "stability": "stable"
          },
          {
            "language_key": "fr",
            "label": "Fran√ßais",
            "flag": "üá´üá∑",
            "tokens_per_second": 91.87,
            "first_token_latency_s": 0.0415,
            "inter_token_latency_ms": 10.76,
            "peak_memory_gb": 1.381,
            "stability": "stable"
          },
          {
            "language_key": "zh",
            "label": "Mandarin",
            "flag": "üá®üá≥",
            "tokens_per_second": 92.68,
            "first_token_latency_s": 0.0529,
            "inter_token_latency_ms": 10.63,
            "peak_memory_gb": 1.401,
            "stability": "stable"
          },
          {
            "language_key": "es",
            "label": "Espagnol",
            "flag": "üá™üá∏",
            "tokens_per_second": 94.41,
            "first_token_latency_s": 0.0443,
            "inter_token_latency_ms": 10.46,
            "peak_memory_gb": 1.409,
            "stability": "stable"
          },
          {
            "language_key": "de",
            "label": "Allemand",
            "flag": "üá©üá™",
            "tokens_per_second": 96.09,
            "first_token_latency_s": 0.0415,
            "inter_token_latency_ms": 10.28,
            "peak_memory_gb": 1.418,
            "stability": "stable"
          },
          {
            "language_key": "ar",
            "label": "Arabe",
            "flag": "üá∏üá¶",
            "tokens_per_second": 94.74,
            "first_token_latency_s": 0.0553,
            "inter_token_latency_ms": 10.38,
            "peak_memory_gb": 1.442,
            "stability": "stable"
          }
        ]
      }
    },
    "prompt_type_comparison": {
      "tinyllama-1.1b": {
        "axis": "prompt_type",
        "model_name": "TinyLlama 1.1B",
        "model_key": "tinyllama-1.1b",
        "params": "1.1B",
        "variants_tested": [
          "general",
          "code",
          "reasoning",
          "creative",
          "math"
        ],
        "inference_mode": "server",
        "status": "completed",
        "results": {
          "general": {
            "prompt_type": "general",
            "label": "G√©n√©ral / Connaissances",
            "icon": "üìö",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 2.6296,
                "first_token_latency_s": 0.0308,
                "tokens_per_second": 97.35,
                "memory_before_gb": 1.369,
                "memory_after_gb": 1.37,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 1.108,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.19,
                "p90_inter_token_latency_ms": 10.69
              },
              {
                "tokens_generated": 256,
                "total_time_s": 2.6362,
                "first_token_latency_s": 0.0372,
                "tokens_per_second": 97.11,
                "memory_before_gb": 1.37,
                "memory_after_gb": 1.377,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 1.114,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.19,
                "p90_inter_token_latency_ms": 10.76
              },
              {
                "tokens_generated": 256,
                "total_time_s": 2.6238,
                "first_token_latency_s": 0.0311,
                "tokens_per_second": 97.57,
                "memory_before_gb": 1.377,
                "memory_after_gb": 1.377,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 1.114,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.17,
                "p90_inter_token_latency_ms": 10.64
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 97.34,
              "std_tokens_per_second": 0.19,
              "avg_first_token_latency_s": 0.033,
              "avg_inter_token_latency_ms": 10.18,
              "avg_total_time_s": 2.6299,
              "peak_memory_gb": 1.377,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 15,
              "duration_s": 7.7521116733551025,
              "cpu": {
                "avg_percent": 51.9,
                "max_percent": 57.3,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 38.2,
                "max_percent": 38.3,
                "peak_used_gb": 11.9
              },
              "gpu": {
                "avg_utilization_percent": 31.5,
                "max_utilization_percent": 38.0,
                "peak_memory_mb": 1352.0,
                "max_temperature_c": 43.0
              }
            }
          },
          "code": {
            "prompt_type": "code",
            "label": "Code / Programmation",
            "icon": "üíª",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 2.6872,
                "first_token_latency_s": 0.0798,
                "tokens_per_second": 95.27,
                "memory_before_gb": 1.377,
                "memory_after_gb": 1.379,
                "memory_delta_gb": 0.002,
                "server_memory_gb": 1.117,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.22,
                "p90_inter_token_latency_ms": 10.72
              },
              {
                "tokens_generated": 256,
                "total_time_s": 2.6435,
                "first_token_latency_s": 0.0318,
                "tokens_per_second": 96.84,
                "memory_before_gb": 1.379,
                "memory_after_gb": 1.386,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 1.124,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.24,
                "p90_inter_token_latency_ms": 10.78
              },
              {
                "tokens_generated": 256,
                "total_time_s": 2.6372,
                "first_token_latency_s": 0.0345,
                "tokens_per_second": 97.07,
                "memory_before_gb": 1.386,
                "memory_after_gb": 1.394,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 1.132,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.21,
                "p90_inter_token_latency_ms": 10.79
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 96.39,
              "std_tokens_per_second": 0.8,
              "avg_first_token_latency_s": 0.0487,
              "avg_inter_token_latency_ms": 10.22,
              "avg_total_time_s": 2.656,
              "peak_memory_gb": 1.394,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 15,
              "duration_s": 7.759425163269043,
              "cpu": {
                "avg_percent": 51.8,
                "max_percent": 56.7,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 38.3,
                "max_percent": 38.3,
                "peak_used_gb": 11.92
              },
              "gpu": {
                "avg_utilization_percent": 31.7,
                "max_utilization_percent": 43.0,
                "peak_memory_mb": 1344.0,
                "max_temperature_c": 42.0
              }
            }
          },
          "reasoning": {
            "prompt_type": "reasoning",
            "label": "Raisonnement / Logique",
            "icon": "üß†",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 2.7198,
                "first_token_latency_s": 0.0923,
                "tokens_per_second": 94.12,
                "memory_before_gb": 1.394,
                "memory_after_gb": 1.397,
                "memory_delta_gb": 0.003,
                "server_memory_gb": 1.135,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.3,
                "p90_inter_token_latency_ms": 10.95
              },
              {
                "tokens_generated": 256,
                "total_time_s": 2.6733,
                "first_token_latency_s": 0.0363,
                "tokens_per_second": 95.76,
                "memory_before_gb": 1.397,
                "memory_after_gb": 1.405,
                "memory_delta_gb": 0.008,
                "server_memory_gb": 1.143,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.34,
                "p90_inter_token_latency_ms": 10.91
              },
              {
                "tokens_generated": 256,
                "total_time_s": 2.6966,
                "first_token_latency_s": 0.027,
                "tokens_per_second": 94.93,
                "memory_before_gb": 1.405,
                "memory_after_gb": 1.413,
                "memory_delta_gb": 0.008,
                "server_memory_gb": 1.151,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.47,
                "p90_inter_token_latency_ms": 10.99
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 94.94,
              "std_tokens_per_second": 0.67,
              "avg_first_token_latency_s": 0.0519,
              "avg_inter_token_latency_ms": 10.37,
              "avg_total_time_s": 2.6966,
              "peak_memory_gb": 1.413,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 15,
              "duration_s": 7.738031625747681,
              "cpu": {
                "avg_percent": 52.5,
                "max_percent": 59.7,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 38.2,
                "max_percent": 38.3,
                "peak_used_gb": 11.91
              },
              "gpu": {
                "avg_utilization_percent": 29.8,
                "max_utilization_percent": 34.0,
                "peak_memory_mb": 1361.0,
                "max_temperature_c": 42.0
              }
            }
          },
          "creative": {
            "prompt_type": "creative",
            "label": "Cr√©atif / R√©daction",
            "icon": "‚úçÔ∏è",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 2.6898,
                "first_token_latency_s": 0.0673,
                "tokens_per_second": 95.17,
                "memory_before_gb": 1.413,
                "memory_after_gb": 1.413,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 1.151,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.28,
                "p90_inter_token_latency_ms": 10.83
              },
              {
                "tokens_generated": 256,
                "total_time_s": 2.6438,
                "first_token_latency_s": 0.0303,
                "tokens_per_second": 96.83,
                "memory_before_gb": 1.413,
                "memory_after_gb": 1.42,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 1.158,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.25,
                "p90_inter_token_latency_ms": 10.74
              },
              {
                "tokens_generated": 256,
                "total_time_s": 2.6271,
                "first_token_latency_s": 0.0268,
                "tokens_per_second": 97.44,
                "memory_before_gb": 1.42,
                "memory_after_gb": 1.42,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 1.158,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.2,
                "p90_inter_token_latency_ms": 10.82
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 96.48,
              "std_tokens_per_second": 0.96,
              "avg_first_token_latency_s": 0.0415,
              "avg_inter_token_latency_ms": 10.24,
              "avg_total_time_s": 2.6536,
              "peak_memory_gb": 1.42,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 15,
              "duration_s": 7.7266106605529785,
              "cpu": {
                "avg_percent": 52.1,
                "max_percent": 57.0,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 38.3,
                "max_percent": 38.3,
                "peak_used_gb": 11.92
              },
              "gpu": {
                "avg_utilization_percent": 29.9,
                "max_utilization_percent": 33.0,
                "peak_memory_mb": 1349.0,
                "max_temperature_c": 42.0
              }
            }
          },
          "math": {
            "prompt_type": "math",
            "label": "Math√©matiques",
            "icon": "üî¢",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 2.7481,
                "first_token_latency_s": 0.1126,
                "tokens_per_second": 93.15,
                "memory_before_gb": 1.42,
                "memory_after_gb": 1.422,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 1.159,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.33,
                "p90_inter_token_latency_ms": 10.91
              },
              {
                "tokens_generated": 256,
                "total_time_s": 2.6679,
                "first_token_latency_s": 0.0308,
                "tokens_per_second": 95.95,
                "memory_before_gb": 1.422,
                "memory_after_gb": 1.43,
                "memory_delta_gb": 0.008,
                "server_memory_gb": 1.167,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.34,
                "p90_inter_token_latency_ms": 10.92
              },
              {
                "tokens_generated": 256,
                "total_time_s": 2.6642,
                "first_token_latency_s": 0.0382,
                "tokens_per_second": 96.09,
                "memory_before_gb": 1.43,
                "memory_after_gb": 1.438,
                "memory_delta_gb": 0.008,
                "server_memory_gb": 1.175,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 10.3,
                "p90_inter_token_latency_ms": 10.84
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 95.06,
              "std_tokens_per_second": 1.35,
              "avg_first_token_latency_s": 0.0605,
              "avg_inter_token_latency_ms": 10.32,
              "avg_total_time_s": 2.6934,
              "peak_memory_gb": 1.438,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 15,
              "duration_s": 7.694031238555908,
              "cpu": {
                "avg_percent": 52.3,
                "max_percent": 57.4,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 38.3,
                "max_percent": 38.4,
                "peak_used_gb": 11.94
              },
              "gpu": {
                "avg_utilization_percent": 31.9,
                "max_utilization_percent": 40.0,
                "peak_memory_mb": 1357.0,
                "max_temperature_c": 42.0
              }
            }
          }
        },
        "backend": {
          "backend": "cuda",
          "n_gpu_layers": -1,
          "details": "NVIDIA GPU d√©tect√©, utilisation de CUDA",
          "inference_mode": "server"
        },
        "comparison_table": [
          {
            "prompt_type_key": "general",
            "label": "G√©n√©ral / Connaissances",
            "icon": "üìö",
            "tokens_per_second": 97.34,
            "first_token_latency_s": 0.033,
            "inter_token_latency_ms": 10.18,
            "peak_memory_gb": 1.377,
            "stability": "stable"
          },
          {
            "prompt_type_key": "code",
            "label": "Code / Programmation",
            "icon": "üíª",
            "tokens_per_second": 96.39,
            "first_token_latency_s": 0.0487,
            "inter_token_latency_ms": 10.22,
            "peak_memory_gb": 1.394,
            "stability": "stable"
          },
          {
            "prompt_type_key": "reasoning",
            "label": "Raisonnement / Logique",
            "icon": "üß†",
            "tokens_per_second": 94.94,
            "first_token_latency_s": 0.0519,
            "inter_token_latency_ms": 10.37,
            "peak_memory_gb": 1.413,
            "stability": "stable"
          },
          {
            "prompt_type_key": "creative",
            "label": "Cr√©atif / R√©daction",
            "icon": "‚úçÔ∏è",
            "tokens_per_second": 96.48,
            "first_token_latency_s": 0.0415,
            "inter_token_latency_ms": 10.24,
            "peak_memory_gb": 1.42,
            "stability": "stable"
          },
          {
            "prompt_type_key": "math",
            "label": "Math√©matiques",
            "icon": "üî¢",
            "tokens_per_second": 95.06,
            "first_token_latency_s": 0.0605,
            "inter_token_latency_ms": 10.32,
            "peak_memory_gb": 1.438,
            "stability": "stable"
          }
        ]
      }
    }
  }
}