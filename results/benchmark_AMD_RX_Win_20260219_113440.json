{
  "id": "20260219_113440",
  "timestamp": "2026-02-19T11:34:40.073100",
  "version": "1.0.0",
  "hardware": {
    "os": {
      "system": "Windows",
      "release": "10",
      "version": "10.0.26200",
      "machine": "AMD64",
      "architecture": "64bit",
      "platform": "Windows-10-10.0.26200-SP0",
      "python_version": "3.9.13"
    },
    "cpu": {
      "physical_cores": 8,
      "logical_cores": 16,
      "architecture": "AMD64",
      "model": "AMD Ryzen 7 4800H with Radeon Graphics",
      "max_clock_speed_mhz": "2900",
      "architecture_type": "x86_64",
      "is_apple_silicon": false,
      "frequency_mhz": {
        "current": 2900.0,
        "min": null,
        "max": 2900.0
      }
    },
    "gpu": {
      "gpus": [
        {
          "type": "AMD",
          "name": "AMD Radeon(TM) Graphics",
          "backend": "directml",
          "detected_via": "wmi",
          "vram_total_mb": 512,
          "driver_version": "31.0.21921.1000",
          "gpu_index": 0
        },
        {
          "type": "AMD",
          "name": "Radeon RX 5500M",
          "backend": "directml",
          "detected_via": "wmi",
          "vram_total_mb": 4080,
          "driver_version": "32.0.12019.1028",
          "gpu_index": 1
        }
      ],
      "backends": [
        "directml",
        "cpu"
      ],
      "primary_backend": "directml",
      "python_backends": {
        "pytorch": true,
        "pytorch_version": "2.8.0+cpu",
        "pytorch_cuda": false,
        "pytorch_mps": false,
        "pytorch_xpu": false,
        "llama_cpp": false,
        "llama_server": false,
        "pytorch_rocm": false,
        "ipex": false,
        "directml": false
      }
    },
    "ram": {
      "total_gb": 15.4,
      "available_gb": 0.69,
      "used_gb": 14.71,
      "percent_used": 95.5,
      "swap_total_gb": 10.5,
      "swap_used_gb": 0.61,
      "unified_memory": false
    },
    "selected_gpu": {
      "gpu_index": 1,
      "name": "Radeon RX 5500M",
      "backend": "directml"
    }
  },
  "classic_benchmarks": {
    "type": "classic_benchmarks",
    "total_time_s": 12.39,
    "benchmarks": {
      "cpu_single_thread": {
        "test": "CPU Single-Thread",
        "method": "Matrix multiplication (NumPy, 1 thread, subprocess isol√©)",
        "results": {
          "512x512": {
            "times_s": [
              0.0029,
              0.0029,
              0.0028
            ],
            "mean_s": 0.0028,
            "median_s": 0.0029,
            "std_s": 0.0,
            "gflops": 94.12
          },
          "1024x1024": {
            "times_s": [
              0.0202,
              0.0201,
              0.0199
            ],
            "mean_s": 0.0201,
            "median_s": 0.0201,
            "std_s": 0.0001,
            "gflops": 107.01
          },
          "2048x2048": {
            "times_s": [
              0.161,
              0.1629,
              0.1597
            ],
            "mean_s": 0.1612,
            "median_s": 0.161,
            "std_s": 0.0013,
            "gflops": 106.71
          }
        }
      },
      "cpu_multi_thread": {
        "test": "CPU Multi-Thread",
        "method": "Matrix multiplication (NumPy, 16 threads, subprocess isol√©)",
        "n_threads": 16,
        "results": {
          "512x512": {
            "times_s": [
              0.0018,
              0.002,
              0.0017
            ],
            "mean_s": 0.0018,
            "median_s": 0.0018,
            "std_s": 0.0001,
            "gflops": 148.64
          },
          "1024x1024": {
            "times_s": [
              0.0068,
              0.0071,
              0.0064
            ],
            "mean_s": 0.0068,
            "median_s": 0.0068,
            "std_s": 0.0003,
            "gflops": 314.54
          },
          "2048x2048": {
            "times_s": [
              0.0396,
              0.0384,
              0.0408
            ],
            "mean_s": 0.0396,
            "median_s": 0.0396,
            "std_s": 0.001,
            "gflops": 434.18
          }
        }
      },
      "memory_bandwidth": {
        "test": "Memory Bandwidth",
        "method": "NumPy array operations (256 Mo)",
        "data_size_gb": 0.25,
        "results": {
          "write": {
            "mean_s": 0.0808,
            "median_s": 0.0785,
            "bandwidth_gb_s": 3.18
          },
          "read": {
            "mean_s": 0.0516,
            "median_s": 0.05,
            "bandwidth_gb_s": 5.0
          },
          "copy": {
            "mean_s": 0.0747,
            "median_s": 0.0812,
            "bandwidth_gb_s": 3.08
          }
        }
      },
      "gpu_compute": {
        "test": "GPU Compute (Raw)",
        "status": "skipped",
        "reason": "GPU AMD (AMD Radeon(TM) Graphics) d√©tect√©, mais PyTorch n'a pas le support ROCm activ√©.",
        "advice": "ROCm n'est pas disponible sur Windows. Installez torch-directml pour activer le support GPU AMD :\npip install torch-directml"
      },
      "gpu_system": {
        "test": "GPU System Score",
        "status": "skipped",
        "reason": "GPU AMD (AMD Radeon(TM) Graphics) d√©tect√©, mais PyTorch n'a pas le support ROCm activ√©.",
        "advice": "ROCm n'est pas disponible sur Windows. Installez torch-directml pour activer le support GPU AMD :\npip install torch-directml"
      }
    },
    "resource_usage": {
      "n_samples": 22,
      "duration_s": 11.845240354537964,
      "cpu": {
        "avg_percent": 22.6,
        "max_percent": 39.0,
        "min_percent": 0.0
      },
      "ram": {
        "avg_percent": 94.1,
        "max_percent": 97.5,
        "peak_used_gb": 15.01
      }
    }
  },
  "ai_benchmarks": {
    "type": "ai_benchmarks",
    "inference_mode": "server",
    "prompt": "Explain the concept of artificial intelligence in simple terms. What are its main applications and how does it impact our daily lives? Provide specific examples.",
    "inference_config": {
      "max_tokens": 256,
      "temperature": 0.7,
      "top_p": 0.9,
      "repeat_penalty": 1.1,
      "n_ctx": 2048,
      "seed": 42,
      "n_warmup_runs": 2,
      "n_benchmark_runs": 3
    },
    "total_time_s": 399.73,
    "models_tested": 1,
    "results": {
      "tinyllama-1.1b": {
        "model": "TinyLlama 1.1B",
        "params": "1.1B",
        "status": "completed",
        "inference_mode": "server",
        "runs": [
          {
            "tokens_generated": 256,
            "total_time_s": 5.2695,
            "first_token_latency_s": 0.0264,
            "tokens_per_second": 48.58,
            "memory_before_gb": 0.947,
            "memory_after_gb": 0.948,
            "memory_delta_gb": 0.001,
            "server_memory_gb": 0.741,
            "error": null,
            "success": true,
            "inference_mode": "server",
            "avg_inter_token_latency_ms": 20.56,
            "p90_inter_token_latency_ms": 21.86
          },
          {
            "tokens_generated": 256,
            "total_time_s": 5.4152,
            "first_token_latency_s": 0.0418,
            "tokens_per_second": 47.27,
            "memory_before_gb": 0.948,
            "memory_after_gb": 0.956,
            "memory_delta_gb": 0.007,
            "server_memory_gb": 0.748,
            "error": null,
            "success": true,
            "inference_mode": "server",
            "avg_inter_token_latency_ms": 21.07,
            "p90_inter_token_latency_ms": 22.94
          },
          {
            "tokens_generated": 256,
            "total_time_s": 5.2584,
            "first_token_latency_s": 0.045,
            "tokens_per_second": 48.68,
            "memory_before_gb": 0.956,
            "memory_after_gb": 0.955,
            "memory_delta_gb": -0.0,
            "server_memory_gb": 0.748,
            "error": null,
            "success": true,
            "inference_mode": "server",
            "avg_inter_token_latency_ms": 20.44,
            "p90_inter_token_latency_ms": 21.76
          }
        ],
        "model_load_time_s": 2.84,
        "backend": {
          "backend": "cpu",
          "n_gpu_layers": 0,
          "details": "AMD GPU d√©tect√© (AMD Radeon(TM) Graphics) ‚Äî DirectML n'est pas support√© par llama-cpp-python. Utilisez le mode llama-server avec un binaire Vulkan pour l'acc√©l√©ration GPU, sinon fallback CPU.",
          "inference_mode": "server",
          "server_url": "http://127.0.0.1:8080"
        },
        "resource_usage": {
          "n_samples": 29,
          "duration_s": 15.978121757507324,
          "cpu": {
            "avg_percent": 62.4,
            "max_percent": 76.6,
            "min_percent": 0.0
          },
          "ram": {
            "avg_percent": 93.8,
            "max_percent": 94.7,
            "peak_used_gb": 14.59
          }
        },
        "summary": {
          "n_successful_runs": 3,
          "n_total_runs": 3,
          "avg_tokens_per_second": 48.18,
          "std_tokens_per_second": 0.64,
          "avg_first_token_latency_s": 0.0377,
          "avg_total_time_s": 5.3144,
          "peak_memory_gb": 0.956,
          "stability": "stable"
        }
      }
    },
    "quantization_comparison": {
      "tinyllama-1.1b": {
        "model_name": "TinyLlama 1.1B",
        "model_key": "tinyllama-1.1b",
        "params": "1.1B",
        "variants_tested": [
          "Q2_K",
          "Q3_K_M",
          "Q4_K_M",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "results": {
          "Q2_K": {
            "quantization": "Q2_K",
            "bits": 2,
            "file_size_gb": 0.45,
            "status": "completed",
            "inference_mode": "server",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 3.8898,
                "first_token_latency_s": 0.0204,
                "tokens_per_second": 65.81,
                "memory_before_gb": 0.79,
                "memory_after_gb": 0.791,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 0.584,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 15.17,
                "p90_inter_token_latency_ms": 16.51
              },
              {
                "tokens_generated": 256,
                "total_time_s": 3.9077,
                "first_token_latency_s": 0.0384,
                "tokens_per_second": 65.51,
                "memory_before_gb": 0.791,
                "memory_after_gb": 0.797,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.59,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 15.17,
                "p90_inter_token_latency_ms": 16.46
              },
              {
                "tokens_generated": 256,
                "total_time_s": 4.0154,
                "first_token_latency_s": 0.0205,
                "tokens_per_second": 63.76,
                "memory_before_gb": 0.797,
                "memory_after_gb": 0.798,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 0.591,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 15.66,
                "p90_inter_token_latency_ms": 17.03
              }
            ],
            "actual_file_size_gb": 0.45,
            "model_load_time_s": 1.54,
            "backend": {
              "backend": "cpu",
              "n_gpu_layers": 0,
              "details": "AMD GPU d√©tect√© (AMD Radeon(TM) Graphics) ‚Äî DirectML n'est pas support√© par llama-cpp-python. Utilisez le mode llama-server avec un binaire Vulkan pour l'acc√©l√©ration GPU, sinon fallback CPU.",
              "inference_mode": "server",
              "server_url": "http://127.0.0.1:8080"
            },
            "memory_after_load_gb": 0.782,
            "server_memory_after_load_gb": 0.575,
            "resource_usage": {
              "n_samples": 22,
              "duration_s": 11.770447254180908,
              "cpu": {
                "avg_percent": 54.7,
                "max_percent": 65.4,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 92.5,
                "max_percent": 93.2,
                "peak_used_gb": 14.36
              }
            },
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 65.03,
              "std_tokens_per_second": 0.9,
              "min_tokens_per_second": 63.76,
              "max_tokens_per_second": 65.81,
              "avg_first_token_latency_s": 0.0264,
              "avg_inter_token_latency_ms": 15.33,
              "avg_total_time_s": 3.9376,
              "peak_memory_gb": 0.798,
              "stability": "stable"
            }
          },
          "Q3_K_M": {
            "quantization": "Q3_K_M",
            "bits": 3,
            "file_size_gb": 0.51,
            "status": "completed",
            "inference_mode": "server",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 4.5961,
                "first_token_latency_s": 0.0229,
                "tokens_per_second": 55.7,
                "memory_before_gb": 0.809,
                "memory_after_gb": 0.801,
                "memory_delta_gb": -0.008,
                "server_memory_gb": 0.631,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 17.93,
                "p90_inter_token_latency_ms": 19.06
              },
              {
                "tokens_generated": 256,
                "total_time_s": 4.3545,
                "first_token_latency_s": 0.0486,
                "tokens_per_second": 58.79,
                "memory_before_gb": 0.796,
                "memory_after_gb": 0.761,
                "memory_delta_gb": -0.035,
                "server_memory_gb": 0.592,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 16.88,
                "p90_inter_token_latency_ms": 18.32
              },
              {
                "tokens_generated": 256,
                "total_time_s": 4.6022,
                "first_token_latency_s": 0.0382,
                "tokens_per_second": 55.63,
                "memory_before_gb": 0.761,
                "memory_after_gb": 0.761,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 0.593,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 17.9,
                "p90_inter_token_latency_ms": 19.89
              }
            ],
            "actual_file_size_gb": 0.513,
            "model_load_time_s": 1.56,
            "backend": {
              "backend": "cpu",
              "n_gpu_layers": 0,
              "details": "AMD GPU d√©tect√© (AMD Radeon(TM) Graphics) ‚Äî DirectML n'est pas support√© par llama-cpp-python. Utilisez le mode llama-server avec un binaire Vulkan pour l'acc√©l√©ration GPU, sinon fallback CPU.",
              "inference_mode": "server",
              "server_url": "http://127.0.0.1:8080"
            },
            "memory_after_load_gb": 0.8,
            "server_memory_after_load_gb": 0.631,
            "resource_usage": {
              "n_samples": 25,
              "duration_s": 13.582398414611816,
              "cpu": {
                "avg_percent": 58.6,
                "max_percent": 71.6,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 89.7,
                "max_percent": 91.7,
                "peak_used_gb": 14.13
              }
            },
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 56.71,
              "std_tokens_per_second": 1.47,
              "min_tokens_per_second": 55.63,
              "max_tokens_per_second": 58.79,
              "avg_first_token_latency_s": 0.0366,
              "avg_inter_token_latency_ms": 17.57,
              "avg_total_time_s": 4.5176,
              "peak_memory_gb": 0.801,
              "stability": "stable"
            }
          },
          "Q4_K_M": {
            "quantization": "Q4_K_M",
            "bits": 4,
            "file_size_gb": 0.62,
            "status": "completed",
            "inference_mode": "server",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 4.9415,
                "first_token_latency_s": 0.0249,
                "tokens_per_second": 51.81,
                "memory_before_gb": 0.91,
                "memory_after_gb": 0.911,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 0.743,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.28,
                "p90_inter_token_latency_ms": 20.83
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.0948,
                "first_token_latency_s": 0.0264,
                "tokens_per_second": 50.25,
                "memory_before_gb": 0.911,
                "memory_after_gb": 0.918,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.749,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.88,
                "p90_inter_token_latency_ms": 21.37
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.1878,
                "first_token_latency_s": 0.0246,
                "tokens_per_second": 49.35,
                "memory_before_gb": 0.918,
                "memory_after_gb": 0.918,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 0.749,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 20.25,
                "p90_inter_token_latency_ms": 23.03
              }
            ],
            "actual_file_size_gb": 0.623,
            "model_load_time_s": 1.57,
            "backend": {
              "backend": "cpu",
              "n_gpu_layers": 0,
              "details": "AMD GPU d√©tect√© (AMD Radeon(TM) Graphics) ‚Äî DirectML n'est pas support√© par llama-cpp-python. Utilisez le mode llama-server avec un binaire Vulkan pour l'acc√©l√©ration GPU, sinon fallback CPU.",
              "inference_mode": "server",
              "server_url": "http://127.0.0.1:8080"
            },
            "memory_after_load_gb": 0.902,
            "server_memory_after_load_gb": 0.734,
            "resource_usage": {
              "n_samples": 27,
              "duration_s": 14.76880407333374,
              "cpu": {
                "avg_percent": 56.7,
                "max_percent": 68.6,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 88.8,
                "max_percent": 89.2,
                "peak_used_gb": 13.74
              }
            },
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 50.47,
              "std_tokens_per_second": 1.02,
              "min_tokens_per_second": 49.35,
              "max_tokens_per_second": 51.81,
              "avg_first_token_latency_s": 0.0253,
              "avg_inter_token_latency_ms": 19.8,
              "avg_total_time_s": 5.0747,
              "peak_memory_gb": 0.918,
              "stability": "stable"
            }
          },
          "Q5_K_M": {
            "quantization": "Q5_K_M",
            "bits": 5,
            "file_size_gb": 0.73,
            "status": "completed",
            "inference_mode": "server",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 5.6469,
                "first_token_latency_s": 0.0278,
                "tokens_per_second": 45.33,
                "memory_before_gb": 1.009,
                "memory_after_gb": 1.01,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 0.841,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 22.03,
                "p90_inter_token_latency_ms": 23.35
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.6812,
                "first_token_latency_s": 0.0306,
                "tokens_per_second": 45.06,
                "memory_before_gb": 1.011,
                "memory_after_gb": 1.018,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.849,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 22.16,
                "p90_inter_token_latency_ms": 23.72
              },
              {
                "tokens_generated": 256,
                "total_time_s": 6.0331,
                "first_token_latency_s": 0.0287,
                "tokens_per_second": 42.43,
                "memory_before_gb": 1.018,
                "memory_after_gb": 1.018,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 0.849,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 23.55,
                "p90_inter_token_latency_ms": 25.23
              }
            ],
            "actual_file_size_gb": 0.729,
            "model_load_time_s": 1.56,
            "backend": {
              "backend": "cpu",
              "n_gpu_layers": 0,
              "details": "AMD GPU d√©tect√© (AMD Radeon(TM) Graphics) ‚Äî DirectML n'est pas support√© par llama-cpp-python. Utilisez le mode llama-server avec un binaire Vulkan pour l'acc√©l√©ration GPU, sinon fallback CPU.",
              "inference_mode": "server",
              "server_url": "http://127.0.0.1:8080"
            },
            "memory_after_load_gb": 1.001,
            "server_memory_after_load_gb": 0.832,
            "resource_usage": {
              "n_samples": 31,
              "duration_s": 17.02686834335327,
              "cpu": {
                "avg_percent": 57.5,
                "max_percent": 69.8,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 89.9,
                "max_percent": 90.5,
                "peak_used_gb": 13.94
              }
            },
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 44.27,
              "std_tokens_per_second": 1.31,
              "min_tokens_per_second": 42.43,
              "max_tokens_per_second": 45.33,
              "avg_first_token_latency_s": 0.029,
              "avg_inter_token_latency_ms": 22.58,
              "avg_total_time_s": 5.7871,
              "peak_memory_gb": 1.018,
              "stability": "stable"
            }
          },
          "Q6_K": {
            "quantization": "Q6_K",
            "bits": 6,
            "file_size_gb": 0.84,
            "status": "completed",
            "inference_mode": "server",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 6.7736,
                "first_token_latency_s": 0.0338,
                "tokens_per_second": 37.79,
                "memory_before_gb": 1.091,
                "memory_after_gb": 1.092,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 0.945,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 26.43,
                "p90_inter_token_latency_ms": 28.37
              },
              {
                "tokens_generated": 256,
                "total_time_s": 6.4656,
                "first_token_latency_s": 0.0331,
                "tokens_per_second": 39.59,
                "memory_before_gb": 1.092,
                "memory_after_gb": 1.099,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.952,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 25.22,
                "p90_inter_token_latency_ms": 26.61
              },
              {
                "tokens_generated": 256,
                "total_time_s": 6.6725,
                "first_token_latency_s": 0.0295,
                "tokens_per_second": 38.37,
                "memory_before_gb": 1.099,
                "memory_after_gb": 1.099,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 0.952,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 26.05,
                "p90_inter_token_latency_ms": 27.78
              }
            ],
            "actual_file_size_gb": 0.842,
            "model_load_time_s": 1.54,
            "backend": {
              "backend": "cpu",
              "n_gpu_layers": 0,
              "details": "AMD GPU d√©tect√© (AMD Radeon(TM) Graphics) ‚Äî DirectML n'est pas support√© par llama-cpp-python. Utilisez le mode llama-server avec un binaire Vulkan pour l'acc√©l√©ration GPU, sinon fallback CPU.",
              "inference_mode": "server",
              "server_url": "http://127.0.0.1:8080"
            },
            "memory_after_load_gb": 1.083,
            "server_memory_after_load_gb": 0.936,
            "resource_usage": {
              "n_samples": 36,
              "duration_s": 19.81754755973816,
              "cpu": {
                "avg_percent": 59.8,
                "max_percent": 74.2,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 88.1,
                "max_percent": 88.6,
                "peak_used_gb": 13.65
              }
            },
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 38.58,
              "std_tokens_per_second": 0.75,
              "min_tokens_per_second": 37.79,
              "max_tokens_per_second": 39.59,
              "avg_first_token_latency_s": 0.0321,
              "avg_inter_token_latency_ms": 25.9,
              "avg_total_time_s": 6.6372,
              "peak_memory_gb": 1.099,
              "stability": "stable"
            }
          },
          "Q8_0": {
            "quantization": "Q8_0",
            "bits": 8,
            "file_size_gb": 1.09,
            "status": "completed",
            "inference_mode": "server",
            "runs": [
              {
                "tokens_generated": 14,
                "total_time_s": 0.4697,
                "first_token_latency_s": 0.0389,
                "tokens_per_second": 29.81,
                "memory_before_gb": 1.312,
                "memory_after_gb": 1.312,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 1.178,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 30.81,
                "p90_inter_token_latency_ms": 31.74
              },
              {
                "tokens_generated": 14,
                "total_time_s": 0.4579,
                "first_token_latency_s": 0.0365,
                "tokens_per_second": 30.57,
                "memory_before_gb": 1.312,
                "memory_after_gb": 1.312,
                "memory_delta_gb": -0.0,
                "server_memory_gb": 1.178,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 30.1,
                "p90_inter_token_latency_ms": 30.67
              },
              {
                "tokens_generated": 14,
                "total_time_s": 0.4779,
                "first_token_latency_s": 0.0353,
                "tokens_per_second": 29.3,
                "memory_before_gb": 1.312,
                "memory_after_gb": 1.312,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 1.178,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 31.45,
                "p90_inter_token_latency_ms": 33.38
              }
            ],
            "actual_file_size_gb": 1.09,
            "model_load_time_s": 1.56,
            "backend": {
              "backend": "cpu",
              "n_gpu_layers": 0,
              "details": "AMD GPU d√©tect√© (AMD Radeon(TM) Graphics) ‚Äî DirectML n'est pas support√© par llama-cpp-python. Utilisez le mode llama-server avec un binaire Vulkan pour l'acc√©l√©ration GPU, sinon fallback CPU.",
              "inference_mode": "server",
              "server_url": "http://127.0.0.1:8080"
            },
            "memory_after_load_gb": 1.317,
            "server_memory_after_load_gb": 1.171,
            "resource_usage": {
              "n_samples": 3,
              "duration_s": 1.1601691246032715,
              "cpu": {
                "avg_percent": 38.8,
                "max_percent": 60.1,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 88.6,
                "max_percent": 88.7,
                "peak_used_gb": 13.66
              }
            },
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 29.89,
              "std_tokens_per_second": 0.52,
              "min_tokens_per_second": 29.3,
              "max_tokens_per_second": 30.57,
              "avg_first_token_latency_s": 0.0369,
              "avg_inter_token_latency_ms": 30.79,
              "avg_total_time_s": 0.4685,
              "peak_memory_gb": 1.312,
              "stability": "stable"
            }
          }
        },
        "comparison_table": [
          {
            "quantization": "Q2_K",
            "bits": 2,
            "file_size_gb": 0.45,
            "tokens_per_second": 65.03,
            "first_token_latency_s": 0.0264,
            "inter_token_latency_ms": 15.33,
            "peak_memory_gb": 0.798,
            "model_load_time_s": 1.54,
            "stability": "stable"
          },
          {
            "quantization": "Q3_K_M",
            "bits": 3,
            "file_size_gb": 0.513,
            "tokens_per_second": 56.71,
            "first_token_latency_s": 0.0366,
            "inter_token_latency_ms": 17.57,
            "peak_memory_gb": 0.801,
            "model_load_time_s": 1.56,
            "stability": "stable"
          },
          {
            "quantization": "Q4_K_M",
            "bits": 4,
            "file_size_gb": 0.623,
            "tokens_per_second": 50.47,
            "first_token_latency_s": 0.0253,
            "inter_token_latency_ms": 19.8,
            "peak_memory_gb": 0.918,
            "model_load_time_s": 1.57,
            "stability": "stable"
          },
          {
            "quantization": "Q5_K_M",
            "bits": 5,
            "file_size_gb": 0.729,
            "tokens_per_second": 44.27,
            "first_token_latency_s": 0.029,
            "inter_token_latency_ms": 22.58,
            "peak_memory_gb": 1.018,
            "model_load_time_s": 1.56,
            "stability": "stable"
          },
          {
            "quantization": "Q6_K",
            "bits": 6,
            "file_size_gb": 0.842,
            "tokens_per_second": 38.58,
            "first_token_latency_s": 0.0321,
            "inter_token_latency_ms": 25.9,
            "peak_memory_gb": 1.099,
            "model_load_time_s": 1.54,
            "stability": "stable"
          },
          {
            "quantization": "Q8_0",
            "bits": 8,
            "file_size_gb": 1.09,
            "tokens_per_second": 29.89,
            "first_token_latency_s": 0.0369,
            "inter_token_latency_ms": 30.79,
            "peak_memory_gb": 1.312,
            "model_load_time_s": 1.56,
            "stability": "stable"
          }
        ]
      }
    },
    "temperature_comparison": {
      "tinyllama-1.1b": {
        "axis": "temperature",
        "model_name": "TinyLlama 1.1B",
        "model_key": "tinyllama-1.1b",
        "params": "1.1B",
        "variants_tested": [
          "low",
          "medium",
          "high"
        ],
        "inference_mode": "server",
        "status": "completed",
        "results": {
          "low": {
            "temperature": 0.25,
            "label": "Basse (0.25)",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 4.9984,
                "first_token_latency_s": 0.0254,
                "tokens_per_second": 51.22,
                "memory_before_gb": 0.863,
                "memory_after_gb": 0.864,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 0.743,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.5,
                "p90_inter_token_latency_ms": 21.0
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.0823,
                "first_token_latency_s": 0.0477,
                "tokens_per_second": 50.37,
                "memory_before_gb": 0.865,
                "memory_after_gb": 0.872,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.75,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.74,
                "p90_inter_token_latency_ms": 21.35
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.0618,
                "first_token_latency_s": 0.051,
                "tokens_per_second": 50.57,
                "memory_before_gb": 0.872,
                "memory_after_gb": 0.873,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 0.75,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.65,
                "p90_inter_token_latency_ms": 20.87
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 50.72,
              "std_tokens_per_second": 0.36,
              "avg_first_token_latency_s": 0.0414,
              "avg_inter_token_latency_ms": 19.63,
              "avg_total_time_s": 5.0475,
              "peak_memory_gb": 0.873,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 27,
              "duration_s": 14.678978443145752,
              "cpu": {
                "avg_percent": 56.9,
                "max_percent": 68.9,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 76.7,
                "max_percent": 77.6,
                "peak_used_gb": 11.95
              }
            }
          },
          "medium": {
            "temperature": 0.5,
            "label": "Moyenne (0.50)",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 4.9865,
                "first_token_latency_s": 0.0252,
                "tokens_per_second": 51.34,
                "memory_before_gb": 0.873,
                "memory_after_gb": 0.873,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 0.75,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.46,
                "p90_inter_token_latency_ms": 20.73
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.0006,
                "first_token_latency_s": 0.0264,
                "tokens_per_second": 51.19,
                "memory_before_gb": 0.873,
                "memory_after_gb": 0.88,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.757,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.51,
                "p90_inter_token_latency_ms": 20.77
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.0059,
                "first_token_latency_s": 0.0399,
                "tokens_per_second": 51.14,
                "memory_before_gb": 0.88,
                "memory_after_gb": 0.88,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 0.757,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.47,
                "p90_inter_token_latency_ms": 20.83
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 51.22,
              "std_tokens_per_second": 0.08,
              "avg_first_token_latency_s": 0.0305,
              "avg_inter_token_latency_ms": 19.48,
              "avg_total_time_s": 4.9977,
              "peak_memory_gb": 0.88,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 27,
              "duration_s": 14.706591129302979,
              "cpu": {
                "avg_percent": 54.3,
                "max_percent": 61.6,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 76.8,
                "max_percent": 76.9,
                "peak_used_gb": 11.84
              }
            }
          },
          "high": {
            "temperature": 0.75,
            "label": "Haute (0.75)",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 5.0008,
                "first_token_latency_s": 0.0372,
                "tokens_per_second": 51.19,
                "memory_before_gb": 0.88,
                "memory_after_gb": 0.88,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 0.757,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.46,
                "p90_inter_token_latency_ms": 20.82
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.0145,
                "first_token_latency_s": 0.0551,
                "tokens_per_second": 51.05,
                "memory_before_gb": 0.88,
                "memory_after_gb": 0.887,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.764,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.45,
                "p90_inter_token_latency_ms": 20.74
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.011,
                "first_token_latency_s": 0.0508,
                "tokens_per_second": 51.09,
                "memory_before_gb": 0.887,
                "memory_after_gb": 0.887,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 0.764,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.45,
                "p90_inter_token_latency_ms": 20.79
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 51.11,
              "std_tokens_per_second": 0.06,
              "avg_first_token_latency_s": 0.0477,
              "avg_inter_token_latency_ms": 19.45,
              "avg_total_time_s": 5.0088,
              "peak_memory_gb": 0.887,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 27,
              "duration_s": 14.674616813659668,
              "cpu": {
                "avg_percent": 54.4,
                "max_percent": 59.6,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 76.8,
                "max_percent": 76.9,
                "peak_used_gb": 11.84
              }
            }
          }
        },
        "backend": {
          "backend": "cpu",
          "n_gpu_layers": 0,
          "details": "AMD GPU d√©tect√© (AMD Radeon(TM) Graphics) ‚Äî DirectML n'est pas support√© par llama-cpp-python. Utilisez le mode llama-server avec un binaire Vulkan pour l'acc√©l√©ration GPU, sinon fallback CPU.",
          "inference_mode": "server"
        },
        "comparison_table": [
          {
            "temperature_key": "low",
            "temperature": 0.25,
            "label": "Basse (0.25)",
            "tokens_per_second": 50.72,
            "first_token_latency_s": 0.0414,
            "inter_token_latency_ms": 19.63,
            "peak_memory_gb": 0.873,
            "stability": "stable"
          },
          {
            "temperature_key": "medium",
            "temperature": 0.5,
            "label": "Moyenne (0.50)",
            "tokens_per_second": 51.22,
            "first_token_latency_s": 0.0305,
            "inter_token_latency_ms": 19.48,
            "peak_memory_gb": 0.88,
            "stability": "stable"
          },
          {
            "temperature_key": "high",
            "temperature": 0.75,
            "label": "Haute (0.75)",
            "tokens_per_second": 51.11,
            "first_token_latency_s": 0.0477,
            "inter_token_latency_ms": 19.45,
            "peak_memory_gb": 0.887,
            "stability": "stable"
          }
        ]
      }
    },
    "language_comparison": {
      "tinyllama-1.1b": {
        "axis": "language",
        "model_name": "TinyLlama 1.1B",
        "model_key": "tinyllama-1.1b",
        "params": "1.1B",
        "variants_tested": [
          "en",
          "fr",
          "zh",
          "es",
          "de",
          "ar"
        ],
        "inference_mode": "server",
        "status": "completed",
        "results": {
          "en": {
            "language": "en",
            "label": "Anglais",
            "flag": "üá¨üáß",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 5.5635,
                "first_token_latency_s": 0.0296,
                "tokens_per_second": 46.01,
                "memory_before_gb": 0.865,
                "memory_after_gb": 0.865,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 0.743,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 21.7,
                "p90_inter_token_latency_ms": 24.72
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.1215,
                "first_token_latency_s": 0.0429,
                "tokens_per_second": 49.99,
                "memory_before_gb": 0.865,
                "memory_after_gb": 0.873,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.75,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.92,
                "p90_inter_token_latency_ms": 21.5
              },
              {
                "tokens_generated": 256,
                "total_time_s": 4.9911,
                "first_token_latency_s": 0.0235,
                "tokens_per_second": 51.29,
                "memory_before_gb": 0.873,
                "memory_after_gb": 0.873,
                "memory_delta_gb": -0.0,
                "server_memory_gb": 0.75,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.48,
                "p90_inter_token_latency_ms": 20.54
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 49.1,
              "std_tokens_per_second": 2.25,
              "avg_first_token_latency_s": 0.032,
              "avg_inter_token_latency_ms": 20.37,
              "avg_total_time_s": 5.2254,
              "peak_memory_gb": 0.873,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 28,
              "duration_s": 15.420286893844604,
              "cpu": {
                "avg_percent": 59.8,
                "max_percent": 86.1,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 77.1,
                "max_percent": 77.9,
                "peak_used_gb": 12.0
              }
            }
          },
          "fr": {
            "language": "fr",
            "label": "Fran√ßais",
            "flag": "üá´üá∑",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 5.4064,
                "first_token_latency_s": 0.3861,
                "tokens_per_second": 47.35,
                "memory_before_gb": 0.873,
                "memory_after_gb": 0.874,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 0.751,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.69,
                "p90_inter_token_latency_ms": 21.41
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.3233,
                "first_token_latency_s": 0.048,
                "tokens_per_second": 48.09,
                "memory_before_gb": 0.874,
                "memory_after_gb": 0.881,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.758,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 20.69,
                "p90_inter_token_latency_ms": 23.35
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.0807,
                "first_token_latency_s": 0.0289,
                "tokens_per_second": 50.39,
                "memory_before_gb": 0.881,
                "memory_after_gb": 0.888,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.765,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.81,
                "p90_inter_token_latency_ms": 21.14
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 48.61,
              "std_tokens_per_second": 1.29,
              "avg_first_token_latency_s": 0.1543,
              "avg_inter_token_latency_ms": 20.06,
              "avg_total_time_s": 5.2701,
              "peak_memory_gb": 0.888,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 28,
              "duration_s": 15.304085731506348,
              "cpu": {
                "avg_percent": 56.1,
                "max_percent": 73.1,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 77.4,
                "max_percent": 78.4,
                "peak_used_gb": 12.08
              }
            }
          },
          "zh": {
            "language": "zh",
            "label": "Mandarin",
            "flag": "üá®üá≥",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 6.1286,
                "first_token_latency_s": 0.3684,
                "tokens_per_second": 41.77,
                "memory_before_gb": 0.888,
                "memory_after_gb": 0.889,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 0.766,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 22.59,
                "p90_inter_token_latency_ms": 26.43
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.278,
                "first_token_latency_s": 0.0491,
                "tokens_per_second": 48.5,
                "memory_before_gb": 0.889,
                "memory_after_gb": 0.897,
                "memory_delta_gb": 0.008,
                "server_memory_gb": 0.774,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 20.5,
                "p90_inter_token_latency_ms": 21.83
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.3258,
                "first_token_latency_s": 0.054,
                "tokens_per_second": 48.07,
                "memory_before_gb": 0.897,
                "memory_after_gb": 0.904,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.781,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 20.67,
                "p90_inter_token_latency_ms": 21.73
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 46.11,
              "std_tokens_per_second": 3.08,
              "avg_first_token_latency_s": 0.1572,
              "avg_inter_token_latency_ms": 21.25,
              "avg_total_time_s": 5.5775,
              "peak_memory_gb": 0.904,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 30,
              "duration_s": 16.51481056213379,
              "cpu": {
                "avg_percent": 60.9,
                "max_percent": 83.4,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 77.7,
                "max_percent": 78.5,
                "peak_used_gb": 12.09
              }
            }
          },
          "es": {
            "language": "es",
            "label": "Espagnol",
            "flag": "üá™üá∏",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 5.3485,
                "first_token_latency_s": 0.3536,
                "tokens_per_second": 47.86,
                "memory_before_gb": 0.904,
                "memory_after_gb": 0.905,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 0.782,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.59,
                "p90_inter_token_latency_ms": 20.93
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.0598,
                "first_token_latency_s": 0.029,
                "tokens_per_second": 50.59,
                "memory_before_gb": 0.905,
                "memory_after_gb": 0.912,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.789,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.73,
                "p90_inter_token_latency_ms": 21.04
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.1713,
                "first_token_latency_s": 0.0521,
                "tokens_per_second": 49.5,
                "memory_before_gb": 0.912,
                "memory_after_gb": 0.919,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.796,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 20.07,
                "p90_inter_token_latency_ms": 21.22
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 49.32,
              "std_tokens_per_second": 1.12,
              "avg_first_token_latency_s": 0.1449,
              "avg_inter_token_latency_ms": 19.8,
              "avg_total_time_s": 5.1932,
              "peak_memory_gb": 0.919,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 28,
              "duration_s": 15.198565483093262,
              "cpu": {
                "avg_percent": 54.1,
                "max_percent": 65.0,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 77.9,
                "max_percent": 78.5,
                "peak_used_gb": 12.09
              }
            }
          },
          "de": {
            "language": "de",
            "label": "Allemand",
            "flag": "üá©üá™",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 5.4009,
                "first_token_latency_s": 0.351,
                "tokens_per_second": 47.4,
                "memory_before_gb": 0.919,
                "memory_after_gb": 0.92,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 0.797,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.8,
                "p90_inter_token_latency_ms": 21.62
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.072,
                "first_token_latency_s": 0.0289,
                "tokens_per_second": 50.47,
                "memory_before_gb": 0.92,
                "memory_after_gb": 0.927,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.804,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.78,
                "p90_inter_token_latency_ms": 21.0
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.0935,
                "first_token_latency_s": 0.0388,
                "tokens_per_second": 50.26,
                "memory_before_gb": 0.927,
                "memory_after_gb": 0.934,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.811,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.82,
                "p90_inter_token_latency_ms": 21.28
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 49.38,
              "std_tokens_per_second": 1.4,
              "avg_first_token_latency_s": 0.1396,
              "avg_inter_token_latency_ms": 19.8,
              "avg_total_time_s": 5.1888,
              "peak_memory_gb": 0.934,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 28,
              "duration_s": 15.18486738204956,
              "cpu": {
                "avg_percent": 55.0,
                "max_percent": 69.5,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 77.8,
                "max_percent": 78.6,
                "peak_used_gb": 12.1
              }
            }
          },
          "ar": {
            "language": "ar",
            "label": "Arabe",
            "flag": "üá∏üá¶",
            "runs": [
              {
                "tokens_generated": 241,
                "total_time_s": 5.488,
                "first_token_latency_s": 0.375,
                "tokens_per_second": 43.91,
                "memory_before_gb": 0.934,
                "memory_after_gb": 0.935,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 0.811,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 21.3,
                "p90_inter_token_latency_ms": 21.62
              },
              {
                "tokens_generated": 246,
                "total_time_s": 5.1937,
                "first_token_latency_s": 0.042,
                "tokens_per_second": 47.37,
                "memory_before_gb": 0.935,
                "memory_after_gb": 0.943,
                "memory_delta_gb": 0.008,
                "server_memory_gb": 0.82,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 21.03,
                "p90_inter_token_latency_ms": 21.73
              },
              {
                "tokens_generated": 246,
                "total_time_s": 5.1879,
                "first_token_latency_s": 0.0264,
                "tokens_per_second": 47.42,
                "memory_before_gb": 0.943,
                "memory_after_gb": 0.951,
                "memory_delta_gb": 0.008,
                "server_memory_gb": 0.828,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 21.07,
                "p90_inter_token_latency_ms": 21.79
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 46.23,
              "std_tokens_per_second": 1.64,
              "avg_first_token_latency_s": 0.1478,
              "avg_inter_token_latency_ms": 21.13,
              "avg_total_time_s": 5.2899,
              "peak_memory_gb": 0.951,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 29,
              "duration_s": 15.764845609664917,
              "cpu": {
                "avg_percent": 54.6,
                "max_percent": 64.1,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 77.9,
                "max_percent": 78.7,
                "peak_used_gb": 12.12
              }
            }
          }
        },
        "backend": {
          "backend": "cpu",
          "n_gpu_layers": 0,
          "details": "AMD GPU d√©tect√© (AMD Radeon(TM) Graphics) ‚Äî DirectML n'est pas support√© par llama-cpp-python. Utilisez le mode llama-server avec un binaire Vulkan pour l'acc√©l√©ration GPU, sinon fallback CPU.",
          "inference_mode": "server"
        },
        "comparison_table": [
          {
            "language_key": "en",
            "label": "Anglais",
            "flag": "üá¨üáß",
            "tokens_per_second": 49.1,
            "first_token_latency_s": 0.032,
            "inter_token_latency_ms": 20.37,
            "peak_memory_gb": 0.873,
            "stability": "stable"
          },
          {
            "language_key": "fr",
            "label": "Fran√ßais",
            "flag": "üá´üá∑",
            "tokens_per_second": 48.61,
            "first_token_latency_s": 0.1543,
            "inter_token_latency_ms": 20.06,
            "peak_memory_gb": 0.888,
            "stability": "stable"
          },
          {
            "language_key": "zh",
            "label": "Mandarin",
            "flag": "üá®üá≥",
            "tokens_per_second": 46.11,
            "first_token_latency_s": 0.1572,
            "inter_token_latency_ms": 21.25,
            "peak_memory_gb": 0.904,
            "stability": "stable"
          },
          {
            "language_key": "es",
            "label": "Espagnol",
            "flag": "üá™üá∏",
            "tokens_per_second": 49.32,
            "first_token_latency_s": 0.1449,
            "inter_token_latency_ms": 19.8,
            "peak_memory_gb": 0.919,
            "stability": "stable"
          },
          {
            "language_key": "de",
            "label": "Allemand",
            "flag": "üá©üá™",
            "tokens_per_second": 49.38,
            "first_token_latency_s": 0.1396,
            "inter_token_latency_ms": 19.8,
            "peak_memory_gb": 0.934,
            "stability": "stable"
          },
          {
            "language_key": "ar",
            "label": "Arabe",
            "flag": "üá∏üá¶",
            "tokens_per_second": 46.23,
            "first_token_latency_s": 0.1478,
            "inter_token_latency_ms": 21.13,
            "peak_memory_gb": 0.951,
            "stability": "stable"
          }
        ]
      }
    },
    "prompt_type_comparison": {
      "tinyllama-1.1b": {
        "axis": "prompt_type",
        "model_name": "TinyLlama 1.1B",
        "model_key": "tinyllama-1.1b",
        "params": "1.1B",
        "variants_tested": [
          "general",
          "code",
          "reasoning",
          "creative",
          "math"
        ],
        "inference_mode": "server",
        "status": "completed",
        "results": {
          "general": {
            "prompt_type": "general",
            "label": "G√©n√©ral / Connaissances",
            "icon": "üìö",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 5.2953,
                "first_token_latency_s": 0.0241,
                "tokens_per_second": 48.34,
                "memory_before_gb": 0.865,
                "memory_after_gb": 0.866,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 0.743,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 20.67,
                "p90_inter_token_latency_ms": 22.92
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.1226,
                "first_token_latency_s": 0.0264,
                "tokens_per_second": 49.97,
                "memory_before_gb": 0.866,
                "memory_after_gb": 0.873,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.75,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.98,
                "p90_inter_token_latency_ms": 22.07
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.1638,
                "first_token_latency_s": 0.0347,
                "tokens_per_second": 49.58,
                "memory_before_gb": 0.873,
                "memory_after_gb": 0.873,
                "memory_delta_gb": -0.0,
                "server_memory_gb": 0.75,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 20.11,
                "p90_inter_token_latency_ms": 21.68
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 49.3,
              "std_tokens_per_second": 0.69,
              "avg_first_token_latency_s": 0.0284,
              "avg_inter_token_latency_ms": 20.25,
              "avg_total_time_s": 5.1939,
              "peak_memory_gb": 0.873,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 28,
              "duration_s": 15.382338762283325,
              "cpu": {
                "avg_percent": 61.4,
                "max_percent": 74.8,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 77.8,
                "max_percent": 78.4,
                "peak_used_gb": 12.08
              }
            }
          },
          "code": {
            "prompt_type": "code",
            "label": "Code / Programmation",
            "icon": "üíª",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 5.5835,
                "first_token_latency_s": 0.3781,
                "tokens_per_second": 45.85,
                "memory_before_gb": 0.873,
                "memory_after_gb": 0.874,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 0.751,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 20.41,
                "p90_inter_token_latency_ms": 22.48
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.2258,
                "first_token_latency_s": 0.0528,
                "tokens_per_second": 48.99,
                "memory_before_gb": 0.874,
                "memory_after_gb": 0.881,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.758,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 20.29,
                "p90_inter_token_latency_ms": 22.99
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.2969,
                "first_token_latency_s": 0.0519,
                "tokens_per_second": 48.33,
                "memory_before_gb": 0.881,
                "memory_after_gb": 0.889,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.766,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 20.57,
                "p90_inter_token_latency_ms": 21.76
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 47.72,
              "std_tokens_per_second": 1.35,
              "avg_first_token_latency_s": 0.1609,
              "avg_inter_token_latency_ms": 20.42,
              "avg_total_time_s": 5.3687,
              "peak_memory_gb": 0.889,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 29,
              "duration_s": 15.917323589324951,
              "cpu": {
                "avg_percent": 57.8,
                "max_percent": 75.0,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 77.7,
                "max_percent": 78.6,
                "peak_used_gb": 12.11
              }
            }
          },
          "reasoning": {
            "prompt_type": "reasoning",
            "label": "Raisonnement / Logique",
            "icon": "üß†",
            "runs": [
              {
                "tokens_generated": 179,
                "total_time_s": 3.9367,
                "first_token_latency_s": 0.372,
                "tokens_per_second": 45.47,
                "memory_before_gb": 0.889,
                "memory_after_gb": 0.889,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 0.766,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.9,
                "p90_inter_token_latency_ms": 21.83
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.2771,
                "first_token_latency_s": 0.0326,
                "tokens_per_second": 48.51,
                "memory_before_gb": 0.889,
                "memory_after_gb": 0.895,
                "memory_delta_gb": 0.006,
                "server_memory_gb": 0.772,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 20.57,
                "p90_inter_token_latency_ms": 22.2
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.1407,
                "first_token_latency_s": 0.0474,
                "tokens_per_second": 49.8,
                "memory_before_gb": 0.895,
                "memory_after_gb": 0.903,
                "memory_delta_gb": 0.008,
                "server_memory_gb": 0.78,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.97,
                "p90_inter_token_latency_ms": 21.29
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 47.93,
              "std_tokens_per_second": 1.82,
              "avg_first_token_latency_s": 0.1507,
              "avg_inter_token_latency_ms": 20.15,
              "avg_total_time_s": 4.7848,
              "peak_memory_gb": 0.903,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 26,
              "duration_s": 14.18332552909851,
              "cpu": {
                "avg_percent": 56.6,
                "max_percent": 68.4,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 77.9,
                "max_percent": 78.6,
                "peak_used_gb": 12.11
              }
            }
          },
          "creative": {
            "prompt_type": "creative",
            "label": "Cr√©atif / R√©daction",
            "icon": "‚úçÔ∏è",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 5.4079,
                "first_token_latency_s": 0.3619,
                "tokens_per_second": 47.34,
                "memory_before_gb": 0.903,
                "memory_after_gb": 0.904,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 0.78,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.79,
                "p90_inter_token_latency_ms": 21.52
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.4006,
                "first_token_latency_s": 0.0538,
                "tokens_per_second": 47.4,
                "memory_before_gb": 0.904,
                "memory_after_gb": 0.911,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.788,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 20.97,
                "p90_inter_token_latency_ms": 22.96
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.9923,
                "first_token_latency_s": 0.0476,
                "tokens_per_second": 42.72,
                "memory_before_gb": 0.911,
                "memory_after_gb": 0.918,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.795,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 23.31,
                "p90_inter_token_latency_ms": 26.34
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 45.82,
              "std_tokens_per_second": 2.19,
              "avg_first_token_latency_s": 0.1544,
              "avg_inter_token_latency_ms": 21.36,
              "avg_total_time_s": 5.6003,
              "peak_memory_gb": 0.918,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 30,
              "duration_s": 16.633085012435913,
              "cpu": {
                "avg_percent": 63.1,
                "max_percent": 85.6,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 78.1,
                "max_percent": 79.2,
                "peak_used_gb": 12.2
              }
            }
          },
          "math": {
            "prompt_type": "math",
            "label": "Math√©matiques",
            "icon": "üî¢",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 5.5023,
                "first_token_latency_s": 0.3575,
                "tokens_per_second": 46.53,
                "memory_before_gb": 0.918,
                "memory_after_gb": 0.918,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 0.795,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 20.17,
                "p90_inter_token_latency_ms": 21.55
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.4848,
                "first_token_latency_s": 0.0394,
                "tokens_per_second": 46.67,
                "memory_before_gb": 0.918,
                "memory_after_gb": 0.926,
                "memory_delta_gb": 0.008,
                "server_memory_gb": 0.803,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 21.35,
                "p90_inter_token_latency_ms": 23.13
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.202,
                "first_token_latency_s": 0.032,
                "tokens_per_second": 49.21,
                "memory_before_gb": 0.926,
                "memory_after_gb": 0.934,
                "memory_delta_gb": 0.008,
                "server_memory_gb": 0.81,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 20.27,
                "p90_inter_token_latency_ms": 21.44
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 47.47,
              "std_tokens_per_second": 1.23,
              "avg_first_token_latency_s": 0.143,
              "avg_inter_token_latency_ms": 20.6,
              "avg_total_time_s": 5.3964,
              "peak_memory_gb": 0.934,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 29,
              "duration_s": 15.912339687347412,
              "cpu": {
                "avg_percent": 57.5,
                "max_percent": 70.7,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 78.7,
                "max_percent": 79.4,
                "peak_used_gb": 12.24
              }
            }
          }
        },
        "backend": {
          "backend": "cpu",
          "n_gpu_layers": 0,
          "details": "AMD GPU d√©tect√© (AMD Radeon(TM) Graphics) ‚Äî DirectML n'est pas support√© par llama-cpp-python. Utilisez le mode llama-server avec un binaire Vulkan pour l'acc√©l√©ration GPU, sinon fallback CPU.",
          "inference_mode": "server"
        },
        "comparison_table": [
          {
            "prompt_type_key": "general",
            "label": "G√©n√©ral / Connaissances",
            "icon": "üìö",
            "tokens_per_second": 49.3,
            "first_token_latency_s": 0.0284,
            "inter_token_latency_ms": 20.25,
            "peak_memory_gb": 0.873,
            "stability": "stable"
          },
          {
            "prompt_type_key": "code",
            "label": "Code / Programmation",
            "icon": "üíª",
            "tokens_per_second": 47.72,
            "first_token_latency_s": 0.1609,
            "inter_token_latency_ms": 20.42,
            "peak_memory_gb": 0.889,
            "stability": "stable"
          },
          {
            "prompt_type_key": "reasoning",
            "label": "Raisonnement / Logique",
            "icon": "üß†",
            "tokens_per_second": 47.93,
            "first_token_latency_s": 0.1507,
            "inter_token_latency_ms": 20.15,
            "peak_memory_gb": 0.903,
            "stability": "stable"
          },
          {
            "prompt_type_key": "creative",
            "label": "Cr√©atif / R√©daction",
            "icon": "‚úçÔ∏è",
            "tokens_per_second": 45.82,
            "first_token_latency_s": 0.1544,
            "inter_token_latency_ms": 21.36,
            "peak_memory_gb": 0.918,
            "stability": "stable"
          },
          {
            "prompt_type_key": "math",
            "label": "Math√©matiques",
            "icon": "üî¢",
            "tokens_per_second": 47.47,
            "first_token_latency_s": 0.143,
            "inter_token_latency_ms": 20.6,
            "peak_memory_gb": 0.934,
            "stability": "stable"
          }
        ]
      }
    }
  }
}