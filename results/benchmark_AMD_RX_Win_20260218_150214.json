{
  "id": "20260218_150214",
  "timestamp": "2026-02-18T15:02:14.066942",
  "version": "1.0.0",
  "hardware": {
    "os": {
      "system": "Windows",
      "release": "10",
      "version": "10.0.26200",
      "machine": "AMD64",
      "architecture": "64bit",
      "platform": "Windows-10-10.0.26200-SP0",
      "python_version": "3.9.13"
    },
    "cpu": {
      "physical_cores": 8,
      "logical_cores": 16,
      "architecture": "AMD64",
      "model": "AMD Ryzen 7 4800H with Radeon Graphics",
      "max_clock_speed_mhz": "2900",
      "architecture_type": "x86_64",
      "is_apple_silicon": false,
      "frequency_mhz": {
        "current": 2900.0,
        "min": null,
        "max": 2900.0
      }
    },
    "gpu": {
      "gpus": [
        {
          "type": "AMD",
          "name": "AMD Radeon(TM) Graphics",
          "backend": "directml",
          "detected_via": "wmi",
          "vram_total_mb": 512,
          "driver_version": "31.0.21921.1000",
          "gpu_index": 0
        },
        {
          "type": "AMD",
          "name": "Radeon RX 5500M",
          "backend": "directml",
          "detected_via": "wmi",
          "vram_total_mb": 4080,
          "driver_version": "32.0.12019.1028",
          "gpu_index": 1
        }
      ],
      "backends": [
        "directml",
        "cpu"
      ],
      "primary_backend": "directml",
      "python_backends": {
        "pytorch": true,
        "pytorch_version": "2.4.1+cpu",
        "pytorch_cuda": false,
        "pytorch_mps": false,
        "pytorch_xpu": false,
        "llama_cpp": false,
        "llama_server": false,
        "pytorch_rocm": false,
        "ipex": false,
        "directml": true,
        "directml_available": true,
        "directml_device_count": 2,
        "directml_device_name": "Radeon RX 5500M\u0000"
      }
    },
    "ram": {
      "total_gb": 15.4,
      "available_gb": 4.55,
      "used_gb": 10.85,
      "percent_used": 70.5,
      "swap_total_gb": 10.0,
      "swap_used_gb": 0.44,
      "unified_memory": false
    },
    "selected_gpu": {
      "gpu_index": 1,
      "name": "Radeon RX 5500M",
      "backend": "directml"
    }
  },
  "classic_benchmarks": {
    "type": "classic_benchmarks",
    "total_time_s": 13.45,
    "benchmarks": {
      "cpu_single_thread": {
        "test": "CPU Single-Thread",
        "method": "Matrix multiplication (NumPy, 1 thread, subprocess isol√©)",
        "results": {
          "512x512": {
            "times_s": [
              0.0027,
              0.003,
              0.0031
            ],
            "mean_s": 0.003,
            "median_s": 0.003,
            "std_s": 0.0002,
            "gflops": 88.94
          },
          "1024x1024": {
            "times_s": [
              0.0194,
              0.0191,
              0.0192
            ],
            "mean_s": 0.0192,
            "median_s": 0.0192,
            "std_s": 0.0001,
            "gflops": 111.66
          },
          "2048x2048": {
            "times_s": [
              0.1462,
              0.1457,
              0.1458
            ],
            "mean_s": 0.1459,
            "median_s": 0.1458,
            "std_s": 0.0002,
            "gflops": 117.86
          }
        }
      },
      "cpu_multi_thread": {
        "test": "CPU Multi-Thread",
        "method": "Matrix multiplication (NumPy, 16 threads, subprocess isol√©)",
        "n_threads": 16,
        "results": {
          "512x512": {
            "times_s": [
              0.0019,
              0.0022,
              0.0019
            ],
            "mean_s": 0.002,
            "median_s": 0.0019,
            "std_s": 0.0001,
            "gflops": 138.45
          },
          "1024x1024": {
            "times_s": [
              0.0069,
              0.0067,
              0.0067
            ],
            "mean_s": 0.0068,
            "median_s": 0.0067,
            "std_s": 0.0001,
            "gflops": 318.29
          },
          "2048x2048": {
            "times_s": [
              0.0411,
              0.0357,
              0.0356
            ],
            "mean_s": 0.0375,
            "median_s": 0.0357,
            "std_s": 0.0026,
            "gflops": 481.64
          }
        }
      },
      "memory_bandwidth": {
        "test": "Memory Bandwidth",
        "method": "NumPy array operations (256 Mo)",
        "data_size_gb": 0.25,
        "results": {
          "write": {
            "mean_s": 0.0788,
            "median_s": 0.084,
            "bandwidth_gb_s": 2.97
          },
          "read": {
            "mean_s": 0.0381,
            "median_s": 0.0379,
            "bandwidth_gb_s": 6.59
          },
          "copy": {
            "mean_s": 0.0715,
            "median_s": 0.0765,
            "bandwidth_gb_s": 3.27
          }
        }
      },
      "gpu_compute": {
        "test": "GPU Compute (Raw)",
        "status": "completed",
        "device": "Radeon RX 5500M\u0000",
        "backend": "DirectML",
        "gpu_index": 0,
        "results": {
          "1024x1024": {
            "times_s": [
              0.0025,
              0.0023,
              0.0023
            ],
            "mean_s": 0.0024,
            "median_s": 0.0023,
            "gflops": 923.57
          },
          "2048x2048": {
            "times_s": [
              0.0102,
              0.0102,
              0.0103
            ],
            "mean_s": 0.0102,
            "median_s": 0.0102,
            "gflops": 1677.2
          },
          "4096x4096": {
            "times_s": [
              0.0647,
              0.0614,
              0.0623
            ],
            "mean_s": 0.0628,
            "median_s": 0.0623,
            "gflops": 2204.95
          }
        }
      },
      "gpu_system": {
        "test": "GPU System Score",
        "status": "completed",
        "device": "Radeon RX 5500M\u0000",
        "backend": "DirectML",
        "gpu_index": 0,
        "results": {
          "1024x1024": {
            "pipeline_times_s": [
              0.0087,
              0.0083,
              0.0088
            ],
            "pipeline_median_s": 0.0087,
            "transfer_to_median_s": 0.0048,
            "compute_median_s": 0.0023,
            "transfer_back_median_s": 0.0014,
            "gflops_pipeline": 246.91,
            "gflops_compute": 945.78,
            "transfer_bandwidth_gb_s": 1.87,
            "data_transferred_gb": 0.0117,
            "pct_transfer_to": 55.4,
            "pct_compute": 26.1,
            "pct_transfer_back": 16.6
          },
          "2048x2048": {
            "pipeline_times_s": [
              0.0289,
              0.0273,
              0.0275
            ],
            "pipeline_median_s": 0.0275,
            "transfer_to_median_s": 0.0125,
            "compute_median_s": 0.0101,
            "transfer_back_median_s": 0.0049,
            "gflops_pipeline": 625.32,
            "gflops_compute": 1702.81,
            "transfer_bandwidth_gb_s": 2.69,
            "data_transferred_gb": 0.0469,
            "pct_transfer_to": 45.5,
            "pct_compute": 36.7,
            "pct_transfer_back": 17.9
          },
          "4096x4096": {
            "pipeline_times_s": [
              0.129,
              0.1249,
              0.125
            ],
            "pipeline_median_s": 0.125,
            "transfer_to_median_s": 0.0433,
            "compute_median_s": 0.0614,
            "transfer_back_median_s": 0.0204,
            "gflops_pipeline": 1099.88,
            "gflops_compute": 2238.1,
            "transfer_bandwidth_gb_s": 2.94,
            "data_transferred_gb": 0.1875,
            "pct_transfer_to": 34.7,
            "pct_compute": 49.1,
            "pct_transfer_back": 16.3
          }
        }
      }
    },
    "resource_usage": {
      "n_samples": 24,
      "duration_s": 12.90405797958374,
      "cpu": {
        "avg_percent": 11.3,
        "max_percent": 27.9,
        "min_percent": 0.0
      },
      "ram": {
        "avg_percent": 71.7,
        "max_percent": 75.3,
        "peak_used_gb": 11.6
      }
    }
  },
  "ai_benchmarks": {
    "type": "ai_benchmarks",
    "inference_mode": "server",
    "prompt": "Explain the concept of artificial intelligence in simple terms. What are its main applications and how does it impact our daily lives? Provide specific examples.",
    "inference_config": {
      "max_tokens": 256,
      "temperature": 0.7,
      "top_p": 0.9,
      "repeat_penalty": 1.1,
      "n_ctx": 2048,
      "seed": 42,
      "n_warmup_runs": 2,
      "n_benchmark_runs": 3
    },
    "total_time_s": 392.79,
    "models_tested": 1,
    "results": {
      "tinyllama-1.1b": {
        "model": "TinyLlama 1.1B",
        "params": "1.1B",
        "status": "completed",
        "inference_mode": "server",
        "runs": [
          {
            "tokens_generated": 256,
            "total_time_s": 5.0018,
            "first_token_latency_s": 0.0261,
            "tokens_per_second": 51.18,
            "memory_before_gb": 1.579,
            "memory_after_gb": 1.58,
            "memory_delta_gb": 0.001,
            "server_memory_gb": 0.742,
            "error": null,
            "success": true,
            "inference_mode": "server",
            "avg_inter_token_latency_ms": 19.51,
            "p90_inter_token_latency_ms": 20.63
          },
          {
            "tokens_generated": 256,
            "total_time_s": 5.0194,
            "first_token_latency_s": 0.0492,
            "tokens_per_second": 51.0,
            "memory_before_gb": 1.58,
            "memory_after_gb": 1.586,
            "memory_delta_gb": 0.007,
            "server_memory_gb": 0.749,
            "error": null,
            "success": true,
            "inference_mode": "server",
            "avg_inter_token_latency_ms": 19.49,
            "p90_inter_token_latency_ms": 20.67
          },
          {
            "tokens_generated": 256,
            "total_time_s": 5.0589,
            "first_token_latency_s": 0.0242,
            "tokens_per_second": 50.6,
            "memory_before_gb": 1.586,
            "memory_after_gb": 1.586,
            "memory_delta_gb": 0.0,
            "server_memory_gb": 0.749,
            "error": null,
            "success": true,
            "inference_mode": "server",
            "avg_inter_token_latency_ms": 19.74,
            "p90_inter_token_latency_ms": 20.92
          }
        ],
        "model_load_time_s": 1.52,
        "backend": {
          "backend": "cpu",
          "n_gpu_layers": 0,
          "details": "AMD GPU d√©tect√© (AMD Radeon(TM) Graphics) ‚Äî DirectML n'est pas support√© par llama-cpp-python. Utilisez le mode llama-server avec un binaire Vulkan pour l'acc√©l√©ration GPU, sinon fallback CPU.",
          "inference_mode": "server",
          "server_url": "http://127.0.0.1:8080"
        },
        "resource_usage": {
          "n_samples": 27,
          "duration_s": 14.885364294052124,
          "cpu": {
            "avg_percent": 56.3,
            "max_percent": 64.6,
            "min_percent": 0.0
          },
          "ram": {
            "avg_percent": 79.1,
            "max_percent": 79.3,
            "peak_used_gb": 12.22
          }
        },
        "summary": {
          "n_successful_runs": 3,
          "n_total_runs": 3,
          "avg_tokens_per_second": 50.93,
          "std_tokens_per_second": 0.24,
          "avg_first_token_latency_s": 0.0332,
          "avg_total_time_s": 5.0267,
          "peak_memory_gb": 1.586,
          "stability": "stable"
        }
      }
    },
    "quantization_comparison": {
      "tinyllama-1.1b": {
        "model_name": "TinyLlama 1.1B",
        "model_key": "tinyllama-1.1b",
        "params": "1.1B",
        "variants_tested": [
          "Q2_K",
          "Q3_K_M",
          "Q4_K_M",
          "Q5_K_M",
          "Q6_K",
          "Q8_0"
        ],
        "results": {
          "Q2_K": {
            "quantization": "Q2_K",
            "bits": 2,
            "file_size_gb": 0.45,
            "status": "completed",
            "inference_mode": "server",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 5.0316,
                "first_token_latency_s": 0.0214,
                "tokens_per_second": 50.88,
                "memory_before_gb": 1.42,
                "memory_after_gb": 1.421,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 0.584,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.65,
                "p90_inter_token_latency_ms": 23.16
              },
              {
                "tokens_generated": 256,
                "total_time_s": 4.4253,
                "first_token_latency_s": 0.0291,
                "tokens_per_second": 57.85,
                "memory_before_gb": 1.421,
                "memory_after_gb": 1.428,
                "memory_delta_gb": 0.006,
                "server_memory_gb": 0.59,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 17.24,
                "p90_inter_token_latency_ms": 19.3
              },
              {
                "tokens_generated": 256,
                "total_time_s": 3.8641,
                "first_token_latency_s": 0.045,
                "tokens_per_second": 66.25,
                "memory_before_gb": 1.428,
                "memory_after_gb": 1.428,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 0.59,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 14.98,
                "p90_inter_token_latency_ms": 16.83
              }
            ],
            "actual_file_size_gb": 0.45,
            "model_load_time_s": 1.56,
            "backend": {
              "backend": "cpu",
              "n_gpu_layers": 0,
              "details": "AMD GPU d√©tect√© (AMD Radeon(TM) Graphics) ‚Äî DirectML n'est pas support√© par llama-cpp-python. Utilisez le mode llama-server avec un binaire Vulkan pour l'acc√©l√©ration GPU, sinon fallback CPU.",
              "inference_mode": "server",
              "server_url": "http://127.0.0.1:8080"
            },
            "memory_after_load_gb": 1.412,
            "server_memory_after_load_gb": 0.575,
            "resource_usage": {
              "n_samples": 24,
              "duration_s": 13.16991925239563,
              "cpu": {
                "avg_percent": 64.9,
                "max_percent": 89.5,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 79.2,
                "max_percent": 79.9,
                "peak_used_gb": 12.31
              }
            },
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 58.33,
              "std_tokens_per_second": 6.28,
              "min_tokens_per_second": 50.88,
              "max_tokens_per_second": 66.25,
              "avg_first_token_latency_s": 0.0318,
              "avg_inter_token_latency_ms": 17.29,
              "avg_total_time_s": 4.4403,
              "peak_memory_gb": 1.428,
              "stability": "stable"
            }
          },
          "Q3_K_M": {
            "quantization": "Q3_K_M",
            "bits": 3,
            "file_size_gb": 0.51,
            "status": "completed",
            "inference_mode": "server",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 4.276,
                "first_token_latency_s": 0.0325,
                "tokens_per_second": 59.87,
                "memory_before_gb": 1.477,
                "memory_after_gb": 1.478,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 0.641,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 16.64,
                "p90_inter_token_latency_ms": 18.57
              },
              {
                "tokens_generated": 256,
                "total_time_s": 4.6773,
                "first_token_latency_s": 0.045,
                "tokens_per_second": 54.73,
                "memory_before_gb": 1.478,
                "memory_after_gb": 1.485,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.648,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 18.16,
                "p90_inter_token_latency_ms": 21.01
              },
              {
                "tokens_generated": 256,
                "total_time_s": 4.9428,
                "first_token_latency_s": 0.0412,
                "tokens_per_second": 51.79,
                "memory_before_gb": 1.485,
                "memory_after_gb": 1.485,
                "memory_delta_gb": -0.0,
                "server_memory_gb": 0.648,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.22,
                "p90_inter_token_latency_ms": 24.01
              }
            ],
            "actual_file_size_gb": 0.513,
            "model_load_time_s": 1.54,
            "backend": {
              "backend": "cpu",
              "n_gpu_layers": 0,
              "details": "AMD GPU d√©tect√© (AMD Radeon(TM) Graphics) ‚Äî DirectML n'est pas support√© par llama-cpp-python. Utilisez le mode llama-server avec un binaire Vulkan pour l'acc√©l√©ration GPU, sinon fallback CPU.",
              "inference_mode": "server",
              "server_url": "http://127.0.0.1:8080"
            },
            "memory_after_load_gb": 1.469,
            "server_memory_after_load_gb": 0.632,
            "resource_usage": {
              "n_samples": 25,
              "duration_s": 13.733296394348145,
              "cpu": {
                "avg_percent": 61.1,
                "max_percent": 85.1,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 80.5,
                "max_percent": 80.9,
                "peak_used_gb": 12.47
              }
            },
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 55.46,
              "std_tokens_per_second": 3.34,
              "min_tokens_per_second": 51.79,
              "max_tokens_per_second": 59.87,
              "avg_first_token_latency_s": 0.0396,
              "avg_inter_token_latency_ms": 18.01,
              "avg_total_time_s": 4.632,
              "peak_memory_gb": 1.485,
              "stability": "stable"
            }
          },
          "Q4_K_M": {
            "quantization": "Q4_K_M",
            "bits": 4,
            "file_size_gb": 0.62,
            "status": "completed",
            "inference_mode": "server",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 5.4303,
                "first_token_latency_s": 0.0326,
                "tokens_per_second": 47.14,
                "memory_before_gb": 1.579,
                "memory_after_gb": 1.58,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 0.742,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 21.17,
                "p90_inter_token_latency_ms": 23.86
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.5267,
                "first_token_latency_s": 0.0452,
                "tokens_per_second": 46.32,
                "memory_before_gb": 1.58,
                "memory_after_gb": 1.586,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.749,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 21.49,
                "p90_inter_token_latency_ms": 24.74
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.6889,
                "first_token_latency_s": 0.024,
                "tokens_per_second": 45.0,
                "memory_before_gb": 1.586,
                "memory_after_gb": 1.586,
                "memory_delta_gb": -0.0,
                "server_memory_gb": 0.749,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 22.21,
                "p90_inter_token_latency_ms": 25.75
              }
            ],
            "actual_file_size_gb": 0.623,
            "model_load_time_s": 1.56,
            "backend": {
              "backend": "cpu",
              "n_gpu_layers": 0,
              "details": "AMD GPU d√©tect√© (AMD Radeon(TM) Graphics) ‚Äî DirectML n'est pas support√© par llama-cpp-python. Utilisez le mode llama-server avec un binaire Vulkan pour l'acc√©l√©ration GPU, sinon fallback CPU.",
              "inference_mode": "server",
              "server_url": "http://127.0.0.1:8080"
            },
            "memory_after_load_gb": 1.571,
            "server_memory_after_load_gb": 0.734,
            "resource_usage": {
              "n_samples": 29,
              "duration_s": 16.276639699935913,
              "cpu": {
                "avg_percent": 65.7,
                "max_percent": 83.6,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 82.0,
                "max_percent": 82.2,
                "peak_used_gb": 12.66
              }
            },
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 46.15,
              "std_tokens_per_second": 0.88,
              "min_tokens_per_second": 45.0,
              "max_tokens_per_second": 47.14,
              "avg_first_token_latency_s": 0.0339,
              "avg_inter_token_latency_ms": 21.62,
              "avg_total_time_s": 5.5486,
              "peak_memory_gb": 1.586,
              "stability": "stable"
            }
          },
          "Q5_K_M": {
            "quantization": "Q5_K_M",
            "bits": 5,
            "file_size_gb": 0.73,
            "status": "completed",
            "inference_mode": "server",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 5.7626,
                "first_token_latency_s": 0.0282,
                "tokens_per_second": 44.42,
                "memory_before_gb": 1.678,
                "memory_after_gb": 1.679,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 0.841,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 22.49,
                "p90_inter_token_latency_ms": 23.8
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.779,
                "first_token_latency_s": 0.0558,
                "tokens_per_second": 44.3,
                "memory_before_gb": 1.679,
                "memory_after_gb": 1.685,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.848,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 22.44,
                "p90_inter_token_latency_ms": 23.76
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.7717,
                "first_token_latency_s": 0.0427,
                "tokens_per_second": 44.35,
                "memory_before_gb": 1.685,
                "memory_after_gb": 1.685,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 0.848,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 22.47,
                "p90_inter_token_latency_ms": 23.63
              }
            ],
            "actual_file_size_gb": 0.729,
            "model_load_time_s": 1.54,
            "backend": {
              "backend": "cpu",
              "n_gpu_layers": 0,
              "details": "AMD GPU d√©tect√© (AMD Radeon(TM) Graphics) ‚Äî DirectML n'est pas support√© par llama-cpp-python. Utilisez le mode llama-server avec un binaire Vulkan pour l'acc√©l√©ration GPU, sinon fallback CPU.",
              "inference_mode": "server",
              "server_url": "http://127.0.0.1:8080"
            },
            "memory_after_load_gb": 1.669,
            "server_memory_after_load_gb": 0.832,
            "resource_usage": {
              "n_samples": 31,
              "duration_s": 17.22170639038086,
              "cpu": {
                "avg_percent": 55.2,
                "max_percent": 59.8,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 81.9,
                "max_percent": 82.0,
                "peak_used_gb": 12.63
              }
            },
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 44.36,
              "std_tokens_per_second": 0.05,
              "min_tokens_per_second": 44.3,
              "max_tokens_per_second": 44.42,
              "avg_first_token_latency_s": 0.0422,
              "avg_inter_token_latency_ms": 22.47,
              "avg_total_time_s": 5.7711,
              "peak_memory_gb": 1.685,
              "stability": "stable"
            }
          },
          "Q6_K": {
            "quantization": "Q6_K",
            "bits": 6,
            "file_size_gb": 0.84,
            "status": "completed",
            "inference_mode": "server",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 6.5045,
                "first_token_latency_s": 0.0312,
                "tokens_per_second": 39.36,
                "memory_before_gb": 1.782,
                "memory_after_gb": 1.784,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 0.946,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 25.38,
                "p90_inter_token_latency_ms": 26.66
              },
              {
                "tokens_generated": 256,
                "total_time_s": 6.5028,
                "first_token_latency_s": 0.0333,
                "tokens_per_second": 39.37,
                "memory_before_gb": 1.784,
                "memory_after_gb": 1.79,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.953,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 25.37,
                "p90_inter_token_latency_ms": 26.44
              },
              {
                "tokens_generated": 256,
                "total_time_s": 6.5643,
                "first_token_latency_s": 0.0544,
                "tokens_per_second": 39.0,
                "memory_before_gb": 1.79,
                "memory_after_gb": 1.79,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 0.953,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 25.53,
                "p90_inter_token_latency_ms": 26.83
              }
            ],
            "actual_file_size_gb": 0.842,
            "model_load_time_s": 1.53,
            "backend": {
              "backend": "cpu",
              "n_gpu_layers": 0,
              "details": "AMD GPU d√©tect√© (AMD Radeon(TM) Graphics) ‚Äî DirectML n'est pas support√© par llama-cpp-python. Utilisez le mode llama-server avec un binaire Vulkan pour l'acc√©l√©ration GPU, sinon fallback CPU.",
              "inference_mode": "server",
              "server_url": "http://127.0.0.1:8080"
            },
            "memory_after_load_gb": 1.774,
            "server_memory_after_load_gb": 0.937,
            "resource_usage": {
              "n_samples": 35,
              "duration_s": 19.499542713165283,
              "cpu": {
                "avg_percent": 57.1,
                "max_percent": 64.7,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 81.9,
                "max_percent": 82.0,
                "peak_used_gb": 12.63
              }
            },
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 39.24,
              "std_tokens_per_second": 0.17,
              "min_tokens_per_second": 39.0,
              "max_tokens_per_second": 39.37,
              "avg_first_token_latency_s": 0.0396,
              "avg_inter_token_latency_ms": 25.43,
              "avg_total_time_s": 6.5239,
              "peak_memory_gb": 1.79,
              "stability": "stable"
            }
          },
          "Q8_0": {
            "quantization": "Q8_0",
            "bits": 8,
            "file_size_gb": 1.09,
            "status": "completed",
            "inference_mode": "server",
            "runs": [
              {
                "tokens_generated": 14,
                "total_time_s": 0.4701,
                "first_token_latency_s": 0.0379,
                "tokens_per_second": 29.78,
                "memory_before_gb": 2.013,
                "memory_after_gb": 2.013,
                "memory_delta_gb": -0.0,
                "server_memory_gb": 1.178,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 30.9,
                "p90_inter_token_latency_ms": 31.93
              },
              {
                "tokens_generated": 14,
                "total_time_s": 0.4693,
                "first_token_latency_s": 0.0368,
                "tokens_per_second": 29.83,
                "memory_before_gb": 2.013,
                "memory_after_gb": 2.013,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 1.178,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 30.92,
                "p90_inter_token_latency_ms": 31.87
              },
              {
                "tokens_generated": 14,
                "total_time_s": 0.4858,
                "first_token_latency_s": 0.0532,
                "tokens_per_second": 28.82,
                "memory_before_gb": 2.013,
                "memory_after_gb": 2.013,
                "memory_delta_gb": -0.0,
                "server_memory_gb": 1.178,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 30.9,
                "p90_inter_token_latency_ms": 32.38
              }
            ],
            "actual_file_size_gb": 1.09,
            "model_load_time_s": 1.57,
            "backend": {
              "backend": "cpu",
              "n_gpu_layers": 0,
              "details": "AMD GPU d√©tect√© (AMD Radeon(TM) Graphics) ‚Äî DirectML n'est pas support√© par llama-cpp-python. Utilisez le mode llama-server avec un binaire Vulkan pour l'acc√©l√©ration GPU, sinon fallback CPU.",
              "inference_mode": "server",
              "server_url": "http://127.0.0.1:8080"
            },
            "memory_after_load_gb": 2.005,
            "server_memory_after_load_gb": 1.17,
            "resource_usage": {
              "n_samples": 3,
              "duration_s": 1.225459337234497,
              "cpu": {
                "avg_percent": 39.5,
                "max_percent": 60.3,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 80.8,
                "max_percent": 80.8,
                "peak_used_gb": 12.45
              }
            },
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 29.48,
              "std_tokens_per_second": 0.46,
              "min_tokens_per_second": 28.82,
              "max_tokens_per_second": 29.83,
              "avg_first_token_latency_s": 0.0426,
              "avg_inter_token_latency_ms": 30.91,
              "avg_total_time_s": 0.4751,
              "peak_memory_gb": 2.013,
              "stability": "stable"
            }
          }
        },
        "comparison_table": [
          {
            "quantization": "Q2_K",
            "bits": 2,
            "file_size_gb": 0.45,
            "tokens_per_second": 58.33,
            "first_token_latency_s": 0.0318,
            "inter_token_latency_ms": 17.29,
            "peak_memory_gb": 1.428,
            "model_load_time_s": 1.56,
            "stability": "stable"
          },
          {
            "quantization": "Q3_K_M",
            "bits": 3,
            "file_size_gb": 0.513,
            "tokens_per_second": 55.46,
            "first_token_latency_s": 0.0396,
            "inter_token_latency_ms": 18.01,
            "peak_memory_gb": 1.485,
            "model_load_time_s": 1.54,
            "stability": "stable"
          },
          {
            "quantization": "Q4_K_M",
            "bits": 4,
            "file_size_gb": 0.623,
            "tokens_per_second": 46.15,
            "first_token_latency_s": 0.0339,
            "inter_token_latency_ms": 21.62,
            "peak_memory_gb": 1.586,
            "model_load_time_s": 1.56,
            "stability": "stable"
          },
          {
            "quantization": "Q5_K_M",
            "bits": 5,
            "file_size_gb": 0.729,
            "tokens_per_second": 44.36,
            "first_token_latency_s": 0.0422,
            "inter_token_latency_ms": 22.47,
            "peak_memory_gb": 1.685,
            "model_load_time_s": 1.54,
            "stability": "stable"
          },
          {
            "quantization": "Q6_K",
            "bits": 6,
            "file_size_gb": 0.842,
            "tokens_per_second": 39.24,
            "first_token_latency_s": 0.0396,
            "inter_token_latency_ms": 25.43,
            "peak_memory_gb": 1.79,
            "model_load_time_s": 1.53,
            "stability": "stable"
          },
          {
            "quantization": "Q8_0",
            "bits": 8,
            "file_size_gb": 1.09,
            "tokens_per_second": 29.48,
            "first_token_latency_s": 0.0426,
            "inter_token_latency_ms": 30.91,
            "peak_memory_gb": 2.013,
            "model_load_time_s": 1.57,
            "stability": "stable"
          }
        ]
      }
    },
    "temperature_comparison": {
      "tinyllama-1.1b": {
        "axis": "temperature",
        "model_name": "TinyLlama 1.1B",
        "model_key": "tinyllama-1.1b",
        "params": "1.1B",
        "variants_tested": [
          "low",
          "medium",
          "high"
        ],
        "inference_mode": "server",
        "status": "completed",
        "results": {
          "low": {
            "temperature": 0.25,
            "label": "Basse (0.25)",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 5.0408,
                "first_token_latency_s": 0.0251,
                "tokens_per_second": 50.79,
                "memory_before_gb": 1.577,
                "memory_after_gb": 1.578,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 0.743,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.67,
                "p90_inter_token_latency_ms": 20.81
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.0535,
                "first_token_latency_s": 0.0463,
                "tokens_per_second": 50.66,
                "memory_before_gb": 1.578,
                "memory_after_gb": 1.585,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.749,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.64,
                "p90_inter_token_latency_ms": 20.95
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.0436,
                "first_token_latency_s": 0.0243,
                "tokens_per_second": 50.76,
                "memory_before_gb": 1.585,
                "memory_after_gb": 1.585,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 0.749,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.68,
                "p90_inter_token_latency_ms": 20.93
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 50.74,
              "std_tokens_per_second": 0.06,
              "avg_first_token_latency_s": 0.0319,
              "avg_inter_token_latency_ms": 19.66,
              "avg_total_time_s": 5.046,
              "peak_memory_gb": 1.585,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 27,
              "duration_s": 14.867287635803223,
              "cpu": {
                "avg_percent": 57.4,
                "max_percent": 66.7,
                "min_percent": 53.9
              },
              "ram": {
                "avg_percent": 77.8,
                "max_percent": 77.9,
                "peak_used_gb": 12.0
              }
            }
          },
          "medium": {
            "temperature": 0.5,
            "label": "Moyenne (0.50)",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 5.0421,
                "first_token_latency_s": 0.0252,
                "tokens_per_second": 50.77,
                "memory_before_gb": 1.585,
                "memory_after_gb": 1.585,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 0.75,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.67,
                "p90_inter_token_latency_ms": 20.95
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.0537,
                "first_token_latency_s": 0.0485,
                "tokens_per_second": 50.66,
                "memory_before_gb": 1.585,
                "memory_after_gb": 1.592,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.756,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.63,
                "p90_inter_token_latency_ms": 20.81
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.0445,
                "first_token_latency_s": 0.0246,
                "tokens_per_second": 50.75,
                "memory_before_gb": 1.592,
                "memory_after_gb": 1.592,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 0.757,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.68,
                "p90_inter_token_latency_ms": 20.88
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 50.73,
              "std_tokens_per_second": 0.05,
              "avg_first_token_latency_s": 0.0328,
              "avg_inter_token_latency_ms": 19.66,
              "avg_total_time_s": 5.0468,
              "peak_memory_gb": 1.592,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 27,
              "duration_s": 14.938272953033447,
              "cpu": {
                "avg_percent": 57.3,
                "max_percent": 64.0,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 77.9,
                "max_percent": 78.0,
                "peak_used_gb": 12.01
              }
            }
          },
          "high": {
            "temperature": 0.75,
            "label": "Haute (0.75)",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 5.1118,
                "first_token_latency_s": 0.047,
                "tokens_per_second": 50.08,
                "memory_before_gb": 1.592,
                "memory_after_gb": 1.592,
                "memory_delta_gb": -0.0,
                "server_memory_gb": 0.757,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.86,
                "p90_inter_token_latency_ms": 21.14
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.1023,
                "first_token_latency_s": 0.0569,
                "tokens_per_second": 50.17,
                "memory_before_gb": 1.592,
                "memory_after_gb": 1.599,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.764,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.78,
                "p90_inter_token_latency_ms": 20.96
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.0721,
                "first_token_latency_s": 0.0476,
                "tokens_per_second": 50.47,
                "memory_before_gb": 1.599,
                "memory_after_gb": 1.599,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 0.764,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.7,
                "p90_inter_token_latency_ms": 20.99
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 50.24,
              "std_tokens_per_second": 0.17,
              "avg_first_token_latency_s": 0.0505,
              "avg_inter_token_latency_ms": 19.78,
              "avg_total_time_s": 5.0954,
              "peak_memory_gb": 1.599,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 27,
              "duration_s": 14.8928964138031,
              "cpu": {
                "avg_percent": 55.7,
                "max_percent": 68.0,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 77.9,
                "max_percent": 78.0,
                "peak_used_gb": 12.01
              }
            }
          }
        },
        "backend": {
          "backend": "cpu",
          "n_gpu_layers": 0,
          "details": "AMD GPU d√©tect√© (AMD Radeon(TM) Graphics) ‚Äî DirectML n'est pas support√© par llama-cpp-python. Utilisez le mode llama-server avec un binaire Vulkan pour l'acc√©l√©ration GPU, sinon fallback CPU.",
          "inference_mode": "server"
        },
        "comparison_table": [
          {
            "temperature_key": "low",
            "temperature": 0.25,
            "label": "Basse (0.25)",
            "tokens_per_second": 50.74,
            "first_token_latency_s": 0.0319,
            "inter_token_latency_ms": 19.66,
            "peak_memory_gb": 1.585,
            "stability": "stable"
          },
          {
            "temperature_key": "medium",
            "temperature": 0.5,
            "label": "Moyenne (0.50)",
            "tokens_per_second": 50.73,
            "first_token_latency_s": 0.0328,
            "inter_token_latency_ms": 19.66,
            "peak_memory_gb": 1.592,
            "stability": "stable"
          },
          {
            "temperature_key": "high",
            "temperature": 0.75,
            "label": "Haute (0.75)",
            "tokens_per_second": 50.24,
            "first_token_latency_s": 0.0505,
            "inter_token_latency_ms": 19.78,
            "peak_memory_gb": 1.599,
            "stability": "stable"
          }
        ]
      }
    },
    "language_comparison": {
      "tinyllama-1.1b": {
        "axis": "language",
        "model_name": "TinyLlama 1.1B",
        "model_key": "tinyllama-1.1b",
        "params": "1.1B",
        "variants_tested": [
          "en",
          "fr",
          "zh",
          "es",
          "de",
          "ar"
        ],
        "inference_mode": "server",
        "status": "completed",
        "results": {
          "en": {
            "language": "en",
            "label": "Anglais",
            "flag": "üá¨üáß",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 5.0514,
                "first_token_latency_s": 0.025,
                "tokens_per_second": 50.68,
                "memory_before_gb": 1.577,
                "memory_after_gb": 1.578,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 0.742,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.71,
                "p90_inter_token_latency_ms": 21.19
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.0458,
                "first_token_latency_s": 0.0308,
                "tokens_per_second": 50.74,
                "memory_before_gb": 1.578,
                "memory_after_gb": 1.585,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.749,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.67,
                "p90_inter_token_latency_ms": 20.95
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.0989,
                "first_token_latency_s": 0.0387,
                "tokens_per_second": 50.21,
                "memory_before_gb": 1.585,
                "memory_after_gb": 1.585,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 0.749,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.84,
                "p90_inter_token_latency_ms": 21.09
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 50.54,
              "std_tokens_per_second": 0.24,
              "avg_first_token_latency_s": 0.0315,
              "avg_inter_token_latency_ms": 19.74,
              "avg_total_time_s": 5.0654,
              "peak_memory_gb": 1.585,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 27,
              "duration_s": 14.857372283935547,
              "cpu": {
                "avg_percent": 57.7,
                "max_percent": 66.8,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 77.7,
                "max_percent": 77.7,
                "peak_used_gb": 11.97
              }
            }
          },
          "fr": {
            "language": "fr",
            "label": "Fran√ßais",
            "flag": "üá´üá∑",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 5.2799,
                "first_token_latency_s": 0.2162,
                "tokens_per_second": 48.49,
                "memory_before_gb": 1.585,
                "memory_after_gb": 1.585,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 0.75,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.86,
                "p90_inter_token_latency_ms": 21.27
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.1542,
                "first_token_latency_s": 0.031,
                "tokens_per_second": 49.67,
                "memory_before_gb": 1.585,
                "memory_after_gb": 1.593,
                "memory_delta_gb": 0.008,
                "server_memory_gb": 0.757,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 20.09,
                "p90_inter_token_latency_ms": 21.47
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.0844,
                "first_token_latency_s": 0.0274,
                "tokens_per_second": 50.35,
                "memory_before_gb": 1.593,
                "memory_after_gb": 1.6,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.764,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.83,
                "p90_inter_token_latency_ms": 21.05
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 49.5,
              "std_tokens_per_second": 0.77,
              "avg_first_token_latency_s": 0.0915,
              "avg_inter_token_latency_ms": 19.93,
              "avg_total_time_s": 5.1728,
              "peak_memory_gb": 1.6,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 28,
              "duration_s": 15.48233675956726,
              "cpu": {
                "avg_percent": 56.0,
                "max_percent": 67.5,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 77.8,
                "max_percent": 77.8,
                "peak_used_gb": 11.99
              }
            }
          },
          "zh": {
            "language": "zh",
            "label": "Mandarin",
            "flag": "üá®üá≥",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 5.3316,
                "first_token_latency_s": 0.239,
                "tokens_per_second": 48.02,
                "memory_before_gb": 1.6,
                "memory_after_gb": 1.601,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 0.765,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.97,
                "p90_inter_token_latency_ms": 21.13
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.1343,
                "first_token_latency_s": 0.0275,
                "tokens_per_second": 49.86,
                "memory_before_gb": 1.601,
                "memory_after_gb": 1.608,
                "memory_delta_gb": 0.008,
                "server_memory_gb": 0.773,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 20.03,
                "p90_inter_token_latency_ms": 21.22
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.1261,
                "first_token_latency_s": 0.0277,
                "tokens_per_second": 49.94,
                "memory_before_gb": 1.608,
                "memory_after_gb": 1.616,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.78,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.99,
                "p90_inter_token_latency_ms": 21.17
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 49.27,
              "std_tokens_per_second": 0.89,
              "avg_first_token_latency_s": 0.0981,
              "avg_inter_token_latency_ms": 20.0,
              "avg_total_time_s": 5.1973,
              "peak_memory_gb": 1.616,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 28,
              "duration_s": 15.356107473373413,
              "cpu": {
                "avg_percent": 55.3,
                "max_percent": 62.3,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 77.8,
                "max_percent": 77.8,
                "peak_used_gb": 11.99
              }
            }
          },
          "es": {
            "language": "es",
            "label": "Espagnol",
            "flag": "üá™üá∏",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 5.3628,
                "first_token_latency_s": 0.2253,
                "tokens_per_second": 47.74,
                "memory_before_gb": 1.616,
                "memory_after_gb": 1.616,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 0.781,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 20.15,
                "p90_inter_token_latency_ms": 22.53
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.2165,
                "first_token_latency_s": 0.0301,
                "tokens_per_second": 49.08,
                "memory_before_gb": 1.616,
                "memory_after_gb": 1.624,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.788,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 20.34,
                "p90_inter_token_latency_ms": 21.78
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.1019,
                "first_token_latency_s": 0.0421,
                "tokens_per_second": 50.18,
                "memory_before_gb": 1.624,
                "memory_after_gb": 1.631,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.795,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.84,
                "p90_inter_token_latency_ms": 21.2
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 49.0,
              "std_tokens_per_second": 1.0,
              "avg_first_token_latency_s": 0.0992,
              "avg_inter_token_latency_ms": 20.11,
              "avg_total_time_s": 5.2271,
              "peak_memory_gb": 1.631,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 28,
              "duration_s": 15.449976921081543,
              "cpu": {
                "avg_percent": 56.2,
                "max_percent": 76.4,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 78.0,
                "max_percent": 78.2,
                "peak_used_gb": 12.04
              }
            }
          },
          "de": {
            "language": "de",
            "label": "Allemand",
            "flag": "üá©üá™",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 5.2734,
                "first_token_latency_s": 0.2294,
                "tokens_per_second": 48.55,
                "memory_before_gb": 1.631,
                "memory_after_gb": 1.632,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 0.796,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.78,
                "p90_inter_token_latency_ms": 21.11
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.1104,
                "first_token_latency_s": 0.0511,
                "tokens_per_second": 50.09,
                "memory_before_gb": 1.632,
                "memory_after_gb": 1.639,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.803,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.84,
                "p90_inter_token_latency_ms": 20.98
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.09,
                "first_token_latency_s": 0.0425,
                "tokens_per_second": 50.29,
                "memory_before_gb": 1.639,
                "memory_after_gb": 1.646,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.81,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.79,
                "p90_inter_token_latency_ms": 21.03
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 49.64,
              "std_tokens_per_second": 0.78,
              "avg_first_token_latency_s": 0.1077,
              "avg_inter_token_latency_ms": 19.8,
              "avg_total_time_s": 5.1579,
              "peak_memory_gb": 1.646,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 28,
              "duration_s": 15.412934064865112,
              "cpu": {
                "avg_percent": 55.1,
                "max_percent": 65.0,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 78.1,
                "max_percent": 78.2,
                "peak_used_gb": 12.04
              }
            }
          },
          "ar": {
            "language": "ar",
            "label": "Arabe",
            "flag": "üá∏üá¶",
            "runs": [
              {
                "tokens_generated": 241,
                "total_time_s": 5.4296,
                "first_token_latency_s": 0.234,
                "tokens_per_second": 44.39,
                "memory_before_gb": 1.646,
                "memory_after_gb": 1.646,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 0.81,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 21.65,
                "p90_inter_token_latency_ms": 22.05
              },
              {
                "tokens_generated": 246,
                "total_time_s": 5.2168,
                "first_token_latency_s": 0.0267,
                "tokens_per_second": 47.16,
                "memory_before_gb": 1.646,
                "memory_after_gb": 1.655,
                "memory_delta_gb": 0.008,
                "server_memory_gb": 0.819,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 21.18,
                "p90_inter_token_latency_ms": 21.63
              },
              {
                "tokens_generated": 246,
                "total_time_s": 5.2503,
                "first_token_latency_s": 0.0466,
                "tokens_per_second": 46.85,
                "memory_before_gb": 1.655,
                "memory_after_gb": 1.663,
                "memory_delta_gb": 0.008,
                "server_memory_gb": 0.827,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 21.24,
                "p90_inter_token_latency_ms": 22.31
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 46.13,
              "std_tokens_per_second": 1.24,
              "avg_first_token_latency_s": 0.1024,
              "avg_inter_token_latency_ms": 21.36,
              "avg_total_time_s": 5.2989,
              "peak_memory_gb": 1.663,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 28,
              "duration_s": 15.457031011581421,
              "cpu": {
                "avg_percent": 56.2,
                "max_percent": 65.1,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 77.9,
                "max_percent": 78.2,
                "peak_used_gb": 12.04
              }
            }
          }
        },
        "backend": {
          "backend": "cpu",
          "n_gpu_layers": 0,
          "details": "AMD GPU d√©tect√© (AMD Radeon(TM) Graphics) ‚Äî DirectML n'est pas support√© par llama-cpp-python. Utilisez le mode llama-server avec un binaire Vulkan pour l'acc√©l√©ration GPU, sinon fallback CPU.",
          "inference_mode": "server"
        },
        "comparison_table": [
          {
            "language_key": "en",
            "label": "Anglais",
            "flag": "üá¨üáß",
            "tokens_per_second": 50.54,
            "first_token_latency_s": 0.0315,
            "inter_token_latency_ms": 19.74,
            "peak_memory_gb": 1.585,
            "stability": "stable"
          },
          {
            "language_key": "fr",
            "label": "Fran√ßais",
            "flag": "üá´üá∑",
            "tokens_per_second": 49.5,
            "first_token_latency_s": 0.0915,
            "inter_token_latency_ms": 19.93,
            "peak_memory_gb": 1.6,
            "stability": "stable"
          },
          {
            "language_key": "zh",
            "label": "Mandarin",
            "flag": "üá®üá≥",
            "tokens_per_second": 49.27,
            "first_token_latency_s": 0.0981,
            "inter_token_latency_ms": 20.0,
            "peak_memory_gb": 1.616,
            "stability": "stable"
          },
          {
            "language_key": "es",
            "label": "Espagnol",
            "flag": "üá™üá∏",
            "tokens_per_second": 49.0,
            "first_token_latency_s": 0.0992,
            "inter_token_latency_ms": 20.11,
            "peak_memory_gb": 1.631,
            "stability": "stable"
          },
          {
            "language_key": "de",
            "label": "Allemand",
            "flag": "üá©üá™",
            "tokens_per_second": 49.64,
            "first_token_latency_s": 0.1077,
            "inter_token_latency_ms": 19.8,
            "peak_memory_gb": 1.646,
            "stability": "stable"
          },
          {
            "language_key": "ar",
            "label": "Arabe",
            "flag": "üá∏üá¶",
            "tokens_per_second": 46.13,
            "first_token_latency_s": 0.1024,
            "inter_token_latency_ms": 21.36,
            "peak_memory_gb": 1.663,
            "stability": "stable"
          }
        ]
      }
    },
    "prompt_type_comparison": {
      "tinyllama-1.1b": {
        "axis": "prompt_type",
        "model_name": "TinyLlama 1.1B",
        "model_key": "tinyllama-1.1b",
        "params": "1.1B",
        "variants_tested": [
          "general",
          "code",
          "reasoning",
          "creative",
          "math"
        ],
        "inference_mode": "server",
        "status": "completed",
        "results": {
          "general": {
            "prompt_type": "general",
            "label": "G√©n√©ral / Connaissances",
            "icon": "üìö",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 5.0233,
                "first_token_latency_s": 0.0393,
                "tokens_per_second": 50.96,
                "memory_before_gb": 1.577,
                "memory_after_gb": 1.579,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 0.743,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.54,
                "p90_inter_token_latency_ms": 20.78
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.0373,
                "first_token_latency_s": 0.0452,
                "tokens_per_second": 50.82,
                "memory_before_gb": 1.579,
                "memory_after_gb": 1.585,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.75,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.58,
                "p90_inter_token_latency_ms": 20.84
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.1692,
                "first_token_latency_s": 0.0381,
                "tokens_per_second": 49.52,
                "memory_before_gb": 1.585,
                "memory_after_gb": 1.585,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 0.75,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 20.12,
                "p90_inter_token_latency_ms": 22.09
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 50.43,
              "std_tokens_per_second": 0.65,
              "avg_first_token_latency_s": 0.0409,
              "avg_inter_token_latency_ms": 19.75,
              "avg_total_time_s": 5.0766,
              "peak_memory_gb": 1.585,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 27,
              "duration_s": 14.889450788497925,
              "cpu": {
                "avg_percent": 56.5,
                "max_percent": 64.1,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 77.4,
                "max_percent": 77.5,
                "peak_used_gb": 11.93
              }
            }
          },
          "code": {
            "prompt_type": "code",
            "label": "Code / Programmation",
            "icon": "üíª",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 6.1559,
                "first_token_latency_s": 0.2265,
                "tokens_per_second": 41.59,
                "memory_before_gb": 1.585,
                "memory_after_gb": 1.586,
                "memory_delta_gb": 0.001,
                "server_memory_gb": 0.75,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 23.25,
                "p90_inter_token_latency_ms": 26.41
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.6695,
                "first_token_latency_s": 0.0322,
                "tokens_per_second": 45.15,
                "memory_before_gb": 1.586,
                "memory_after_gb": 1.593,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.758,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 22.11,
                "p90_inter_token_latency_ms": 25.84
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.1846,
                "first_token_latency_s": 0.0513,
                "tokens_per_second": 49.38,
                "memory_before_gb": 1.594,
                "memory_after_gb": 1.601,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.765,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 20.13,
                "p90_inter_token_latency_ms": 22.04
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 45.37,
              "std_tokens_per_second": 3.18,
              "avg_first_token_latency_s": 0.1033,
              "avg_inter_token_latency_ms": 21.83,
              "avg_total_time_s": 5.67,
              "peak_memory_gb": 1.601,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 30,
              "duration_s": 16.864013671875,
              "cpu": {
                "avg_percent": 65.1,
                "max_percent": 82.8,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 79.7,
                "max_percent": 80.9,
                "peak_used_gb": 12.46
              }
            }
          },
          "reasoning": {
            "prompt_type": "reasoning",
            "label": "Raisonnement / Logique",
            "icon": "üß†",
            "runs": [
              {
                "tokens_generated": 179,
                "total_time_s": 3.64,
                "first_token_latency_s": 0.2305,
                "tokens_per_second": 49.18,
                "memory_before_gb": 1.601,
                "memory_after_gb": 1.601,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 0.765,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.04,
                "p90_inter_token_latency_ms": 20.16
              },
              {
                "tokens_generated": 256,
                "total_time_s": 4.9446,
                "first_token_latency_s": 0.0261,
                "tokens_per_second": 51.77,
                "memory_before_gb": 1.601,
                "memory_after_gb": 1.608,
                "memory_delta_gb": 0.006,
                "server_memory_gb": 0.772,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.29,
                "p90_inter_token_latency_ms": 20.53
              },
              {
                "tokens_generated": 256,
                "total_time_s": 4.983,
                "first_token_latency_s": 0.0372,
                "tokens_per_second": 51.37,
                "memory_before_gb": 1.608,
                "memory_after_gb": 1.615,
                "memory_delta_gb": 0.008,
                "server_memory_gb": 0.779,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.39,
                "p90_inter_token_latency_ms": 20.49
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 50.77,
              "std_tokens_per_second": 1.14,
              "avg_first_token_latency_s": 0.0979,
              "avg_inter_token_latency_ms": 19.24,
              "avg_total_time_s": 4.5225,
              "peak_memory_gb": 1.615,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 24,
              "duration_s": 13.07686471939087,
              "cpu": {
                "avg_percent": 51.3,
                "max_percent": 59.8,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 80.4,
                "max_percent": 80.4,
                "peak_used_gb": 12.39
              }
            }
          },
          "creative": {
            "prompt_type": "creative",
            "label": "Cr√©atif / R√©daction",
            "icon": "‚úçÔ∏è",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 5.1527,
                "first_token_latency_s": 0.2281,
                "tokens_per_second": 49.68,
                "memory_before_gb": 1.615,
                "memory_after_gb": 1.616,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 0.78,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.31,
                "p90_inter_token_latency_ms": 20.81
              },
              {
                "tokens_generated": 256,
                "total_time_s": 4.9863,
                "first_token_latency_s": 0.0467,
                "tokens_per_second": 51.34,
                "memory_before_gb": 1.616,
                "memory_after_gb": 1.623,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.787,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.37,
                "p90_inter_token_latency_ms": 21.08
              },
              {
                "tokens_generated": 256,
                "total_time_s": 4.9049,
                "first_token_latency_s": 0.0437,
                "tokens_per_second": 52.19,
                "memory_before_gb": 1.623,
                "memory_after_gb": 1.63,
                "memory_delta_gb": 0.007,
                "server_memory_gb": 0.794,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.06,
                "p90_inter_token_latency_ms": 20.46
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 51.07,
              "std_tokens_per_second": 1.04,
              "avg_first_token_latency_s": 0.1062,
              "avg_inter_token_latency_ms": 19.25,
              "avg_total_time_s": 5.0146,
              "peak_memory_gb": 1.63,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 27,
              "duration_s": 14.747391700744629,
              "cpu": {
                "avg_percent": 53.8,
                "max_percent": 63.6,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 80.3,
                "max_percent": 80.4,
                "peak_used_gb": 12.39
              }
            }
          },
          "math": {
            "prompt_type": "math",
            "label": "Math√©matiques",
            "icon": "üî¢",
            "runs": [
              {
                "tokens_generated": 256,
                "total_time_s": 5.1937,
                "first_token_latency_s": 0.2291,
                "tokens_per_second": 49.29,
                "memory_before_gb": 1.63,
                "memory_after_gb": 1.63,
                "memory_delta_gb": 0.0,
                "server_memory_gb": 0.794,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.47,
                "p90_inter_token_latency_ms": 20.92
              },
              {
                "tokens_generated": 256,
                "total_time_s": 4.9447,
                "first_token_latency_s": 0.0376,
                "tokens_per_second": 51.77,
                "memory_before_gb": 1.63,
                "memory_after_gb": 1.638,
                "memory_delta_gb": 0.008,
                "server_memory_gb": 0.802,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.24,
                "p90_inter_token_latency_ms": 20.39
              },
              {
                "tokens_generated": 256,
                "total_time_s": 5.0559,
                "first_token_latency_s": 0.0429,
                "tokens_per_second": 50.63,
                "memory_before_gb": 1.638,
                "memory_after_gb": 1.646,
                "memory_delta_gb": 0.008,
                "server_memory_gb": 0.81,
                "error": null,
                "success": true,
                "inference_mode": "server",
                "avg_inter_token_latency_ms": 19.66,
                "p90_inter_token_latency_ms": 21.07
              }
            ],
            "summary": {
              "n_successful_runs": 3,
              "n_total_runs": 3,
              "avg_tokens_per_second": 50.56,
              "std_tokens_per_second": 1.01,
              "avg_first_token_latency_s": 0.1032,
              "avg_inter_token_latency_ms": 19.46,
              "avg_total_time_s": 5.0648,
              "peak_memory_gb": 1.646,
              "stability": "stable"
            },
            "resource_usage": {
              "n_samples": 27,
              "duration_s": 14.723595142364502,
              "cpu": {
                "avg_percent": 52.1,
                "max_percent": 63.1,
                "min_percent": 0.0
              },
              "ram": {
                "avg_percent": 80.3,
                "max_percent": 80.4,
                "peak_used_gb": 12.38
              }
            }
          }
        },
        "backend": {
          "backend": "cpu",
          "n_gpu_layers": 0,
          "details": "AMD GPU d√©tect√© (AMD Radeon(TM) Graphics) ‚Äî DirectML n'est pas support√© par llama-cpp-python. Utilisez le mode llama-server avec un binaire Vulkan pour l'acc√©l√©ration GPU, sinon fallback CPU.",
          "inference_mode": "server"
        },
        "comparison_table": [
          {
            "prompt_type_key": "general",
            "label": "G√©n√©ral / Connaissances",
            "icon": "üìö",
            "tokens_per_second": 50.43,
            "first_token_latency_s": 0.0409,
            "inter_token_latency_ms": 19.75,
            "peak_memory_gb": 1.585,
            "stability": "stable"
          },
          {
            "prompt_type_key": "code",
            "label": "Code / Programmation",
            "icon": "üíª",
            "tokens_per_second": 45.37,
            "first_token_latency_s": 0.1033,
            "inter_token_latency_ms": 21.83,
            "peak_memory_gb": 1.601,
            "stability": "stable"
          },
          {
            "prompt_type_key": "reasoning",
            "label": "Raisonnement / Logique",
            "icon": "üß†",
            "tokens_per_second": 50.77,
            "first_token_latency_s": 0.0979,
            "inter_token_latency_ms": 19.24,
            "peak_memory_gb": 1.615,
            "stability": "stable"
          },
          {
            "prompt_type_key": "creative",
            "label": "Cr√©atif / R√©daction",
            "icon": "‚úçÔ∏è",
            "tokens_per_second": 51.07,
            "first_token_latency_s": 0.1062,
            "inter_token_latency_ms": 19.25,
            "peak_memory_gb": 1.63,
            "stability": "stable"
          },
          {
            "prompt_type_key": "math",
            "label": "Math√©matiques",
            "icon": "üî¢",
            "tokens_per_second": 50.56,
            "first_token_latency_s": 0.1032,
            "inter_token_latency_ms": 19.46,
            "peak_memory_gb": 1.646,
            "stability": "stable"
          }
        ]
      }
    }
  }
}